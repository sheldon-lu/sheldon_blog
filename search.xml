<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Go-Micro系列一]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2FGo-Micro%E7%B3%BB%E5%88%97%E4%B8%80%2F</url>
    <content type="text"><![CDATA[Go-Micro系列一 – 入门篇go 1.13.0 go版本 1.12 - 1.13 官方貌似20200227开始更新版本，关于micro的V2、GO MOD更新 安装go-micro安装consul安装服务发现能力，docker安装consul或者本地安装运行consul，consul在本地开发环境使用方便便于调试。 1、docker安装consul 1docker run -d --net=host -e &apos;CONSUL_LOCAL_CONFIG=&#123;&quot;skip_leave_on_interrupt&quot;: true&#125;&apos; --name consul_server consul agent -server -bind=192.168.0.111 -bootstrap-expect=1 -node=node1 -client 0.0.0.0 -ui 2、本地安装运行consul 12345678910windows:https://www.consul.io/downloads.htmlmac:https://www.consul.io/downloads.html直接下载对应操作系统的版本即可，自行添加环境变量consul命令即可直接运行，默认端口8500consul agent -server -bind=192.168.0.111 -bootstrap-expect=1 -node=node1 -client 0.0.0.0 -uiconsul agent -dev # 本地开发测试直接运行dev即可 以上两种方式运行，浏览器输入http://192.168.0.111:8500 能看到consul的UI页面表示正常启动 安装Micro基础知识补充go的包管理工具 – gomod 可参考：https://segmentfault.com/a/1190000018389353?utm_source=tag-newest https://segmentfault.com/a/1190000020293616?utm_source=tag-newest 安装123456789101112131415161718#linux/mac 下export GO111MODULE=onexport GOPROXY=https://goproxy.cn,direct# windows下设置如下环境变量setx GO111MODULE onsetx GOPROXY https://goproxy.cn,direct# go 1.12export GO111MODULE=onexport GOPROXY=https://goproxy.cn# 使用如下指令安装# micro为二进制命令文件go get -u -v github.com/micro/microgo get -u -v github.com/micro/go-micro#如果没有git请自行安装git#下载地址#https://git-scm.com/downloads/ 安装Protoc1、访问如下网址 1https://github.com/protocolbuffers/protobuf/releases 2、下载,不同的版本文件名称不一样，Win选择protoc-xx-win64.zip，Mac选择protoc-xx-osx-x86_64.zip 3、解压到目标zip文件，添加xxxxx\bin到环境变量path 1234Wine:\dev\protoc-xx-win64\binMac/xxx/user/protoc-xx-osx-x86_64/bin PS： 1go get github.com/micro/protobuf/&#123;proto,protoc-gen-go&#125; 发现无法用 protoc-gen-go 生成代码。所以我去下载了protobuf。 这里我是MAC系统所以，下载对应的mac版本 protobuf，有问题可以自行选择各种系统版本。 安装protoc-gen-micro插件这个插件主要作用是通过.proto文件生成适用于go-micro的代码 1go get -u -v github.com/micro/protoc-gen-micro 验证123consulprotocmicro --version Hello World两种方法，一种使用micro自动生成代码，另一种手动创建；注意，手动创建可以自定义目录，自行go mod init，自动生成代码会生成在goPath中的指定目录。可自行选择 这里使用自动生成代码 12345678910111213141516171819202122232425262728&gt;micro new $GOPATH/microapp/helloCreating service go.micro.srv.hello in $GOPATH\microapp\hello.├── main.go├── plugin.go├── handler│ └── hello.go├── subscriber│ └── hello.go├── proto\hello│ └── hello.proto├── Dockerfile├── Makefile├── README.md└── go.moddownload protobuf for micro:brew install protobufgo get -u github.com/golang/protobuf/&#123;proto,protoc-gen-go&#125;go get -u github.com/micro/protoc-gen-microcompile the proto file hello.proto:cd $GOPATH\microapp\helloprotoc --proto_path=.:$GOPATH/src --go_out=. --micro_out=. proto/hello/hello.proto 生成适配proto的golang代码 注意：在win系统下$GOPATH环境变量无效,因此如上脚本将创建微服务失败,因此我们需要对如上脚本进行处理 12345#切换到项目目录下&gt;cd $GOPATH\microapp\hello# 根据proto生成文件&gt;protoc --proto_path=. --go_out=. --micro_out=. proto/hello/hello.proto 启动应用对于现在的新版本已经默认不使用consul了。可以降低micro版本或者插件代码插入consul插件即可使用consul 123456789101112131415#启动consul&gt;consul agent -dev#默认mdns启动&gt;go run main.go 2019/08/19 13:00:46 Transport [http] Listening on [::]:546892019/08/19 13:00:46 Broker [http] Connected to [::]:546902019/08/19 13:00:46 Registry [mdns] Registering node: go.micro.srv.hello-4851dce2-ab5d-4e4c-801e-44dae5d93f262019/08/19 13:00:46 Subscribing go.micro.srv.hello-4851dce2-ab5d-4e4c-801e-44dae5d93f26 to topic: go.micro.srv.hello2019/08/19 13:00:46 Subscribing go.micro.srv.hello-4851dce2-ab5d-4e4c-801e-44dae5d93f26 to topic: go.micro.srv.hello#指定registry只需要添加参数即可，默认使用端口8500&gt;go run main.go --registry=consul## 注意这里没有指定注册consul，默认使用mdns（windows中无法使用mdns，并不是注册中心，使用的广播）## 了解mdns，自行查看 123456// 在mian.go目录下新建一个插件文件plugins.go// cat plugins.go 导入consul插件即可import _ "github.com/micro/go-plugins/registry/consul"// 使用的时候或者build带上plugins文件// go run main.go plugins.go --registry=consul 官方文档附上链接 查看是否启动这里说一个micro需要跟go-micro的版本兼容，不然无法使用micro命令 当然，如果你后期不适用micro api，micro命令不用也没关系 1234#也可以去consul的ui查看，更加直观&gt;micro list services --registry=consulgo.micro.srv.hellotopic:go.micro.srv.hello 启动restful api接口支持支持注意其中的–namespace参数,我们每一个微服务都属于一个命名空间,通过api暴露出来该命名空间后,满足go.micro.srv.*格式的微服务都可以访问。如go.micro.srv.hello可以通过如下格式访问 1234567http://127.0.0.1:8080/user/call&gt;micro api --namespace=go.micro.srv --registry=consul2019/08/19 13:07:11 Registering API Default Handler at /2019/08/19 13:07:11 HTTP API Listening on [::]:80802019/08/19 13:07:11 Transport [http] Listening on [::]:549342019/08/19 13:07:11 Broker [http] Connected to [::]:549352019/08/19 13:07:11 Registry [consul] Registering node: go.micro.api-1753185c-b8e1-49c4-aa0f-617f243a8e2a 验证api接口123curl -XPOST -d &quot;&#123;name: nihao&#125;&quot; http://127.0.0.1:8080/user/call -H &quot;Content-Type:application/json&quot;# output: hello nihao 系列链接-&gt; Go-Micro系列一-入门篇 -&gt; Go-Micro系列二-micro介绍以及微服务 -&gt; Go-Micro系列三- -&gt; Go-Micro系列四-]]></content>
      <categories>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>Golang</tag>
        <tag>Go-Micro</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go-Micro系列四]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2FGo-Micro%E7%B3%BB%E5%88%97%E5%9B%9B%2F</url>
    <content type="text"><![CDATA[TODO 系列链接-&gt; Go-Micro系列一-入门篇 -&gt; Go-Micro系列二-micro介绍以及微服务 -&gt; Go-Micro系列三- -&gt; Go-Micro系列四-]]></content>
      <categories>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>Golang</tag>
        <tag>Go-Micro</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go-Micro系列三]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2FGo-Micro%E7%B3%BB%E5%88%97%E4%B8%89%2F</url>
    <content type="text"><![CDATA[Go-Micro系列三 – micro使用go_micro框架微服务架构为srv、api 系列链接-&gt; Go-Micro系列一-入门篇 -&gt; Go-Micro系列二-micro介绍以及微服务 -&gt; Go-Micro系列三- -&gt; Go-Micro系列四-]]></content>
      <categories>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>Golang</tag>
        <tag>Go-Micro</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go-Micro系列二]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2FGo-Micro%E7%B3%BB%E5%88%97%E4%BA%8C%2F</url>
    <content type="text"><![CDATA[Go-Micro系列二 – micro介绍以及微服务MicroInstroductgo-micro是一个go语言的微服务框架，可以大幅度的提升开发效率，并完成一套微服务的架构，其架构借用官方的图 首先go-micro框架是一套微服务分布式的框架，其拥有以下特性： 服务注册/发现 负载均衡 消息解码，并默认支持json以及protobuf 基于rpc的请求响应 异步的消息通讯 最后也是我觉得最牛的一点，接口可插拔，你可以不管用运行环境到底是使用的etcd或者consul来做服务发现，是使用http或rabbitmq做通讯，使用kafka做订阅还是用rabbitmq或者redis，是的，所有的一些都是可以插拔的，只要在运行时加入对应的启动参数，就可以使用对应的插件 微服务Instroduct微服务的概念，可以理解为，将单体应用（所有功能揉在一起的应用）拆分成 N 个小型的应用（服务），每个应用只完成很小的一部分服务，拆分的粒度没有硬性标准，一般按照业务边界进行拆分，如订单服务、用户服务这样的垂直拆分。微服务的理念在传统的单机部署中并未见到明显的优势，但当越来越多的应用被部署到云中的时候，微服务的优势就立刻体现出来了，包括： 单应用共用数据库，而微服务每个服务使用不同或相同数据库 独立部署，每个服务独立去部署，独立运行于不同的进程，使得每个服务可以根据负载做部署数量调整 服务降级，当系统负载增高，需要对某些关键业务做优先增配时，可以对不关键、低级别的业务做服务降级，临时关闭某些服务来保证关键业务的稳定 功能单一，技术选型更广，由于微服务的功能相对单一，可以针对不同的微服务采用更适合的编程语言、数据库等，例如做朋友关系、二度好友功能就考虑使用neo4j的图形数据库，底层稳定性要求高的微服务则可以通过Java来开发，并发比较高的微服务则可以考虑使用Go来开发等等 后期维护隔离，对功能的后期开发升级不会影响整体的应用稳定性 单独测试，单独开发，可以使每个微服务由不同的团队去开发、去单独测试，增加了整体开发的速度 复用性，多个不同的应用可以直接复用某个微服务 单应用不同模块运行在单一进程中，而微服务则每个服务运行于不同的进程 而go-micro正是这样一个针对微服务开发的框架，其将所有热插拔的插件发布于go-plugins，开发者可以根据需求引用，同时在运行时追加响应的运行参数即可启用插件。 PS: Micro更新V2公告 Micro项目地址 更新时间：2020-04-21 系列链接-&gt; Go-Micro系列一-入门篇 -&gt; Go-Micro系列二-micro介绍以及微服务 -&gt; Go-Micro系列三- -&gt; Go-Micro系列四-]]></content>
      <categories>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>Golang</tag>
        <tag>Go-Micro</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Go-Micro相关]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2FGo-Micro%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[Go-Micro的那些]]></content>
      <categories>
        <category>Golang</category>
      </categories>
      <tags>
        <tag>Golang</tag>
        <tag>Go-Micro</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[React相关]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2FReact%E7%9B%B8%E5%85%B3%2F</url>
    <content type="text"><![CDATA[TODO]]></content>
  </entry>
  <entry>
    <title><![CDATA[Ceph集群部署]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2FCeph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[CentOS7下安装Ceph供Kubernetes使用环境说明1234系统：CentOS7，一个非系统分区分配给cephdocker：18.06-cekubernetes：1.11.3ceph：luminous Ceph部署准备节点规划1234567192.168.105.92 lab1 # master1192.168.105.93 lab2 # master2192.168.105.94 lab3 # master3192.168.105.95 lab4 # node4192.168.105.96 lab5 # node5192.168.105.97 lab6 # node6192.168.105.98 lab7 # node7 123监控节点：lab1、lab2、lab3OSD节点：lab4、lab5、lab6、lab7MDS节点：lab4 添加yum源我们使用阿里云yum源：(CeontOS和epel也是阿里云yum源) 123456789101112131415161718192021222324vim /etc/yum.repos.d/ceph.repo[Ceph]name=Ceph packages for $basearchbaseurl=https://mirrors.aliyun.com/ceph/rpm-luminous/el7/$basearchenabled=1gpgcheck=1type=rpm-mdgpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc[Ceph-noarch]name=Ceph noarch packagesbaseurl=https://mirrors.aliyun.com/ceph/rpm-luminous/el7/noarchenabled=1gpgcheck=1type=rpm-mdgpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc[ceph-source]name=Ceph source packagesbaseurl=https://mirrors.aliyun.com/ceph/rpm-luminous/el7/SRPMSenabled=1gpgcheck=1type=rpm-mdgpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc 注意：ceph集群中节点都需要添加该yum源 安装Ceph部署工具以下操作在lab1上root用户操作 12安装 ceph-deployyum -y install ceph-deploy 安装时间同步工具chrony以下操作在所有ceph节点root用户操作 123456789yum -y install chronysystemctl start chronydsystemctl enable chronyd# 修改/etc/chrony.conf前面的server段为如下server ntp1.aliyun.com iburstserver ntp2.aliyun.com iburstserver ntp3.aliyun.com iburstserver ntp4.aliyun.com iburst 安装SSH服务默认已正常运行，略 创建部署 CEPH 的用户以下操作在所有ceph节点root用户操作 ceph-deploy 工具必须以普通用户登录 Ceph 节点，且此用户拥有无密码使用 sudo 的权限，因为它需要在安装软件及配置文件的过程中，不必输入密码。 较新版的 ceph-deploy 支持用 –username 选项提供可无密码使用 sudo 的用户名（包括 root ，虽然不建议这样做）。使用 ceph-deploy –username {username} 命令时，指定的用户必须能够通过无密码 SSH 连接到 Ceph 节点，因为 ceph-deploy 中途不会提示输入密码。 建议在集群内的所有 Ceph 节点上给 ceph-deploy 创建一个特定的用户，但不要用 “ceph” 这个名字。全集群统一的用户名可简化操作（非必需），然而你应该避免使用知名用户名，因为黑客们会用它做暴力破解（如 root 、 admin 、 {productname} ）。后续步骤描述了如何创建无 sudo 密码的用户，你要用自己取的名字替换 {username} 。 注意：从 Infernalis 版起，用户名 “ceph” 保留给了 Ceph 守护进程。如果 Ceph 节点上已经有了 “ceph” 用户，升级前必须先删掉这个用户。 我们使用用户名ceph-admin 1234username="ceph-admin"useradd $&#123;username&#125; &amp;&amp; echo 'PASSWORD' | passwd $&#123;ceph-admin&#125; --stdinecho "$&#123;username&#125; ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/$&#123;username&#125;chmod 0440 /etc/sudoers.d/$&#123;username&#125;chmod a+x /etc/sudoers.d/ 允许无密码 SSH 登录以下操作在lab1节点ceph-admin用户操作 正因为 ceph-deploy 不支持输入密码，你必须在管理节点上生成 SSH 密钥并把其公钥分发到各 Ceph 节点。 ceph-deploy 会尝试给初始 monitors 生成 SSH 密钥对。 生成 SSH 密钥对，但不要用 sudo 或 root 用户。提示 “Enter passphrase” 时，直接回车，口令即为空： 123456789101112su - ceph-admin # 切换到此用户，因为ceph-deploy也用此用户ssh-keygenGenerating public/private rsa key pair.Enter file in which to save the key (/home/ceph-admin/.ssh/id_rsa): /home/ceph-admin/.ssh/id_rsa already exists.Overwrite (y/n)? yEnter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in /home/ceph-admin/.ssh/id_rsa.Your public key has been saved in /home/ceph-admin/.ssh/id_rsa.pub.The key fingerprint is: 把公钥拷贝到各 Ceph 节点，把下列命令中的 {username} 替换成前面创建部署 Ceph 的用户里的用户名。 1234username="ceph-admin"ssh-copy-id $&#123;username&#125;@lab1ssh-copy-id $&#123;username&#125;@lab2ssh-copy-id $&#123;username&#125;@lab3 （推荐做法）修改 ceph-deploy 管理节点上的 ~/.ssh/config 文件，这样 ceph-deploy 就能用你所建的用户名登录 Ceph 节点了，而无需每次执行 ceph-deploy 都要指定 –username {username} 。这样做同时也简化了 ssh 和 scp 的用法。 123456789Host lab1 Hostname lab1 User ceph-adminHost lab2 Hostname lab2 User ceph-adminHost lab3 Hostname lab3 User ceph-admin 123Bad owner or permissions on /home/ceph-admin/.ssh/config# 需要用chmod 600 ~/.ssh/config解决。 开放所需端口 以下操作在所有监视器节点root用户操作 Ceph Monitors 之间默认使用 6789 端口通信， OSD 之间默认用 6800:7300 这个范围内的端口通信。 firewall-cmd --zone=public --add-port=6789/tcp --permanent &amp;&amp; firewall-cmd --reload 终端（ TTY ） 以下操作在所有ceph节点root用户操作 在 CentOS 和 RHEL 上执行 ceph-deploy 命令时可能会报错。如果你的 Ceph 节点默认设置了 requiretty ，执行 sudo visudo 禁用它，并找到 Defaults requiretty 选项，把它改为 Defaults:ceph !requiretty 或者直接注释掉，这样 ceph-deploy 就可以用之前创建的用户（创建部署 Ceph 的用户 ）连接了。 1sed -i -r 's@Defaults(.*)!visiblepw@Defaults:ceph-admin\1!visiblepw@g' /etc/sudoers SELINUX 以下操作在所有ceph节点root用户操作 如果原来是开启的，需要重启生效。 永久关闭 修改/etc/sysconfig/selinux文件设置 1sed -i 's/SELINUX=.*/SELINUX=disabled/' /etc/sysconfig/selinux 整理以上所有ceph节点操作123456789101112yum -y install chronysystemctl start chronydsystemctl enable chronydusername="ceph-admin"useradd $&#123;username&#125; &amp;&amp; echo 'PASSWORD' | passwd $&#123;username&#125; --stdinecho "$&#123;username&#125; ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/$&#123;username&#125;chmod 0440 /etc/sudoers.d/$&#123;username&#125;chmod a+x /etc/sudoers.d/sed -i -r 's@Defaults(.*)!visiblepw@Defaults:ceph-admin\1!visiblepw@g' /etc/sudoerssed -i 's/SELINUX=.*/SELINUX=disabled/' /etc/sysconfig/selinuxyum -y install ceph 存储集群部署 以下操作在lab节点ceph-admin用户操作 我们创建一个 Ceph 存储集群，它有一个 Monitor 和两个 OSD 守护进程。一旦集群达到 active + clean 状态，再扩展它：增加第三个 OSD 、增加元数据服务器和两个 Ceph Monitors。 12su - ceph-adminmkdir ceph-cluster 如果在某些地方碰到麻烦，想从头再来，可以用下列命令清除配置： 123456ceph-deploy purgedata &#123;ceph-node&#125; [&#123;ceph-node&#125;]ceph-deploy forgetkeysrm -rf /etc/ceph/*rm -rf /var/lib/ceph/*/*rm -rf /var/log/ceph/*rm -rf /var/run/ceph/* 创建集群并准备配置12cd ceph-clusterceph-deploy new lab1 1234567891011121314151617[ceph-admin@lab1 ceph-cluster]$ ls -ltotal 24-rw-rw-r-- 1 ceph-admin ceph-admin 196 Aug 17 14:31 ceph.conf-rw-rw-r-- 1 ceph-admin ceph-admin 12759 Aug 17 14:31 ceph-deploy-ceph.log-rw------- 1 ceph-admin ceph-admin 73 Aug 17 14:31 ceph.mon.keyring[ceph-admin@lab1 ceph-cluster]$ more ceph.mon.keyring [mon.]key = AQC2a3ZbAAAAABAAor15nkYQCXuC681B/Q53og==caps mon = allow *[ceph-admin@lab1 ceph-cluster]$ more ceph.conf [global]fsid = fb212173-233c-4c5e-a98e-35be9359f8e2mon_initial_members = lab1mon_host = 192.168.105.92auth_cluster_required = cephxauth_service_required = cephxauth_client_required = cephx 把 Ceph 配置文件里的默认副本数从 3 改成 2 ，这样只有两个 OSD 也可以达到 active + clean 状态。把下面这行加入 [global] 段： osd pool default size = 2 再把 public network 写入 Ceph 配置文件的[global]段下 public network = 192.168.105.0/24 安装 Ceph ceph-deploy install lab1 lab4 lab5 --no-adjust-repos 配置monitor(s)并收集所有密钥： ceph-deploy mon create-initial 完成上述操作后，当前目录里应该会出现这些密钥环： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136[ceph-admin@lab1 ceph-cluster]$ ceph-deploy mon create-initial[ceph_deploy.conf][DEBUG ] found configuration file at: /home/ceph-admin/.cephdeploy.conf[ceph_deploy.cli][INFO ] Invoked (2.0.1): /bin/ceph-deploy mon create-initial[ceph_deploy.cli][INFO ] ceph-deploy options:[ceph_deploy.cli][INFO ] username : None[ceph_deploy.cli][INFO ] verbose : False[ceph_deploy.cli][INFO ] overwrite_conf : False[ceph_deploy.cli][INFO ] subcommand : create-initial[ceph_deploy.cli][INFO ] quiet : False[ceph_deploy.cli][INFO ] cd_conf : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x1160f38&gt;[ceph_deploy.cli][INFO ] cluster : ceph[ceph_deploy.cli][INFO ] func : &lt;function mon at 0x115c2a8&gt;[ceph_deploy.cli][INFO ] ceph_conf : None[ceph_deploy.cli][INFO ] default_release : False[ceph_deploy.cli][INFO ] keyrings : None[ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts lab1[ceph_deploy.mon][DEBUG ] detecting platform for host lab1 ...[lab1][DEBUG ] connection detected need for sudo[lab1][DEBUG ] connected to host: lab1 [lab1][DEBUG ] detect platform information from remote host[lab1][DEBUG ] detect machine type[lab1][DEBUG ] find the location of an executable[ceph_deploy.mon][INFO ] distro info: CentOS Linux 7.5.1804 Core[lab1][DEBUG ] determining if provided host has same hostname in remote[lab1][DEBUG ] get remote short hostname[lab1][DEBUG ] deploying mon to lab1[lab1][DEBUG ] get remote short hostname[lab1][DEBUG ] remote hostname: lab1[lab1][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf[lab1][DEBUG ] create the mon path if it does not exist[lab1][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-lab1/done[lab1][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-lab1/done[lab1][INFO ] creating keyring file: /var/lib/ceph/tmp/ceph-lab1.mon.keyring[lab1][DEBUG ] create the monitor keyring file[lab1][INFO ] Running command: sudo ceph-mon --cluster ceph --mkfs -i lab1 --keyring /var/lib/ceph/tmp/ceph-lab1.mon.keyring --setuser 167 --setgroup 167[lab1][INFO ] unlinking keyring file /var/lib/ceph/tmp/ceph-lab1.mon.keyring[lab1][DEBUG ] create a done file to avoid re-doing the mon deployment[lab1][DEBUG ] create the init path if it does not exist[lab1][INFO ] Running command: sudo systemctl enable ceph.target[lab1][INFO ] Running command: sudo systemctl enable ceph-mon@lab1[lab1][INFO ] Running command: sudo systemctl start ceph-mon@lab1[lab1][INFO ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.lab1.asok mon_status[lab1][DEBUG ] ********************************************************************************[lab1][DEBUG ] status for monitor: mon.lab1[lab1][DEBUG ] &#123;[lab1][DEBUG ] "election_epoch": 3, [lab1][DEBUG ] "extra_probe_peers": [], [lab1][DEBUG ] "feature_map": &#123;[lab1][DEBUG ] "mon": &#123;[lab1][DEBUG ] "group": &#123;[lab1][DEBUG ] "features": "0x3ffddff8eea4fffb", [lab1][DEBUG ] "num": 1, [lab1][DEBUG ] "release": "luminous"[lab1][DEBUG ] &#125;[lab1][DEBUG ] &#125;[lab1][DEBUG ] &#125;, [lab1][DEBUG ] "features": &#123;[lab1][DEBUG ] "quorum_con": "4611087853745930235", [lab1][DEBUG ] "quorum_mon": [[lab1][DEBUG ] "kraken", [lab1][DEBUG ] "luminous"[lab1][DEBUG ] ], [lab1][DEBUG ] "required_con": "153140804152475648", [lab1][DEBUG ] "required_mon": [[lab1][DEBUG ] "kraken", [lab1][DEBUG ] "luminous"[lab1][DEBUG ] ][lab1][DEBUG ] &#125;, [lab1][DEBUG ] "monmap": &#123;[lab1][DEBUG ] "created": "2018-08-17 14:46:18.770540", [lab1][DEBUG ] "epoch": 1, [lab1][DEBUG ] "features": &#123;[lab1][DEBUG ] "optional": [], [lab1][DEBUG ] "persistent": [[lab1][DEBUG ] "kraken", [lab1][DEBUG ] "luminous"[lab1][DEBUG ] ][lab1][DEBUG ] &#125;, [lab1][DEBUG ] "fsid": "fb212173-233c-4c5e-a98e-35be9359f8e2", [lab1][DEBUG ] "modified": "2018-08-17 14:46:18.770540", [lab1][DEBUG ] "mons": [[lab1][DEBUG ] &#123;[lab1][DEBUG ] "addr": "192.168.105.92:6789/0", [lab1][DEBUG ] "name": "lab1", [lab1][DEBUG ] "public_addr": "192.168.105.92:6789/0", [lab1][DEBUG ] "rank": 0[lab1][DEBUG ] &#125;[lab1][DEBUG ] ][lab1][DEBUG ] &#125;, [lab1][DEBUG ] "name": "lab1", [lab1][DEBUG ] "outside_quorum": [], [lab1][DEBUG ] "quorum": [[lab1][DEBUG ] 0[lab1][DEBUG ] ], [lab1][DEBUG ] "rank": 0, [lab1][DEBUG ] "state": "leader", [lab1][DEBUG ] "sync_provider": [][lab1][DEBUG ] &#125;[lab1][DEBUG ] ********************************************************************************[lab1][INFO ] monitor: mon.lab1 is running[lab1][INFO ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.lab1.asok mon_status[ceph_deploy.mon][INFO ] processing monitor mon.lab1[lab1][DEBUG ] connection detected need for sudo[lab1][DEBUG ] connected to host: lab1 [lab1][DEBUG ] detect platform information from remote host[lab1][DEBUG ] detect machine type[lab1][DEBUG ] find the location of an executable[lab1][INFO ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.lab1.asok mon_status[ceph_deploy.mon][INFO ] mon.lab1 monitor has reached quorum![ceph_deploy.mon][INFO ] all initial monitors are running and have formed quorum[ceph_deploy.mon][INFO ] Running gatherkeys...[ceph_deploy.gatherkeys][INFO ] Storing keys in temp directory /tmp/tmpfUkCWD[lab1][DEBUG ] connection detected need for sudo[lab1][DEBUG ] connected to host: lab1 [lab1][DEBUG ] detect platform information from remote host[lab1][DEBUG ] detect machine type[lab1][DEBUG ] get remote short hostname[lab1][DEBUG ] fetch remote file[lab1][INFO ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.lab1.asok mon_status[lab1][INFO ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-lab1/keyring auth get client.admin[lab1][INFO ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-lab1/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *[lab1][INFO ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-lab1/keyring auth get client.bootstrap-mds[lab1][INFO ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-lab1/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds[lab1][INFO ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-lab1/keyring auth get client.bootstrap-mgr[lab1][INFO ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-lab1/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr[lab1][INFO ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-lab1/keyring auth get client.bootstrap-osd[lab1][INFO ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-lab1/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd[lab1][INFO ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-lab1/keyring auth get client.bootstrap-rgw[lab1][INFO ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-lab1/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw[ceph_deploy.gatherkeys][INFO ] Storing ceph.client.admin.keyring[ceph_deploy.gatherkeys][INFO ] Storing ceph.bootstrap-mds.keyring[ceph_deploy.gatherkeys][INFO ] Storing ceph.bootstrap-mgr.keyring[ceph_deploy.gatherkeys][INFO ] keyring 'ceph.mon.keyring' already exists[ceph_deploy.gatherkeys][INFO ] Storing ceph.bootstrap-osd.keyring[ceph_deploy.gatherkeys][INFO ] Storing ceph.bootstrap-rgw.keyring[ceph_deploy.gatherkeys][INFO ] Destroy temp directory /tmp/tmpfUkCWD 用 ceph-deploy 把配置文件和 admin 密钥拷贝到管理节点和 Ceph 节点，这样你每次执行 Ceph 命令行时就无需指定 monitor 地址和ceph.client.admin.keyring了。 ceph-deploy admin lab1 lab4 lab5 注意ceph-deploy 和本地管理主机（ admin-node ）通信时，必须通过主机名可达。必要时可修改 /etc/hosts ，加入管理主机的名字。 确保你对 ceph.client.admin.keyring 有正确的操作权限。 sudo chmod +r /etc/ceph/ceph.client.admin.keyring 安装mrg 123ceph-deploy mgr create lab1ceph-deploy mgr create lab2ceph-deploy mgr create lab3 检查集群的健康状况。 12[ceph-admin@lab1 ceph-cluster]$ ceph healthHEALTH_OK 增加OSD列举磁盘并擦净 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364[ceph-admin@lab1 ceph-cluster]$ ceph-deploy disk list lab4[ceph_deploy.conf][DEBUG ] found configuration file at: /home/ceph-admin/.cephdeploy.conf[ceph_deploy.cli][INFO ] Invoked (2.0.1): /bin/ceph-deploy disk list lab4[ceph_deploy.cli][INFO ] ceph-deploy options:[ceph_deploy.cli][INFO ] username : None[ceph_deploy.cli][INFO ] verbose : False[ceph_deploy.cli][INFO ] debug : False[ceph_deploy.cli][INFO ] overwrite_conf : False[ceph_deploy.cli][INFO ] subcommand : list[ceph_deploy.cli][INFO ] quiet : False[ceph_deploy.cli][INFO ] cd_conf : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x20d97e8&gt;[ceph_deploy.cli][INFO ] cluster : ceph[ceph_deploy.cli][INFO ] host : ['lab4'][ceph_deploy.cli][INFO ] func : &lt;function disk at 0x20ca7d0&gt;[ceph_deploy.cli][INFO ] ceph_conf : None[ceph_deploy.cli][INFO ] default_release : False[lab4][DEBUG ] connection detected need for sudo[lab4][DEBUG ] connected to host: lab4 [lab4][DEBUG ] detect platform information from remote host[lab4][DEBUG ] detect machine type[lab4][DEBUG ] find the location of an executable[lab4][INFO ] Running command: sudo fdisk -l[lab4][INFO ] Disk /dev/sda: 107.4 GB, 107374182400 bytes, 209715200 sectors[lab4][INFO ] Disk /dev/sdb: 214.7 GB, 214748364800 bytes, 419430400 sectors[lab4][INFO ] Disk /dev/mapper/cl-root: 97.8 GB, 97840529408 bytes, 191094784 sectors[lab4][INFO ] Disk /dev/mapper/cl-swap: 8455 MB, 8455716864 bytes, 16515072 sectors[lab4][INFO ] Disk /dev/mapper/vg_a66945efa6324ffeb209d165cac8ede9-tp_1f4ce4f4bfb224aa385f35516236af43_tmeta: 12 MB, 12582912 bytes, 24576 sectors[lab4][INFO ] Disk /dev/mapper/vg_a66945efa6324ffeb209d165cac8ede9-tp_1f4ce4f4bfb224aa385f35516236af43_tdata: 2147 MB, 2147483648 bytes, 4194304 sectors[lab4][INFO ] Disk /dev/mapper/vg_a66945efa6324ffeb209d165cac8ede9-tp_1f4ce4f4bfb224aa385f35516236af43-tpool: 2147 MB, 2147483648 bytes, 4194304 sectors[lab4][INFO ] Disk /dev/mapper/vg_a66945efa6324ffeb209d165cac8ede9-tp_1f4ce4f4bfb224aa385f35516236af43: 2147 MB, 2147483648 bytes, 4194304 sectors[lab4][INFO ] Disk /dev/mapper/vg_a66945efa6324ffeb209d165cac8ede9-brick_1f4ce4f4bfb224aa385f35516236af43: 2147 MB, 2147483648 bytes, 4194304 sectors[ceph-admin@lab1 ceph-cluster]$ ceph-deploy disk zap lab4 /dev/sdb[ceph_deploy.conf][DEBUG ] found configuration file at: /home/ceph-admin/.cephdeploy.conf[ceph_deploy.cli][INFO ] Invoked (2.0.1): /bin/ceph-deploy disk zap lab4 /dev/sdb[ceph_deploy.cli][INFO ] ceph-deploy options:[ceph_deploy.cli][INFO ] username : None[ceph_deploy.cli][INFO ] verbose : False[ceph_deploy.cli][INFO ] debug : False[ceph_deploy.cli][INFO ] overwrite_conf : False[ceph_deploy.cli][INFO ] subcommand : zap[ceph_deploy.cli][INFO ] quiet : False[ceph_deploy.cli][INFO ] cd_conf : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0xd447e8&gt;[ceph_deploy.cli][INFO ] cluster : ceph[ceph_deploy.cli][INFO ] host : lab4[ceph_deploy.cli][INFO ] func : &lt;function disk at 0xd357d0&gt;[ceph_deploy.cli][INFO ] ceph_conf : None[ceph_deploy.cli][INFO ] default_release : False[ceph_deploy.cli][INFO ] disk : ['/dev/sdb'][ceph_deploy.osd][DEBUG ] zapping /dev/sdb on lab4[lab4][DEBUG ] connection detected need for sudo[lab4][DEBUG ] connected to host: lab4 [lab4][DEBUG ] detect platform information from remote host[lab4][DEBUG ] detect machine type[lab4][DEBUG ] find the location of an executable[ceph_deploy.osd][INFO ] Distro info: CentOS Linux 7.5.1804 Core[lab4][DEBUG ] zeroing last few blocks of device[lab4][DEBUG ] find the location of an executable[lab4][INFO ] Running command: sudo /usr/sbin/ceph-volume lvm zap /dev/sdb[lab4][DEBUG ] --&gt; Zapping: /dev/sdb[lab4][DEBUG ] Running command: /usr/sbin/cryptsetup status /dev/mapper/[lab4][DEBUG ] stdout: /dev/mapper/ is inactive.[lab4][DEBUG ] Running command: wipefs --all /dev/sdb[lab4][DEBUG ] Running command: dd if=/dev/zero of=/dev/sdb bs=1M count=10[lab4][DEBUG ] --&gt; Zapping successful for: /dev/sdb 同理，lab5的sdb也一样。 创建pv、vg、lv，略。 创建 OSD 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869[ceph-admin@lab1 ceph-cluster]$ ceph-deploy osd create lab4 --fs-type btrfs --data vg1/lvol0 [ceph_deploy.conf][DEBUG ] found configuration file at: /home/ceph-admin/.cephdeploy.conf[ceph_deploy.cli][INFO ] Invoked (2.0.1): /bin/ceph-deploy osd create lab4 --fs-type btrfs --data vg1/lvol0[ceph_deploy.cli][INFO ] ceph-deploy options:[ceph_deploy.cli][INFO ] verbose : False[ceph_deploy.cli][INFO ] bluestore : None[ceph_deploy.cli][INFO ] cd_conf : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x26d4908&gt;[ceph_deploy.cli][INFO ] cluster : ceph[ceph_deploy.cli][INFO ] fs_type : btrfs[ceph_deploy.cli][INFO ] block_wal : None[ceph_deploy.cli][INFO ] default_release : False[ceph_deploy.cli][INFO ] username : None[ceph_deploy.cli][INFO ] journal : None[ceph_deploy.cli][INFO ] subcommand : create[ceph_deploy.cli][INFO ] host : lab4[ceph_deploy.cli][INFO ] filestore : None[ceph_deploy.cli][INFO ] func : &lt;function osd at 0x26c4758&gt;[ceph_deploy.cli][INFO ] ceph_conf : None[ceph_deploy.cli][INFO ] zap_disk : False[ceph_deploy.cli][INFO ] data : vg1/lvol0[ceph_deploy.cli][INFO ] block_db : None[ceph_deploy.cli][INFO ] dmcrypt : False[ceph_deploy.cli][INFO ] overwrite_conf : False[ceph_deploy.cli][INFO ] dmcrypt_key_dir : /etc/ceph/dmcrypt-keys[ceph_deploy.cli][INFO ] quiet : False[ceph_deploy.cli][INFO ] debug : False[ceph_deploy.osd][DEBUG ] Creating OSD on cluster ceph with data device vg1/lvol0[lab4][DEBUG ] connection detected need for sudo[lab4][DEBUG ] connected to host: lab4 [lab4][DEBUG ] detect platform information from remote host[lab4][DEBUG ] detect machine type[lab4][DEBUG ] find the location of an executable[ceph_deploy.osd][INFO ] Distro info: CentOS Linux 7.5.1804 Core[ceph_deploy.osd][DEBUG ] Deploying osd to lab4[lab4][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf[lab4][DEBUG ] find the location of an executable[lab4][INFO ] Running command: sudo /usr/sbin/ceph-volume --cluster ceph lvm create --bluestore --data vg1/lvol0[lab4][DEBUG ] Running command: /bin/ceph-authtool --gen-print-key[lab4][DEBUG ] Running command: /bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring -i - osd new 7bb2b8f4-9e9d-4cd2-a2da-802a953a4d62[lab4][DEBUG ] Running command: /bin/ceph-authtool --gen-print-key[lab4][DEBUG ] Running command: mount -t tmpfs tmpfs /var/lib/ceph/osd/ceph-1[lab4][DEBUG ] Running command: chown -h ceph:ceph /dev/vg1/lvol0[lab4][DEBUG ] Running command: chown -R ceph:ceph /dev/dm-2[lab4][DEBUG ] Running command: ln -s /dev/vg1/lvol0 /var/lib/ceph/osd/ceph-1/block[lab4][DEBUG ] Running command: ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /var/lib/ceph/osd/ceph-1/activate.monmap[lab4][DEBUG ] stderr: got monmap epoch 1[lab4][DEBUG ] Running command: ceph-authtool /var/lib/ceph/osd/ceph-1/keyring --create-keyring --name osd.1 --add-key AQAmjHZbiiGUChAAVCWdPZqHms99mLgSZ7M+fQ==[lab4][DEBUG ] stdout: creating /var/lib/ceph/osd/ceph-1/keyring[lab4][DEBUG ] added entity osd.1 auth auth(auid = 18446744073709551615 key=AQAmjHZbiiGUChAAVCWdPZqHms99mLgSZ7M+fQ== with 0 caps)[lab4][DEBUG ] Running command: chown -R ceph:ceph /var/lib/ceph/osd/ceph-1/keyring[lab4][DEBUG ] Running command: chown -R ceph:ceph /var/lib/ceph/osd/ceph-1/[lab4][DEBUG ] Running command: /bin/ceph-osd --cluster ceph --osd-objectstore bluestore --mkfs -i 1 --monmap /var/lib/ceph/osd/ceph-1/activate.monmap --keyfile - --osd-data /var/lib/ceph/osd/ceph-1/ --osd-uuid 7bb2b8f4-9e9d-4cd2-a2da-802a953a4d62 --setuser ceph --setgroup ceph[lab4][DEBUG ] --&gt; ceph-volume lvm prepare successful for: vg1/lvol0[lab4][DEBUG ] Running command: ceph-bluestore-tool --cluster=ceph prime-osd-dir --dev /dev/vg1/lvol0 --path /var/lib/ceph/osd/ceph-1[lab4][DEBUG ] Running command: ln -snf /dev/vg1/lvol0 /var/lib/ceph/osd/ceph-1/block[lab4][DEBUG ] Running command: chown -h ceph:ceph /var/lib/ceph/osd/ceph-1/block[lab4][DEBUG ] Running command: chown -R ceph:ceph /dev/dm-2[lab4][DEBUG ] Running command: chown -R ceph:ceph /var/lib/ceph/osd/ceph-1[lab4][DEBUG ] Running command: systemctl enable ceph-volume@lvm-1-7bb2b8f4-9e9d-4cd2-a2da-802a953a4d62[lab4][DEBUG ] stderr: Created symlink from /etc/systemd/system/multi-user.target.wants/ceph-volume@lvm-1-7bb2b8f4-9e9d-4cd2-a2da-802a953a4d62.service to /usr/lib/systemd/system/ceph-volume@.service.[lab4][DEBUG ] Running command: systemctl start ceph-osd@1[lab4][DEBUG ] --&gt; ceph-volume lvm activate successful for osd ID: 1[lab4][DEBUG ] --&gt; ceph-volume lvm create successful for: vg1/lvol0[lab4][INFO ] checking OSD status...[lab4][DEBUG ] find the location of an executable[lab4][INFO ] Running command: sudo /bin/ceph --cluster=ceph osd stat --format=json[lab4][WARNIN] there is 1 OSD down[lab4][WARNIN] there is 1 OSD out[ceph_deploy.osd][DEBUG ] Host lab4 is now ready for osd use. ceph-deploy osd create lab5 --fs-type btrfs --data vg1/lvol0 扩展集群一个基本的集群启动并开始运行后，下一步就是扩展集群。在 lab6、lab7 各上添加一个 OSD 守护进程和一个元数据服务器。然后分别在 lab2 和 lab3 上添加 Ceph Monitor，以形成 Monitors 的法定人数。 添加 MONITORS 12ceph-deploy mon add lab2ceph-deploy mon add lab3 过程： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125[ceph-admin@lab1 ceph-cluster]$ ceph-deploy mon add lab3[ceph_deploy.conf][DEBUG ] found configuration file at: /home/ceph-admin/.cephdeploy.conf[ceph_deploy.cli][INFO ] Invoked (2.0.1): /bin/ceph-deploy mon add lab3[ceph_deploy.cli][INFO ] ceph-deploy options:[ceph_deploy.cli][INFO ] username : None[ceph_deploy.cli][INFO ] verbose : False[ceph_deploy.cli][INFO ] overwrite_conf : False[ceph_deploy.cli][INFO ] subcommand : add[ceph_deploy.cli][INFO ] quiet : False[ceph_deploy.cli][INFO ] cd_conf : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x29f0f38&gt;[ceph_deploy.cli][INFO ] cluster : ceph[ceph_deploy.cli][INFO ] mon : ['lab3'][ceph_deploy.cli][INFO ] func : &lt;function mon at 0x29ec2a8&gt;[ceph_deploy.cli][INFO ] address : None[ceph_deploy.cli][INFO ] ceph_conf : None[ceph_deploy.cli][INFO ] default_release : False[ceph_deploy.mon][INFO ] ensuring configuration of new mon host: lab3[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to lab3[lab3][DEBUG ] connection detected need for sudo[lab3][DEBUG ] connected to host: lab3 [lab3][DEBUG ] detect platform information from remote host[lab3][DEBUG ] detect machine type[lab3][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf[ceph_deploy.mon][DEBUG ] Adding mon to cluster ceph, host lab3[ceph_deploy.mon][DEBUG ] using mon address by resolving host: 192.168.105.94[ceph_deploy.mon][DEBUG ] detecting platform for host lab3 ...[lab3][DEBUG ] connection detected need for sudo[lab3][DEBUG ] connected to host: lab3 [lab3][DEBUG ] detect platform information from remote host[lab3][DEBUG ] detect machine type[lab3][DEBUG ] find the location of an executable[ceph_deploy.mon][INFO ] distro info: CentOS Linux 7.5.1804 Core[lab3][DEBUG ] determining if provided host has same hostname in remote[lab3][DEBUG ] get remote short hostname[lab3][DEBUG ] adding mon to lab3[lab3][DEBUG ] get remote short hostname[lab3][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf[lab3][DEBUG ] create the mon path if it does not exist[lab3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-lab3/done[lab3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-lab3/done[lab3][INFO ] creating keyring file: /var/lib/ceph/tmp/ceph-lab3.mon.keyring[lab3][DEBUG ] create the monitor keyring file[lab3][INFO ] Running command: sudo ceph --cluster ceph mon getmap -o /var/lib/ceph/tmp/ceph.lab3.monmap[lab3][WARNIN] got monmap epoch 2[lab3][INFO ] Running command: sudo ceph-mon --cluster ceph --mkfs -i lab3 --monmap /var/lib/ceph/tmp/ceph.lab3.monmap --keyring /var/lib/ceph/tmp/ceph-lab3.mon.keyring --setuser 167 --setgroup 167[lab3][INFO ] unlinking keyring file /var/lib/ceph/tmp/ceph-lab3.mon.keyring[lab3][DEBUG ] create a done file to avoid re-doing the mon deployment[lab3][DEBUG ] create the init path if it does not exist[lab3][INFO ] Running command: sudo systemctl enable ceph.target[lab3][INFO ] Running command: sudo systemctl enable ceph-mon@lab3[lab3][WARNIN] Created symlink from /etc/systemd/system/ceph-mon.target.wants/ceph-mon@lab3.service to /usr/lib/systemd/system/ceph-mon@.service.[lab3][INFO ] Running command: sudo systemctl start ceph-mon@lab3[lab3][INFO ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.lab3.asok mon_status[lab3][WARNIN] lab3 is not defined in `mon initial members`[lab3][WARNIN] monitor lab3 does not exist in monmap[lab3][INFO ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.lab3.asok mon_status[lab3][DEBUG ] ********************************************************************************[lab3][DEBUG ] status for monitor: mon.lab3[lab3][DEBUG ] &#123;[lab3][DEBUG ] "election_epoch": 0, [lab3][DEBUG ] "extra_probe_peers": [[lab3][DEBUG ] "192.168.105.93:6789/0"[lab3][DEBUG ] ], [lab3][DEBUG ] "feature_map": &#123;[lab3][DEBUG ] "mon": &#123;[lab3][DEBUG ] "group": &#123;[lab3][DEBUG ] "features": "0x3ffddff8eea4fffb", [lab3][DEBUG ] "num": 1, [lab3][DEBUG ] "release": "luminous"[lab3][DEBUG ] &#125;[lab3][DEBUG ] &#125;[lab3][DEBUG ] &#125;, [lab3][DEBUG ] "features": &#123;[lab3][DEBUG ] "quorum_con": "0", [lab3][DEBUG ] "quorum_mon": [], [lab3][DEBUG ] "required_con": "144115188077969408", [lab3][DEBUG ] "required_mon": [[lab3][DEBUG ] "kraken", [lab3][DEBUG ] "luminous"[lab3][DEBUG ] ][lab3][DEBUG ] &#125;, [lab3][DEBUG ] "monmap": &#123;[lab3][DEBUG ] "created": "2018-08-17 16:38:21.075805", [lab3][DEBUG ] "epoch": 3, [lab3][DEBUG ] "features": &#123;[lab3][DEBUG ] "optional": [], [lab3][DEBUG ] "persistent": [[lab3][DEBUG ] "kraken", [lab3][DEBUG ] "luminous"[lab3][DEBUG ] ][lab3][DEBUG ] &#125;, [lab3][DEBUG ] "fsid": "4395328d-17fc-4039-96d0-1d3241a4cafa", [lab3][DEBUG ] "modified": "2018-08-17 17:58:23.179585", [lab3][DEBUG ] "mons": [[lab3][DEBUG ] &#123;[lab3][DEBUG ] "addr": "192.168.105.92:6789/0", [lab3][DEBUG ] "name": "lab1", [lab3][DEBUG ] "public_addr": "192.168.105.92:6789/0", [lab3][DEBUG ] "rank": 0[lab3][DEBUG ] &#125;, [lab3][DEBUG ] &#123;[lab3][DEBUG ] "addr": "192.168.105.93:6789/0", [lab3][DEBUG ] "name": "lab2", [lab3][DEBUG ] "public_addr": "192.168.105.93:6789/0", [lab3][DEBUG ] "rank": 1[lab3][DEBUG ] &#125;, [lab3][DEBUG ] &#123;[lab3][DEBUG ] "addr": "192.168.105.94:6789/0", [lab3][DEBUG ] "name": "lab3", [lab3][DEBUG ] "public_addr": "192.168.105.94:6789/0", [lab3][DEBUG ] "rank": 2[lab3][DEBUG ] &#125;[lab3][DEBUG ] ][lab3][DEBUG ] &#125;, [lab3][DEBUG ] "name": "lab3", [lab3][DEBUG ] "outside_quorum": [[lab3][DEBUG ] "lab3"[lab3][DEBUG ] ], [lab3][DEBUG ] "quorum": [], [lab3][DEBUG ] "rank": 2, [lab3][DEBUG ] "state": "probing", [lab3][DEBUG ] "sync_provider": [][lab3][DEBUG ] &#125;[lab3][DEBUG ] ********************************************************************************[lab3][INFO ] monitor: mon.lab3 is running 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849[ceph-admin@lab1 ceph-cluster]$ ceph quorum_status --format json-pretty&#123; "election_epoch": 12, "quorum": [ 0, 1, 2 ], "quorum_names": [ "lab1", "lab2", "lab3" ], "quorum_leader_name": "lab1", "monmap": &#123; "epoch": 3, "fsid": "4395328d-17fc-4039-96d0-1d3241a4cafa", "modified": "2018-08-17 17:58:23.179585", "created": "2018-08-17 16:38:21.075805", "features": &#123; "persistent": [ "kraken", "luminous" ], "optional": [] &#125;, "mons": [ &#123; "rank": 0, "name": "lab1", "addr": "192.168.105.92:6789/0", "public_addr": "192.168.105.92:6789/0" &#125;, &#123; "rank": 1, "name": "lab2", "addr": "192.168.105.93:6789/0", "public_addr": "192.168.105.93:6789/0" &#125;, &#123; "rank": 2, "name": "lab3", "addr": "192.168.105.94:6789/0", "public_addr": "192.168.105.94:6789/0" &#125; ] &#125;&#125; 添加元数据服务器 至少需要一个元数据服务器才能使用 CephFS ，执行下列命令创建元数据服务器： ceph-deploy mds create lab4 到此，可以创建RBD和cephFS的ceph集群搭建完成。 Ceph使用技巧推送配置文件 只推送配置文件 ceph-deploy --overwrite-conf config push lab1 lab2 推送配置文件和client.admin key ceph-deploy admin lab1 lab2 查看状态的常用命令 集群状态 ceph -s 查看正在操作的动作 ceph -w 查看已经创建的磁盘 rbd ls -l 查看ceph集群 ceph osd tree 查看ceph授权信息 ceph auth get client.admin 移除monitor节点 ceph-deploy mon destroy lab1 详细列出集群每块磁盘的使用情况 ceph osd df 检查 MDS 状态: ceph mds stat 开启Dashbord管理界面 创建管理域密钥ceph auth get-or-create mgr.lab1 mon &#39;allow profile mgr&#39; osd &#39;allow *&#39; mds &#39;allow *&#39;或：ceph auth get-key client.admin | base64 开启 ceph-mgr 管理域 ceph-mgr -i master 开启dashboard ceph mgr module enable dashboard 绑定开启 dashboard 模块的 ceph-mgr 节点的 ip 地址 ceph config-key set mgr/dashboard/master/server_addr 192.168.105.92 dashboard 默认运行在7000端口 RBD常用命令 12345678910111213141516创建pool若少于5个OSD， 设置pg_num为128。5~10个OSD，设置pg_num为512。10~50个OSD，设置pg_num为4096。超过50个OSD，可以参考pgcalc计算。ceph osd pool create rbd 128 128 rbd pool init rbd删除poolceph osd pool rm rbd rbd –yes-i-really-really-mean-it ## ceph.conf 添加 ## mon_allow_pool_delete = true手动创建一个rbd磁盘rbd create --image-feature layering [rbd-name] -s 10240 OSD常用命令 清除磁盘上的逻辑卷 ceph-volume lvm zap --destroy /dev/vdc # 本机操作ceph-deploy disk zap lab4 /dev/sdb # 远程操作 创建osd ceph-deploy osd create lab4 --fs-type btrfs --data vg1/lvol0 删除osd节点的node4 查看节点node4上的所有osd，比如osd.9 osd.10： ceph osd tree #查看目前cluster状态 把node4上的所欲osd踢出集群：（node1节点上执行） ceph osd out osd.9ceph osd out osd.10 让node4上的所有osd停止工作：（node4上执行） service ceph stop osd.9service ceph stop osd.10 查看node4上osd的状态是否为down，权重为0 ceph osd tree 移除node4上的所有osd： ceph osd crush remove osd.9ceph osd crush remove osd.10 删除节点node4： ceph osd crush remove ceph-node4 替换一个失效的磁盘驱动 首先ceph osd tree 查看down掉的osd，将因盘问题down掉的osd及相关key删除 1234ceph osd out osd.0 # 都在node1节点下执行ceph osd crush rm osd.0ceph auth del osd.0ceph osd rm osd.0 zap新磁盘 清理新磁盘：ceph-deploy disk zap node1 /dev/sdb在磁盘上新建一个osd，ceph会把它添加为osd:0：ceph-deploy --overwrite-conf osd create node1 /dev/sdb 参考资料[1] http://docs.ceph.com/docs/master/start/[2] http://docs.ceph.org.cn/start/[3] http://docs.ceph.org.cn/install/manual-deployment/[4] http://www.cnblogs.com/freedom314/p/9247602.html[5] http://docs.ceph.org.cn/rados/operations/monitoring/]]></content>
      <categories>
        <category>Ceph</category>
      </categories>
      <tags>
        <tag>Ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph单节点部署]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2FCeph%E5%8D%95%E8%8A%82%E7%82%B9%E9%83%A8%E7%BD%B2%2F</url>
    <content type="text"><![CDATA[Ceph集群单节点安装创建虚拟机（正常步骤） 添加ceph.repo，安装ceph-deploy 1234567891011121314151617181920212223cat /etc/yum.repos.d/ceph.repo[Ceph]name=Ceph packages for $basearchbaseurl=http://hk.ceph.com/rpm-jewel/el7/$basearchenabled=1gpgcheck=0[Ceph-noarch]name=Ceph noarch packagesbaseurl=http://hk.ceph.com/rpm-jewel/el7/noarchenabled=1gpgcheck=0[ceph-source]name=Ceph source packagesbaseurl=http://hk.ceph.com/rpm-jewel/el7/SRPMSenabled=1gpgcheck=0# 安装ceph-deployyum clean allyum install ceph-deploy 创建集群，并安装ceph相关软件包 1234567891011121314151617181920212223242526272829303132333435363738394041# 创建目录mkdir -p /root/cluster &amp; cd /root/cluster &amp; rm -f /root/cluster/*# 进入目录，创建集群cd /root/cluster# 安装依赖yum install -y yum-utils &amp;&amp; yum-config-manager --add-repo https://dl.fedoraproject.org/pub/epel/7/x86_64/ &amp;&amp; yum install --nogpgcheck -y epel-release &amp;&amp; rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 &amp;&amp; rm -f /etc/yum.repos.d/dl.fedoraproject.org*# 这一步非常重要，如果跳过这一步，直接进行ceph的安装，可能会报如下的错误：Error: Package: 1:ceph-common-10.2.10-0.el7.x86_64 (Ceph)​ Requires: libbabeltrace.so.1()(64bit)Error: Package: 1:librados2-10.2.10-0.el7.x86_64 (Ceph)​ Requires: liblttng-ust.so.0()(64bit)Error: Package: 1:librgw2-10.2.10-0.el7.x86_64 (Ceph)​ Requires: libfcgi.so.0()(64bit)Error: Package: 1:librbd1-10.2.10-0.el7.x86_64 (Ceph)​ Requires: liblttng-ust.so.0()(64bit)Error: Package: 1:ceph-common-10.2.10-0.el7.x86_64 (Ceph)​ Requires: libbabeltrace-ctf.so.1()(64bit)# 安装ceph软件包yum install ceph ceph-radosgw# 虚机准备工作sed -i 's/SELINUX=.*/SELINUX=disabled/' /etc/selinux/configsetenforce 0systemctl stop firewalldsystemctl disable firewalld# swapoff -a# 确认ceph软件包版本ceph-deploy --versionceph -v# 开始部署ceph-deploy new $HOSTNAME 修改配置文件，配置初始化 123456789101112131415161718192021222324# osd的pool副本数size为1，可设置echo osd pool default size = 1 &gt;&gt; ceph.conf echo osd crush chooseleaf type = 0 &gt;&gt; ceph.confecho osd max object name len = 256 &gt;&gt; ceph.confecho osd journal size = 128 &gt;&gt; ceph.conf# ceph.conf配置cat ceph.conf[global]fsid = 90602ee8-b900-44f2-b218-9dd4fc8273afmon_initial_members = ceph-nodemon_host = 10.100.2.36auth_cluster_required = cephxauth_service_required = cephxauth_client_required = cephxosd pool default size = 1 osd crush chooseleaf type = 0osd max object name len = 256osd journal size = 128# monitor初始化ceph-deploy mon create-initial 添加osd 123456789101112# 创建文件夹，添加osdmkdir -p /var/run/ceph/chown ceph:ceph /var/run/ceph/mkdir -p /osd &amp; rm -rf /osd/*chown ceph:ceph /osdceph-deploy mon create-initialceph-deploy osd prepare $HOSTNAME:/osdceph-deploy osd activate $HOSTNAME:/osd# 验证ceph状态是否健康ceph -s Ceph开启CephFS挂载安装前确定集群状态是正常的 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748ceph -s​ cluster 817696dd-7d7d-459d-8ce3-314326f2c056​ health HEALTH_OK​ monmap e1: 1 mons at &#123;ceph-node=10.100.2.36:6789/0&#125;​ election epoch 3, quorum 0 ceph-node​ fsmap e5: 1/1/1 up &#123;0=ceph-node=up:active&#125;​ osdmap e10: 1 osds: 1 up, 1 in​ flags sortbitwise,require_jewel_osds​ pgmap v10806: 192 pgs, 3 pools, 2068 bytes data, 20 objects​ 1681 MB used, 49493 MB / 51175 MB avail​ 192 active+clean# cephfs需要启用mds元数据数据服务，检查元数据服务是否创建ceph mds stat# 如下则为正常ceph mds stat# 正常显示为 e5: 1/1/1 up &#123;0=ceph-node=up:active&#125;# 全为0的话，则添加mds节点ceph-deploy mds create ceph-node# 创建cephfs文件系统ceph osd pool create cephfs_data 64ceph osd pool create cephfs_metadata 64ceph fs new cephfs cephfs_metadata cephfs_data# 挂载，这里默认使用admin用户；可自己创建用户挂载mkdir /mnt/cephfsmount -t ceph 10.100.2.36:6789:/ /aseit-data/cephfs -o name=admin,secret=AQBM82RdI6TAExAAoix7KkMBtSPDSh6ZG69iHg==# secret在/etc/ceph/ceph.client.admin.keyring的base64# 可创建client用户来使用# 验证df -h### 安装以及挂载cephfs时出现的问题：# 1.mount error 5 = Input/output error# 2. mount error 22 = Invalid argument# 第一个，首先先查mds服务是正常，不存在则添加，即ceph-deploy mds create ceph-node# 第二个，密钥不正确，检查密钥，即-o name=admin,secret=AQBM82RdI6TAExAAoix7KkMBtSPDSh6ZG69iHg==挂载时使用# 下面为卸载ceph操作ps aux|grep ceph |awk '&#123;print $2&#125;'|xargs kill -9ps -ef|grep cephceph-deploy uninstall [&#123;ceph-node&#125;] # 卸载所有ceph程序ceph-deploy purge [[ceph-node&#125; [&#123;ceph-node&#125;] #删除ceph相关的包ceph-deploy purgedata &#123;ceph-node&#125; [&#123;ceph-node&#125;] # 删除ceph相关的包ceph-deploy forgetkeys 附录:单节点安装ceph脚本12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152#!/bin/bash#echo 10.100.2.36 $HOSTNAME &gt;&gt; /etc/hostsyum clean all#rm -rf /etc/yum.repos.d/*.repo#wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo#wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo#sed -i '/aliyuncs/d' /etc/yum.repos.d/CentOS-Base.repo#sed -i 's/$releasever/7/g' /etc/yum.repos.d/CentOS-Base.repo#sed -i '/aliyuncs/d' /etc/yum.repos.d/epel.repo#echo "#[ceph]#name=ceph#baseurl=https://mirrors.aliyun.com/ceph/rpm-jewel/el7/x86_64/#gpgcheck=0#[ceph-noarch]#name=cephnoarch#baseurl=https://mirrors.aliyun.com/ceph/rpm-jewel/el7/noarch/#gpgcheck=0#" &gt; /etc/yum.repos.d/ceph.repoyum install -y yum-utils &amp;&amp; yum-config-manager --add-repo https://dl.fedoraproject.org/pub/epel/7/x86_64/ &amp;&amp; yum install --nogpgcheck -y epel-release &amp;&amp; rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 &amp;&amp; rm -f /etc/yum.repos.d/dl.fedoraproject.org*yum install ceph ceph-radosgw ceph-deploy -ymkdir -p /root/cluster &amp; cd /root/cluster &amp; rm -f /root/cluster/*cd /root/clustersed -i 's/SELINUX=.*/SELINUX=disabled/' /etc/selinux/configsetenforce 0systemctl stop firewalldsystemctl disable firewalld#swapoff -aceph-deploy new $HOSTNAMEecho osd pool default size = 1 &gt;&gt; ceph.confecho osd crush chooseleaf type = 0 &gt;&gt; ceph.confecho osd max object name len = 256 &gt;&gt; ceph.confecho osd journal size = 128 &gt;&gt; ceph.confmkdir -p /var/run/ceph/chown ceph:ceph /var/run/ceph/mkdir -p /osd &amp; rm -rf /osd/*chown ceph:ceph /osdceph-deploy mon create-initialceph-deploy osd prepare $HOSTNAME:/osdceph-deploy osd activate $HOSTNAME:/osdceph -s ceph重装清理脚本123456789101112131415\#!/bin/bashps aux|grep ceph |awk '&#123;print $2&#125;'|xargs kill -9ps -ef|grep cephumount /var/lib/ceph/osd/*rm -rf /var/lib/ceph/osd/*rm -rf /var/lib/ceph/mon/*rm -rf /var/lib/ceph/mds/*rm -rf /var/lib/ceph/bootstrap-mds/*rm -rf /var/lib/ceph/bootstrap-osd/*rm -rf /var/lib/ceph/bootstrap-mon/*rm -rf /var/lib/ceph/tmp/*rm -rf /etc/ceph/*rm -rf /var/run/ceph/*]]></content>
      <categories>
        <category>Ceph</category>
      </categories>
      <tags>
        <tag>Ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Ceph相关概念]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2FCeph%E7%9B%B8%E5%85%B3%E6%A6%82%E5%BF%B5%2F</url>
    <content type="text"><![CDATA[介绍Ceph是一个统一的分布式存储系统，最早起源于Sage就读博士期间的工作（最早的成果于2004年发表），随后贡献给开源社区。其设计初衷是提供较好的性能、可靠性和可扩展性。在经过多年的发展之后，目前已得到众多云计算厂商的支持并被广泛应用。RedHat及OpenStack都可与Ceph整合以支持虚拟机镜像的后端存储。 Ceph的优势可以概括为以下四个方面： 高性能 摒弃了传统的集中式存储元数据寻址的方案，采用CRUSH算法，数据分布均衡，并行度高 考虑了容灾域的隔离，能够实现各类负载的副本放置规则，例如跨机房、机架感知等 能够支持上千个存储节点的规模。支持TB到PB级的数据 高可用 副本数可以灵活控制 支持故障域分隔，数据强一致性 多种故障场景自动进行修复自愈 没有单点故障，自动管理 高扩展性 去中心化 扩展灵活 随着节点增加，性能线性增长 特性丰富 支持三种存储接口：对象存储，块设备存储，文件存储 支持自定义接口，支持多种语言驱动 Ceph基本结构Ceph的基本组成结构如下图： Ceph的底层是RADOS，RADOS本身也是分布式存储系统，Ceph所有的存储功能都是基于RADOS实现的。RADOS采用C++开发，所提供的原生Librados API包括C和C++两种。Ceph的上层应用调用本机上的librados API，再由后者通过socket与RADOS集群中的其他节点通信并完成各种操作。 RADOS GateWay、RBD其作用是在librados库的基础上提供抽象层次更高、更便于应用或客户端使用的上层接口。其中RADOS GW是一个提供与Amazon S3和Swift兼容的RESTful API的gateway，以供相应的对象存储应用开发使用。RBD则提供了一个标准的块设备接口，常用于在虚拟化的场景下为虚拟机创建volume。目前，RedHat已经将RBD驱动集成在KVM/QEMU中，以提供虚拟机访问性能。这两种方式目前在云计算中应用的比较多。 CephFS则提供了POSIX接口，用户可直接通过客户端挂载使用。它是内核态的程序，所有无需调用用户空间的librados库。它通过内核中的net模块来与RADOS进行交互。 Ceph基本组件及概念Ceph基本组件 如上图所示，Ceph主要有三个基本进程： OSD 用于集群中所有数据与对象的存储。处理集群数据的复制、恢复、回填、再均衡。并向其他osd守护进程发送心跳，然后向Mon提供一些监控信息。 当Ceph存储集群设定的数据有两个副本时（一共存两份），则至少需要两个OSD守护进程，即两个OSD节点，集群才能到达active+clean状态。 MDS(可选) 为Ceph文件系统提供元数据计算、缓存与同步。在Ceph中，元数据也是存储在osd节点中的，mds类似于元数据的代理缓存服务器。MDS进程并不是必须的进程，只有需要使用CephFS时，才需要配置MDS节点。 Monitor 监控整个集群的状态，维护集群的cluster MAP二进制表，保证集群数据的一致性。ClusterMAP描述了对象块存储的物理位置，以及一个将设备聚合到物理位置的桶列表。 概念 Object Ceph最底层的存储单元，每个Object包含元数据和原始数据。 PG 全称Placement Groups，是一个逻辑的概念，一个PG包含多个OSD。引入PG这一层其实是为了更好的分配数据和定位数据。 RADOS 全称Reliable Autonomic Distributed Object Store，是Ceph集群的精华，用户实现数据分配、Failover等集群操作。 Librados Librados是RADOS提供的库。因为RADOS很难直接访问，因此上层的RBD、RGW和CephFS都是通过Librados访问的。 CRUSH 是Ceph使用的数据分布算法，让数据分配到预期的地方。 RBD 全称RADOS block device，是Ceph对外提供的块设备服务。 RGW 全称RADOS GateWay，是Ceph对外提供的对象存储服务，接口与S3、Swift兼容。 CephFS 全称Ceph File System，是Ceph对外提供的文件系统服务。 Ceph 核心组件Ceph的核心组件包括Ceph OSD、Ceph Monitor和Ceph MSD。 OSD首先介绍下Ceph数据的存储过程，如下图： 无论使用哪种存储方式（对象、块、文件系统），存储的数据都会被切分成对象（Objects）。Objects size大小可以由管理员调整，通常为2M或者4M。每个对象都会有一个唯一的OID，有ino和ono生成：ino是文件的File ID，用于全局唯一标示每一个文件，而ono是分片的编号。例如，一个文件FileID是A，它被切成两个对象，一个编号为0，另一个编号为1，那么这两个对象的OID则为A0和A1。OID的好处是可以唯一标示每一个不同的对象，并且存储了对象与文件的从属关系。由于Ceph的所有数据都虚拟成了整齐划一的对象，所以在读写时效率都会比较高。 但是对象并不会直接存储进OSD中，因为对象的size很小，在一个大规模的集群中可能有几百到几千万个对象。如此多的对象光是遍历寻址速度都会很缓慢；而且如果将对象直接通过某种固定映射的哈希算法映射到osd上，当这个osd损坏时，对象无法自动迁移到其他osd上面（因为映射函数不允许）。为了解决这些问题，Ceph引入了归置组的概念，即PG。 PG是一个逻辑概念，Linux系统中可以直接看到对象，但是无法直接看到PG。它在数据寻址时类似于数据库中的索引：每个对象都会固定映射进一个PG中，所以当我们寻找一个对象时，只需要先找到对象所属的PG，然后遍历这个PG就可以了，无需遍历所有对象。而且在数据迁移时，也是一PG作为基本单位进行迁移，Ceph不会直接操作对象。 对象是如何映射进PG的？首先使用静态hash函数对OID做hash取出特征码，用特征码与PG的数量取模，得到的序号就是PGID。由于这种设计，PG的数量多寡直接决定了数据分布的均匀性，所以合理设置的PG数量可以很好地提升Ceph集群的性能并使数据均匀分布。 最后PG会根据管理员设置的副本数量进行复制，然后通过CRUSH算法存储到不同的OSD节点上（其实是把PG中的所有对象存储到节点上），第一个osd节点即为主节点，其余均为从节点。 下面是一段Ceph中的伪代码，简要描述了Ceph的数据存储流程： 123456locator = object_nameobj_hash = hash(locator)pg = obj_hash % num_pgosds_for_pg = crush(pg) # return a list of osdsprimary = osds_for_pg[0]replicas = osds_for_og[1:] 上图中更好地诠释了Ceph数据流的存储过程，数据无论是从三个接口哪一种写入的，最终都要切分成对象存储到底层的RADOS中。逻辑上通过算法先映射到PG上，最终存储进OSD节点里。图中除了之前介绍过的概念之外多了个pools的概念。 Pool是管理员自定义的命名空间，像其他的命名空间一样，用来隔离对象与PG。我们在调用API存储，即使用对象存储时，需要指定对象要存储进哪一个POOL中。除了隔离数据，我们也可以分别对不同的POOL设置不同的优化策略，比如副本数、数据清洗次数、数据块及对象大小等。 OSD是强一致性的分布式存储，它的读写流程如下图： Ceph的读写操作采用主从模型，客户端要读写数据时，只能向对象所对应的的主OSD节点发起请求。主节点在接受写请求时，会同步的向从OSD中写入数据。当所有的OSD节点都写入完成后，主节点才会向客户端报告写入完成的信息，因此保证了主从节点数据的高度一致性。而读取的时候，客户端也只会向主OSD节点发送读请求，并不会有类似数据库中读写分离的情况出现，这也是处于强一致性的考虑。由于所有写操作都要交给主OSD节点来处理，所以在数据量很大的时候，性能可能会比较慢，为了克服这个问题以及让Ceph能支持事务，每个OSD节点都包含一个journal文件，稍后介绍。 数据流向的介绍就先到这，现在回到正题：OSD进程。在Ceph中，每一个OSD进程都可以称作是一个OSD节点，也就是说，每台存储服务器可靠包含了众多的OSD节点，每个OSD节点监听不同的端口，类似于在同一台服务器上跑多个MySQL或Redis。每个OSD节点可以设置一个目录作为实际存储区域，也可以是一个分区、一整块硬盘。如下图，当前这台机器上跑了两个OSD进程，每个OSD监听4个端口，分别用于接收客户请求、传输数据、发送心跳、同步数据等操作。 如上图所示，osd节点默认监听tcp的6800到6803端口，如果同一台服务器上有多个OSD节点，则依次往后排序。 在生成环境中OSD最少可能都有上百个，所以每个OSD都有一个全局的编号，类似OSD0、OSD1、OSD2等等，序号根据OSD诞生的顺序排列，并且是全局唯一的。存储了相同PG的OSD节点除了想Mon节点发送心跳外，还会互相发送心跳信息以检测PG数据副本是否正常。 之前在介绍数据流向时说过，每个OSD节点都包含一个journal文件，如下图： 默认大小为5G，也就是说创建一个OSD节点，还没使用就要被journal占用5G的空间。这个值是可以调整的，具体大小要依据OSD的总大小而定。 Journal的作用类似于MySQL innodb引擎中的事务日志系统。当有突发的大量写入操作时，Ceph可以先把一些零散的，随机的IO请求保存到缓存中进行合并，然后在同一向内核发起IO请求。这样做效率会比较高，但是一旦OSD节点崩溃，缓存中的数据就会丢失，所以数据在还未写进硬盘时，都会记录到journal中，当OSD崩溃后重新启东市，会自动尝试从journal恢复因崩溃而丢失的缓存数据。因此journal的IO是非常密集的，而且由于一个数据IO两次，很大程度上也损耗了硬件的IO性能，所以通常在生产环境中，使用ssd来单独存储journal文件以提高Ceph的读写性能。 monitor节点Mon节点监控着整个Ceph集群的状态信息，监听于tcp的6789端口。每个Ceph集群中至少要有一个Mon节点，官方推荐每个集群至少部署三台。Mon节点中保存了最新的版本集群数据分布图（Cluster Map）的主副本。客户端在使用时，需要挂载Mon节点的6789端口，下载最新的Cluster Map，通过CRUSH算法获得集群中各OSD的IP地址，然后再与OSD节点直接建立连接来传输数据。所以对于Ceph来说，并不需要有集中式的主节点用于计算与寻址，客户端分摊了这部分工作。而且客户端也可以直接和OSD通信，省去了中间代理服务器的额外开销。 Mon节点之间使用Paxos算法来保持各节点Cluster Map的一致性；各Mon节点的功能总体是一样的，相互间的关系可以被简单理解为主备关系。如果主Mon节点损坏，其他Mon存活节点超过半数时，集群还可以正常运行。当故障Mon节点恢复时，会主动从其他Mon节点拉取最新的Cluster Map。 Mon节点并不会主动轮询各个OSD的当前状态，相反，OSD只有在一些特殊情况下才会上报自己的信息，平常只会简单的发送心跳。特殊情况包括：1、新的OSD被加入集群；2、某个OSD发现自身或其他OSD发生异常。Mon节点在收到这些上报信息时，则会更新Cluster Map信息并加以扩缩。 Cluster Map信息是以异步且lazy的形式扩散的。Monitor并不会在每一次Cluster Map版本更新后都将新版广播至全体OSD，而是有OSD向自己上报信息时，将更新恢复给对方。类似的，各个OSD也是在和其他OSD通信时，如果发现对方的OSD中持有的Cluster Map版本较低，则把自己更新的版本发送给对方。 这里的Ceph除了管理网段外，设了两个网段，一个用于客户端读写传输数据，另一个用于各OSD节点之间同步数据和发送心跳信息等。这样做的好处是可以分担网卡的IO压力。否则在数据清洗时，客户端的读写速度会变得极为缓慢。 MDSMds是Ceph集群中的元数据服务器，而通常情况它都不是必须的，因为只有在使用CephFS的时候才需要它，而目前云计算中用到的更广泛的是另外两种存储方式。 Mds虽然是元数据服务器，但是它不负责存储元数据，元数据也是被切成对象存在各个OSD节点中，如下图： 在创建CephFS时，要至少创建两个Pool，一个用于存储数据，另一个用于存放元数据。Mds只是负责接收用户的元数据查询请求，然后从OSD中把数据取出来映射进自己的内存中供客户访问。所以Mds其实类似一个代理缓存服务器，替OSD分担了用户的访问压力，如下图： Ceph 各概念之间的关系存储数据与Object的关系当用户要将数据存储到Ceph集群时，存储数据都会被分割成多个Object，每个Object都有一个Object ID，每个Object的大小是可以设置的，默认为4MB。Object可以看做是Ceph存储的最小存储单元。 Object 与 PG由于Object的数量很多，所以Ceph引入了PG的概念用于管理Object，每个Object最后都会通过CRUSH计算映射到某个PG中，一个PG可以包含多个Object。 Ceph条带化之后，将获得N个带有唯一OID（即Object的id）。Object id是进行线性映射生成的，即有file的元数据、Ceph条带化产生的Object的序号连缀而成。此时object需要映射到PG中，该映射包括两部分： 有Ceph集群指定的静态Hash函数计算Object的OID，获取到其Hash值 将该Hash值与mask进行操作，从而获得PG ID 根据RADOS的设计，假定集群中设定的PG总数为M（M一般为2的整数幂），则mask的值为M-1.由此，Hash值计算之后，进行按位与操作是想从所有PG中近似均匀地随机选择。基于该原理以及概率论的相关原理，当用于数据量庞大的Object以及PG时，获得到的PG ID是近似均匀的， PG 与 OSD由PG映射到数据存储的实际单元OSD中，该映射是由CRUSH算法来确定的，将PG ID作为该算法的输入，获得到包含N个OSD的集合，集合的第一个OSD被作为主OSD，其余的OSD则依次作为从OSD。N为该PG所在POOL下的副本数量，在生产环境中N一般为3；OSD集合中的OSD将共同存储和维护该PG下的Object。需要注意的是，CRUSH算法的结果不是绝对不变的，而是受其他因素的影响。其影响因素主要有以下两个： 当前系统状态，也就是Cluster Map（集群映射）。当系统中的OSD状态、数量发生变化时，Cluster Map可能发生变化，而这种变化将会影响到PG和OSD之间的映射 存储策略配置。这里的策略主要与安全相关。利用策略配置，系统管理员可以指定承载同一个PG的3个OSD分别位于数据中心的不同服务器乃至机架上，从而进一步改善存储的可靠性。 因此，只有在Cluster Map和存储策略都不发生变化的时候，PG和OSD之间的映射关系才是固定不变的。在实际使用中，策略已经配置通常不会改变。而系统状态的改变或者是因为设备损坏，或者是因为存储集群规模扩大。好在Ceph本身提供了对于这种变化的自动化支持，因而即便PG和OSD之间的映射关系发生了变化，并不会对应用造成困扰。事实上，Ceph正是需要有目的的利用这种动态映射关系。正是利用了CRUSH的动态特性，Ceph才可以将一个PG根据需要动态迁移到不同的OSD组合上，从而自动化地实现高可靠性、数据分布re-blancing等特性。 之所以在此次映射中使用CRUSH算法，而不是其他Hash算法，原因之一是CRUSH具有上述可配置特性，可以根据管理员的配置参数决定OSD的物理位置映射策略；另一方面是因为CRUSH具有特殊的“稳定性”，也就是当系统中加入新的OSD导致系统规模增大时，大部分PG与OSD之间的映射关系不会发生改变，只是少部分PG的映射关系会发生变化并引发数据迁移。这种可配置性和稳定性都不是普通Hash算法所能提供的。因此，CRUSH算法的设计也是Ceph的核心内容之一。 PG 与 PGPPG是用来存放Object的，PGP相当于是PG存放OSD的一种排列组合。举个例子，比如有3个OSD，OSD.1、OSD.2和OSD.3，副本数是2，如果PGP的数目为1，那么PG存放的OSD组合就只有一种可能：[OSD.1，OSD.2]，那么所有的PG主从副本分别存放到OSD.1和OSD.2；如果PGP设为2，那么OSD组合就有两种，[OSD.1，OSD.2]和[OSD.1，OSD.3]，是不是很像数学中的排列组合，PGP就是代表这个意思。一般来说应该将PG和PGP的数量设置为相等。接下来我们来通过一组实验来进一步说明： 先创建一个名为testpool包含6个PG和6个PGP的存储池： ceph osd pool create testpool 6 6 通过写数据后我们查看下PG的分布情况，使用以下命令： ceph pg dump pgs | grep ^1 | awk ‘{print $1,$2,$15}’ 1234567dumped pgs in format plain1.1 75 [3,6,0]1.0 83 [7,0,6]1.3 144 [4,1,2]1.2 146 [7,4,1]1.5 86 [4,6,3]1.4 80 [3,0,4] 第1列为PG的ID，第2列为该PG所存储的对象数目，第3列为该PG所在的OSD。 我们扩大PG再看看： ceph osd pool set testpool pg_num 12 再用上面的命令查询分布情况： 1234567891011121.1 37 [3,6,0]1.9 38 [3,6,0]1.0 41 [7,0,6]1.8 42 [7,0,6]1.3 48 [4,1,2]1.b 48 [4,1,2]1.7 48 [4,1,2]1.2 48 [7,4,1]1.6 49 [7,4,1]1.a 49 [7,4,1]1.5 86 [4,6,3]1.4 80 [3,0,4] 可以看到PG的数量增加到12个了，PG1.1的对象数量本来是75个，现在是37个，可以看到它把对象数分给新增的PG1.9了，刚好是38，加起来为75，而且可以看到PG1.1和PG1.9的OSD盘是一样的。而且可以看到OSD盘的组合还是那6种。 我们增加PGP的数量来看下，使用命令： ceph osd pool set testpool pgp_num 12 在看下： 1234567891011121.a 49 [1,2,6]1.b 48 [1,6,2]1.1 37 [3,6,0]1.0 41 [7,0,6]1.3 48 [4,1,2]1.2 48 [7,4,1]1.5 86 [4,6,3]1.4 80 [3,0,4]1.7 48 [1,6,0]1.6 49 [3,6,7]1.9 38 [1,4,2]1.8 42 [1,2,3] 再看PG1.1和PG1.9，可以看到PG1.9不在[3,6,0]上，而是在[1,4,2]上，该组合时新增加的。可以知道增加PGP_NUM其实是增加了OSD盘的组合。 通过上述实验可以得出： PG是指定存储池存储对象的目录有多少个，PGP是存储池PG的OSD分布组合个数 PG的增加会引起PG内的数据进行分裂，分裂到相同的OSD上新生成的PG中 PGP的增加会引起部分PG的分布变化，但是不会引起PG内对象的变动 PG 与 PoolPool也是一个逻辑存储概念，我们创建存储池pool的时候，都需要指定PG和PGP的数量，逻辑上来说PG是属于某个存储池的，就像Object是属于某个PG的一样。 下图表明了存储数据、Object、PG、Pool、OSD、存储磁盘的关系： Ceph 条带化众所周知，存储设备具有吞吐量限制，它影响读写性能和可扩展性能。所以存储系统通常都支持条带化以增加存储系统的吞吐量并提升性能，数据条带化最常见的方式是做RAID。与Ceph的条带化最相似的是RAID 0或者是“带区卷”。Ceph条带化提供了类似RAID 0的吞吐量，N路RAID镜像的可靠性已经更快速的恢复能力。 在磁盘阵列中，数据是以条带（stripe）的方式贯穿在磁盘阵列所有硬盘中的。这种数据的分配方式可以弥补OS读取数据量跟不上的不足。 1.将条带单元（stripe unit）从阵列的第一个硬盘到最后一个硬盘收集起来，就可以称为条带（stripe）。有的时候，条带单元也被称为交错深度。在光纤技术中，一个条带单元被叫做段。 2.数据在阵列中的硬盘上是以条带的形式分布的，条带化是指数据在阵列中所有硬盘中的存储过程。文件中的数据被分割成小块的数据段在阵列中的硬盘顺序上的存储，这个最小数据块就叫做条带单元。 决定Ceph条带化数据的3个因素： 对象大小：处于分布式集群中的对象拥有一个最大可配置的尺寸（例如，2MB，4MB等），对象大小应该足够大以适应大量的条带单元。 条带宽度：条带有一个可以配置的单元大小，Ceph Client端将数据写入对象，分成相同大小的条带单元，除了最后一个条带之外；每个条带宽度，应该是对象大小的一部分，这样使得一个对象可以包含多个条带单元。 条带总量：Ceph客户端写入一系列的条带单元到一系列的对象，这就决定了条带的总量，这些对象被称为对象集。当Ceph客户端写入的对象集合中的最后一个对象之后，它将会返回到对象集合中的第一个对象处。 CRUSH 算法数据分布算法挑战 数据分布和负载均衡： a.数据分布均衡，使数据能均匀地分布到各个节点上 b.负载均衡，使数据访问读写操作的负载在各个节点和磁盘的负载均衡 灵活应对集群伸缩： a.系统可以方便的增加或者删除节点设备，并且对节点失效进行处理 b.增加或者删除节点设备后，能自动实现数据的均衡，并且尽可能少的迁移数据 支持大规模集群： 要求数据分布算法维护的元数据相对较小，并且计算量不能太大。随着集群规模的增加，数据分布算法的开销相对较小 CRUSH 算法说明 CRUSH（Controlled Replication Under Scalable Hashing）是一种基于伪随机控制数据分布、复制的算法。Ceph是为大规模分布式存储系统（PB级的数据和成百上千台存储设备）而设计的，在大规模的存储系统里，必须考虑数据的平衡分布和负载（提高资源利用率）、最大化系统的性能以及系统的扩展和硬件容错等。CRUSH就是为解决上述问题而设计的。在Ceph集群里，CRUSH只需要一个简洁而层次清晰的设备描述，包括存储集群和副本放置策略，就可以有效地把数据对象映射到存储设备上，且这个过程是完全分布式的，在集群系统中的任何一方都可以独立计算任何对象的位置；另外，大型系统存储结构是动态变化的（存储节点的扩展或者缩容、硬件故障等），CRUSH能够处理存储设备的变更（添加或删除），并最小化由于存储设备的变更而导致的数据迁移。 PG到OSD的映射过程的算法叫做CRUSH算法。 CRUSH算法是一个伪随机的过程，它可以从所有的OSD中，随机性选择一个OSD集合，但是同一个PG每次随机选择的结果是不变的，也就是映射的OSD集合是固定的。 CRUSH 基本原理CRUSH 算法因子： 层次化的Cluster Map： 反映了存储系统层级的物理拓扑结构。定义了OSD集群具有层级关系的静态拓扑结构。OSD层级使得CRUSH算法在选择OSD时实现了机架感知能力，也就是通过规则定义，使得副本可以分布在不同的机架、不同的机房中，提高数据的安全性 Placement Rules： 决定了一个PG的对象副本如何选择的规则，通过这些可以自己设定规则，用户可以自定义设置副本在集群中的分布 CRUSH 关系分析从本质上讲，CRUSH算法是通过存储设备的权重来计算数据对象的分布。在计算过程中，通过Cluster Map（集群映射）、Data Distribution Policy（数据分布策略）和给出的一个随机数共同决定数据对象的最终位置。 Cluster MapCluster Map记录所有可用的存储资源以及可用的存储资源及相互之间的空间层次结构（集群中有多少个机架、机架上有多少个服务器、每个服务器上有多少磁盘等信息）。所谓Map，顾名思义，就是类似于我们生活中的地图。在Ceph存储里，数据的索引都是通过各种不同的Map来实现的。另一方面，Map使得Ceph集群存储设备在物理层做了一层防护。例如，在多副本结构上，通过设置合理的Map（故障域设置为Host级），可以保证在某一服务器死机的情况下，有其他副本保留在正常的存储节点上，能够继续提供服务，实现存储的高可用。设置更高的故障域级别（如Rack、Row等）能保证整机柜或同一排机柜在掉电情况下数据的高可用性和完整性。 1.Cluster Map的分层结构Cluster Map由Device和Bucket构成。它们都有自己的ID和权重值，并且形成一个以Device为叶子节点、Bucket为躯干的树状结果。 Bucket拥有不同的类型，如Host、Row、Rack、Room等，通常我们默认把机架类型定义为Rack，主机类型定义为Host，数据中心（IDC机房）定义为Data Center。Bucket的类型都是虚拟结构，可以根据自己的喜好设计合适的类型。Device节点的权重值代表了存储设备的容量与性能。其中，磁盘容量是权重大小的关键因素。 OSD的权重值越高，对应磁盘会被分配写入更多的数据。总体来说，数据会被均匀写入分布于集群所有磁盘，从而提高整体性能和可靠性。无论磁盘的规格容量，总能够均匀使用。 关于OSD权重值的大小值的配比，官方默认值设置为1TB容量的硬盘，对应权重为1。可以在/etc/init.d/ceph源码里查看相关的内容。 Bucket随机算法类型 一般的buckets：适合所有子节点权重相同，而且很少添加删除item list buckets：适用于集群扩展类型。增加item，产生最优的数据移动，查找item，时间复杂对为O(n) tree buckets：查找复杂的为O(log n)，添加删除叶子节点是，其它节点node_id不变 straw buckets：允许所有项通过类似抽签的方式来与其它项公平“竞争”。定位副本时，bucket中的每一项都对应一个随机长度的straw，且拥有最长长度的straw会获得胜利（被选中）；添加或者重新计算，子树之间的数据移动提供最优的解决方案 2.恢复与动态平衡在默认情况下，当集群里有组件故障时（主要是OSD，也可能是磁盘或者网络等），Ceph会把OSD标记为down，如果在300s内未能回复，集群就会开始进行恢复状态。这个“300s”可以通过“mon osd down ourt interval”配置选项修改等待时间。PG（Placement Groups）是Ceph数据管理（包括复制、修复等操作）单元。当客户端把读写请求（对象单元）推送到Ceph时，通过CRUSH提供的Hash算法把对象映射到PG。PG在CRUSH策略的影响下，最终会被映射到OSD上。 Data Distribution PolicyData Distribution Policy由Placement Rules组成。Rule决定了每个数据对象有多少个副本，这些副本存储的限制条件（比如3个副本放在不同的机架中）。一个典型的rule如下所示： 123456789rule replicated_ruleset &#123; ##rule名字 ruleset 0 #rule的ID type replicated ##类型为副本模式，另外一种模式为纠删码（EC） min_size 1 ##如果存储池的副本数大于这个值，此rule不会应用 max_size 10 ##如果存储池的副本数大于这个值，此rule不会应用 step take default ##以default root 为入口 step chooseleaf firstn 0 type host ##隔离城为host级，即不同副本在不同的主机上 step emit ##提交&#125; 根据实际的设备环境，可以定制符合自己需求的Rule。 CRUSH 中的伪随机$$CRUSH(x) \quad \rightarrow \quad (osd1,osd2 \cdots\cdots osdN)$$ CRUSH使用了多参数的Hash函数，在Hash之后，映射都是按既定规则选择的，这使得从x到OSD的集合是确定的和独立的。CRUSH只使用Cluster Map、Placement Ruels、X。CRUSH是伪随机算法，相似输入的结果之间没有相关性。 下面通过计算PG的ID来看CRUSH的一个计算过程： Client输入pool ID和对象ID（如pool=’liverpool’,object-id=’john’） CRUSH获得对象ID并对其Hash运算 CRUSH计算OSD个数，Hash取模获得PG的ID（如0x58） CRUSH获得已命名pool的ID（如liverpool=4） CRUSH预先考虑到pool ID相同的PG ID（如4.0x58） 在Ceph集群里，当有数据对象要写入集群时，需要进行两次映射，第一次从Object–&gt;PG，第二层是PG–&gt;OSD set。每一次的映射都是与其他对象不相关的，这充分体现了CEUSH的独立性（充分分散）和确定性（可确定的存储位置）。 Ceph IO流程及数据分布 正常IO流程图 步骤： Client 创建Cluster handler Client 读取配置文件 Client 连接上Monitor，获取集群map信息 Client 读写IO 根据Crushmap算法请求对应的主OSD数据节点 主OSD数据节点同时写入另外两个副本节点数据 等待主节点以及另外两个副本节点写完数据状态 主节点及副本节点写入状态都成功后，返回给Client，IO写入完成 新主IO流程图说明： 如果新加入的OSD1取代了原有的OSD4成为Primary OSD，由于OSD1上未创建PG，不存在数据，那么PG上的IO无法进行，怎样工作呢？ 步骤： Client 连接Monitor获取集群Map信息 同时新主OSD1由于没有PG，数据会主动上报Monitor告知让OSD2临时接替为主 临时主OSD2会把数据全量同步给新主OSD1 Client IO读写直接连接临时主OSD2进行读写 OSD2收到读写IO，同时写入另外两个副本节点 等待OSD2以及另外两副本写入成功 OSD2三分数据都写入成功返回给Client，此时Client IO读写完毕 如果OSD1数据同步完毕，临时主OSD2会交出主角色 OSD1成为主节点，OSD2变成副本 Ceph IO算法流程 File用户需要读写文件，File-&gt;Object映射： a. ino（File的元数据，File的唯一ID） b. ono（File切分产生的某个Object的序号，默认以4M切分一个块大小） c. oid（object id： ino + ono） Object是RADOS需要的对象。Ceph指定一个静态Hash函数计算OID的值，将OID映射成一个近似均匀分布的伪随机值，然后和mask按位相与，得到PGID。Object-&gt;PG映射： a. hash(oid) &amp; mask -&gt; pgid b. mask = PG总数m（m为2的整数幂）-1 PG（Placement Group），用途是对Object的存储进行组织和位置映射，类似Redis Cluster里面的slot的概念。一个PG里面会有很多Object。采用CRUSH算法，将PGID带入其中，然后得到一组OSD。PG-&gt;OSD映射： CRUSH(pgid) -&gt; (osd1,osd2,osd3) Ceph RBD IO流程 步骤： 客户端创建一个Pool，需要为这个Pool指定PG的数量 创建pool/image RBD设备进行挂载 用户写入的数据进行切块，每个块的大小默认为4M，并且每个块都有一个名字，即Object+序号 将每个Object通过PG进行副本位置的分配 PG根据CRUSH算法会寻找3个OSD，把这个Object分别保存在这三个OSD上 OSD上实际是把底层的Disk进行了格式化操作，一般部署工具会将它格式化为xfs文件系统 Object的存储就变成了存储一个名为rbd0.object1.file的文件 Ceph RBD IO框架图 客户端写数据OSD过程： 采用的是librbd的形式，使用librbd创建一个块设备，向这个块设备写入数据 在客户端本地通过调用librados接口，然后经过pool、rbd、object、pg进行层层映射，在PG这一层中，可以知道数据保存在哪3个OSD上，以及这3个OSD的主从关系 客户端与primary OSD建立socket通信，将要写入的数据传给primary OSD，由primary OSD再将数据发送给其他replica OSD数据节点 Ceph 数据扩容PG分布场景数据迁移流程： 现状3个OSD，4个PG 扩容到4个OSD，4个PG 现状： 扩容后： 说明： 每个OSD上分布很多PG，并且每个PG会自动散落在不同的OSD上。如果扩容，那么相应的PG会自动迁移到新的OSD上，保证PG数量的均衡 Ceph心跳机制心跳介绍心跳是用于节点间检测对方是否发送故障，以便及时发现故障点进入相应的故障处理流程。 问题： 故障检测时间和心跳报文带来的负载之间做权衡 心跳频率太高则过多的心跳报文会影响系统性能 心跳频率过低则会延长发现故障节点的时间，从而影响系统的可用性 故障检测策略应该能够做到： 及时：节点发生异常如宕机或网络中断时，集群可以在可接受的时间范围内感知 适当的压力：包括对节点的压力和网络的压力 容忍网络抖动：网络偶尔延迟 扩散机制：节点存活状态改变导致的元信息变化需要通过某种机制扩散到整个集群 Ceph 心跳检查 OSD节点会监听public、cluster、front和back四个端口 public端口：监听来自Monitor和Client的连接 cluster端口：监听来自OSD Peer的连接 front端口：供客户端连接集群使用的网卡，这里临时给集群内部之间进行心跳 back端口：供集群内部使用的网卡，集群内部之间进行心跳 hbclient：发送ping心跳的messenger Ceph OSD之间相互心跳检查 步骤： 同一个PG内的OSD相互心跳，它们互相发送PING/PONG信息 每隔6s检测一次（实际会在这个基础上加一个随机时间来避免峰值） 20s没有检测到心跳回复，加入failure队列 Ceph OSD与Mon心跳检测 OSD报告给Monitor: OSD有事件发生时（比如故障、PG变更） 自身启动5s内 OSD周期性的上报给Monitor OSD检查failure_queue中的伙伴OSD失败信息 向Monitor发送失效报告，并将失败信息加入failure_pending队列，然后将其从failure_queue移除 收到来自failure_queue或者failure_pending中的OSD的心跳时，将其从这两个队列中移除，并告知Monitor取消之前的失效报告 当发生与Monitor网络重连时，会将failure_pending中的错误报告加回到failure_queue中，并再次发送给Monitor Monitor统计下线OSD Monitor收集来自OSD的伙伴失效报告 当错误报告指向的OSD失效超过一定阈值，且足够多的OSD报告其失效时，将该OSD下线 Ceph心跳检测总结Ceph通过伙伴OSD汇报失效节点和Monitor统计来自OSD的心跳两种方式判定OSD节点失效。 及时：伙伴OSD可以在秒级发现节点失效并汇报Monitor，并在几分钟内由Monitor将失效OSD下线 适当压力：由于有伙伴OSD汇报机制，Monitor与OSD之间的心跳统计更像是一种保险措施，因此OSD向Monitor发送心跳的间隔可以长达600s，Monitor的检测阈值也可以长达900s。Ceph实际上是将故障检测过程中中心节点的压力分散到所有的OSD上，以此提高中心节点Monitor的可靠性，进而提高这个集群的可靠性 容忍网络抖动：Monitor收到OSD对其伙伴OSD的汇报后，并没有马上将目标OSD下线，而是周期性地等待几个条件： 目标OSD的失效时间大于通过固定量osd_heartbeat_grace和历史网络条件动态确定的阈值 来自不同主机的汇报到的mod_osd_min_down_reports 慢速前两个条件失效汇报没有被源OSD取消 扩散：作为中心节点的Monitor并没有在更OSDMap后尝试广播通知所有的OSD和Client，而是惰性的等待OSD和Client来获取，以此来减少Monitor压力并简化交互逻辑 Ceph通信框架Ceph通信框架种类介绍网络通信框架三种不同的实现方式： Simple线程模式 特点：每一个网络链接，都会创建两个线程，一个用于接收，一个用于发送 缺点：大量的链接会产生大量的线程，会消耗CPU资源，影响性能 Async事件的I/O多路复用模式 说明：这种是目前网络通信中广泛采用的方式。K版默认已经使用Async XIO方式使用了开源的网络通信库accelio来实现 说明：这种方式需要依赖第三方库accelio的稳定性 Ceph通信框架设计模式设计模式（Subscribe/Publish） 订阅发布模式又名观察者模式，它意图是”定义对象间的一种一对多的依赖关系，当一个对象的状态发生改变是，所有依赖于它的对象都得到通知并被自动更新“。 Ceph通信框架流程图 步骤： Accepter监听peer的请求，调用SimpleMessenger::add_accept_pip3()创建新的pipe到SimpleMessenger::pipes来处理该请求 Pipe用于消息的读取和发生。该类主要有两个组件：Pipe::Reader，Pipe::Writer用来处理消息读取和发送 Messenger作为消息的发布者，各个Dispatcher子类作为消息的订阅者，Messenger收到消息之后，通过Pipe对取消息，然后转给Dispatcher处理 Dispatcher是订阅者的基类，具体的订阅者后端继承该类，初始化的时候通过Messenger::add_dispatcher_tail/head注册到Messenger::dispatchers。收到消息后，通知该类处理 DispatchQueue该类用来缓存收到的消息，然后唤醒DispatchQueue::dispatch_thread线程找到后端的Dispatch处理消息 Ceph通信框架类图 Ceph通信数据格式通信协议格式需要双方约定数据格式。 消息的内容主要分为三部分： header //消息头 类型消息的信封 user data //需要发送的实际数据 payload //操作保存元数据 middle //预留字段 data //读写数据 footer //消息的结束标记 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152class Message : public RefCountedObject &#123;protected: ceph_msg_header header; // 消息头 ceph_msg_footer footer; // 消息尾 bufferlist payload; // "front" unaligned blob bufferlist middle; // "middle" unaligned blob bufferlist data; // data payload (page-alignment will be preserved where possible) /* recv_stamp is set when the Messenger starts reading the * Message off the wire */ utime_t recv_stamp; //开始接收数据的时间戳 /* dispatch_stamp is set when the Messenger starts calling dispatch() on * its endpoints */ utime_t dispatch_stamp; //dispatch 的时间戳 /* throttle_stamp is the point at which we got throttle */ utime_t throttle_stamp; //获取throttle 的slot的时间戳 /* time at which message was fully read */ utime_t recv_complete_stamp; //接收完成的时间戳 ConnectionRef connection; //网络连接 uint32_t magic = 0; //消息的魔术字 bi::list_member_hook&lt;&gt; dispatch_q; //boost::intrusive 成员字段&#125;;struct ceph_msg_header &#123; __le64 seq; // 当前session内 消息的唯一 序号 __le64 tid; // 消息的全局唯一的 id __le16 type; // 消息类型 __le16 priority; // 优先级 __le16 version; // 版本号 __le32 front_len; // payload 的长度 __le32 middle_len;// middle 的长度 __le32 data_len; // data 的 长度 __le16 data_off; // 对象的数据偏移量 struct ceph_entity_name src; //消息源 /* oldest code we think can decode this. unknown if zero. */ __le16 compat_version; __le16 reserved; __le32 crc; /* header crc32c */&#125; __attribute__ ((packed));struct ceph_msg_footer &#123; __le32 front_crc, middle_crc, data_crc; //crc校验码 __le64 sig; //消息的64位signature __u8 flags; //结束标志&#125; __attribute__ ((packed));]]></content>
      <categories>
        <category>Ceph</category>
      </categories>
      <tags>
        <tag>Ceph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes_Istio-Gateway_Ingress-Controller]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2FKubernetes-Istio-Gateway-Ingress-Controller%2F</url>
    <content type="text"><![CDATA[最近在istio，然后想总结下ingress-controller和istio-gateway的区别。 在Kubernetes环境中，Kubernetes Ingress用于配置需要在集群外部公开的服务。 但是在Istio服务网格中，更好的方法是使用新的配置模型，即Istio Gateway；Gateway允许将Istio流量管理的功能应用于进入集群的流量。 这里的ingress为nginx ingress 通过灰度发布，来看一下ingress与istio的实现方式的差异 环境准备： kubernetes环境 1.13.5 部署istio 1.1.1 Istio： 部署两个服务： old 123456789101112131415161718192021222324252627282930313233343536apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: old-nginxspec: replicas: 2 selector: matchLabels: run: old-nginx template: metadata: labels: run: old-nginx spec: containers: - image: registry.cn-hangzhou.aliyuncs.com/xxxxx/old-nginx imagePullPolicy: Always name: old-nginx ports: - containerPort: 80 protocol: TCP restartPolicy: Always---apiVersion: v1kind: Servicemetadata: name: old-nginxspec: ports: - port: 80 protocol: TCP targetPort: 80 selector: run: old-nginx sessionAffinity: None type: NodePort new 123456789101112131415161718192021222324252627282930313233343536apiVersion: extensions/v1beta1kind: Deploymentmetadata: name: new-nginxspec: replicas: 1 selector: matchLabels: run: new-nginx template: metadata: labels: run: new-nginx spec: containers: - image: registry.cn-hangzhou.aliyuncs.com/xxxxx/new-nginx imagePullPolicy: Always name: new-nginx ports: - containerPort: 80 protocol: TCP restartPolicy: Always---apiVersion: v1kind: Servicemetadata: name: new-nginxspec: ports: - port: 80 protocol: TCP targetPort: 80 selector: run: new-nginx sessionAffinity: None type: NodePort 输出 12345$ kubectl get svcNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEkubernetes ClusterIP 172.19.0.1 &lt;none&gt; 443/TCP 9hnew-nginx NodePort 172.19.8.168 &lt;none&gt; 80:31904/TCP 4hold-nginx NodePort 172.19.12.148 &lt;none&gt; 80:31545/TCP 4h 测试两个服务 1234$ kubectl run -it --rm bash --image=appropriate/curl --restart=Never curl 172.19.8.168new$ kubectl run -it --rm bash --image=appropriate/curl --restart=Never curl 172.19.12.148old 创建Gateway对象/用的istio官方yaml 1234567891011121314apiVersion: networking.istio.io/v1alpha3kind: Gatewaymetadata: name: helloworld-gateway # namespec: selector: istio: ingressgateway # use istio default controller servers: - port: number: 80 name: http protocol: HTTP hosts: - "*" 确定Istio入口IP和port 123$ kubectl get svc istio-ingressgateway -n istio-systemNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEistio-ingressgateway LoadBalancer 172.19.14.182 (pending) 80:31380/TCP,443:31390/TCP,31400:31400/... 10h 确认EXTERNAL-IP设置了值 123export INGRESS_HOST= $(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='&#123;.status.loadBalancer.ingress[0].ip&#125;')export INGRESS_PORT= $(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='&#123;.spec.ports[?(@.name=="http")].port&#125;')export SECURE_INGRESS_PORT= $(kubectl -n istio-system get service istio-ingressgateway -o jsonpath='&#123;.spec.ports[?(@.name=="https")].port&#125;') 满足特定规则： 例如，我们希望请求头中带有user且值为new的客户端请求才能访问new service 我们可以创建一个对应的VirtualService为通过Gateway进入的流量配置路由 123456789101112131415161718192021222324apiVersion: networking.istio.io/v1alpha3kind: VirtualService # istio route rule metadata: name: helloworld # namespec: gateways: - helloworld-gateway # link Gateway hosts: - '*' http: - match: - headers: user: exact: new # match headers -&gt; user (-&gt; exact -&gt;) new [headers: user: new] route: # 会访问new-nginx，满足头部要求即可 - destination: host: new-nginx port: number: 80 - route: # 否则就访问old-nginx - destination: host: old-nginx port: number: 80 客户端测试 123456789101112$ curl -H "user: new" http://$INGRESS_HOST:$INGRESS_PORTnew$ curl -H "user: new" http://$INGRESS_HOST:$INGRESS_PORTnew$ curl -H "user: new" http://$INGRESS_HOST:$INGRESS_PORTnew$ curl http://$INGRESS_HOST:$INGRESS_PORTold$ curl http://$INGRESS_HOST:$INGRESS_PORTold$ curl http://$INGRESS_HOST:$INGRESS_PORTold 在满足特定规则和权重： 例如，我们希望请求头中带有user且值为new的客户端请求有50%的比例访问new service，那么我们可以创建一个这样的VirtualService 12345678910111213141516171819202122232425262728293031apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: helloworldspec: gateways: - helloworld-gateway hosts: - '*' http: - match: - headers: user: exact: new # 这里跟上面的一样，符合头部 user: new就会访问到这一层 route: - destination: host: new-nginx port: number: 80 weight: 50 # 另外因为这里设置了两个访问规则，即首先符合头部，然后会有50%的几率会访问到new-nginx，另外50%会访问到old-nginx - destination: host: old-nginx port: number: 80 weight: 50 - route: # 如果不匹配头部，那就全部访问到old-nginx - destination: host: old-nginx port: number: 80 客户端测试 1234567891011121314151617181920$ curl -H "user: new" http://$INGRESS_HOST:$INGRESS_PORTnew$ curl -H "user: new" http://$INGRESS_HOST:$INGRESS_PORTold$ curl -H "user: new" http://$INGRESS_HOST:$INGRESS_PORTnew$ curl -H "user: new" http://$INGRESS_HOST:$INGRESS_PORTnew$ curl -H "user: new" http://$INGRESS_HOST:$INGRESS_PORTold$ curl -H "user: new" http://$INGRESS_HOST:$INGRESS_PORTnew$ curl -H "user: new" http://$INGRESS_HOST:$INGRESS_PORTold$ curl http://$INGRESS_HOST:$INGRESS_PORTold$ curl http://$INGRESS_HOST:$INGRESS_PORTold$ curl http://$INGRESS_HOST:$INGRESS_PORTold 权重随机访问： 例如，我们仅仅希望20%的客户端请求访问new service，那么我们可以创建一个这样的VirtualService 12345678910111213141516171819202122apiVersion: networking.istio.io/v1alpha3kind: VirtualServicemetadata: name: helloworldspec: gateways: - helloworld-gateway hosts: - '*' http: - route: # 这条virtual Service 设置了权重来几率性访问nginx；20%new-nginx，80% old-nginx - destination: host: new-nginx port: number: 80 weight: 20 - destination: host: old-nginx port: number: 80 weight: 80 客户端测试 12345678910$ curl http://$INGRESS_HOST:$INGRESS_PORTold$ curl http://$INGRESS_HOST:$INGRESS_PORTold$ curl http://$INGRESS_HOST:$INGRESS_PORTold$ curl http://$INGRESS_HOST:$INGRESS_PORTold$ curl http://$INGRESS_HOST:$INGRESS_PORTnew Nginx Ingress 不支持权重，貌似能够支持headers头 Nginx: 使用$http_ 获取http请求的header，根据配置中是否为完整或者正则匹配，匹配foo的值 12345678910location / &#123; .... if ($http_foo = "bar") &#123; //完全匹配 #if ($http_foo ~ "bar") &#123; //正则匹配 proxy_pass http://default-new-nginx-80; break; &#125; proxy_pass http://default-old-nginx-80; ....&#125; 使用$cookie_获取http请求的cookie，根据配置中是否为完整或者正则匹配，匹配foo的值 123456789location / &#123; .... if ($cookie_foo ~ "^bar") &#123; //正则匹配 # if ($cookie_foo = "bar") &#123; //完全匹配 proxy_pass http://default-new-nginx-80; break; &#125; proxy_pass http://default-old-nginx-80; ... 使用$arg_获取http的请求参数，根据配置中是否为完整或者正则匹配，匹配foo的值 123456789location / &#123; .... if ($arg_foo ~ "^bar") &#123; //正则匹配 # if ($arg_foo = "bar") &#123; //完全匹配 proxy_pass http://default-new-nginx-80; break; &#125; proxy_pass http://default-old-nginx-80; ... 这里用到的两个服务为istio用的两个服务： 应用部署完毕后，我们看要怎么配置ingress才能够正确的刷新到ngixn.conf，在nginx-ingress-controller中，它支持 nginx.ingress.kubernetes.io/configuration-snippet 这样一个注解允许我们在location里配置自定义需求。同时结合nginx-ingress-controller的upstream命名方式namespace-serviceName-port的方式配置注解信息。如下ingress yaml所示: 123456789101112131415161718192021222324apiVersion: extensions/v1beta1kind: Ingressmetadata: annotations: nginx.ingress.kubernetes.io/configuration-snippet: | if ($http_foo ~ "^.*bar$") &#123; proxy_pass http://default-new-nginx-80; break; &#125; name: gray-release namespace: defaultspec: rules: - host: www.example.com http: paths: - backend: serviceName: old-nginx servicePort: 80 path: / - backend: serviceName: new-nginx servicePort: 80 path: / 输出 123456789101112 curl -H "Host: www.example.com" http://172.16.0.9old curl -H "Host: www.example.com" http://172.16.0.9old curl -H "Host: www.example.com" http://172.16.0.9old curl -H "Host: www.example.com" -H "foo: bar" http://172.16.0.9new curl -H "Host: www.example.com" -H "foo: bar" http://172.16.0.9new curl -H "Host: www.example.com" -H "foo: bar" http://172.16.0.9new]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Istio</tag>
        <tag>Ingress</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes_1.13.5_ingress]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2FKubernetes-1-13-5%E9%83%A8%E7%BD%B2ingress%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes部署mysql-opreator]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2FKubernetes%E9%83%A8%E7%BD%B2mysql-opreator%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
        <tag>kubernetes_problem</tag>
        <tag>kubernetes-opreator</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes安装-kubeadm_1.13.5(非外部etcd)]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2FKubernetes%E5%AE%89%E8%A3%85-kubeadm-1-13-5-%E9%9D%9E%E5%A4%96%E9%83%A8etcd%2F</url>
    <content type="text"><![CDATA[之前写过一篇kubernetes安装的，用到的是外部的etcd，即etcd单独二进制安装，kubeadm初始化指定即可； 本篇用的纯kubeadm安装高可用集群，master组件静态pod 环境描述 kubernetes version：1.13.5 docker version：18.6.3 Redhat：7.6 x.x.x.1 master-1 kube-apiserver、kube-controller-manager、kube-scheduler、etcd、keepalive、docker x.x.x.2 master-2 kube-apiserver、kube-controller-manager、kube-scheduler、etcd、keepalive、docker x.x.x.3 master-3 kube-apiserver、kube-controller-manager、kube-scheduler、etcd、keepalive、docker x.x.x.4 vip x.x.x.5 node-1 kubelet、docker x.x.x.6 node-2 kubelet、docker 因为担心kubeadm起来的etcd不稳定，这里用到的etcd对于kubernetes来说作为外部etcd集群。即使用二进制安装etcd集群，其余组件用kubeadm来完成安装。 环境准备1、准备工作12345678910111213141516echo "1" &gt; /proc/sys/net/bridge/bridge-nf-call-iptables# 停防火墙systemctl stop firewalldsystemctl disable firewalldsystemctl disable firewalld# 关闭Swapswapoff -ased 's/.*swap.*/#&amp;/' /etc/fstab# 关闭防火墙systemctl disable firewalld &amp;&amp; systemctl stop firewalld &amp;&amp; systemctl status firewalld# 关闭Selinuxsetenforce 0sed -i "s/^SELINUX=enforcing/SELINUX=disabled/g" /etc/sysconfig/selinuxsed -i "s/^SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/configsed -i "s/^SELINUX=permissive/SELINUX=disabled/g" /etc/sysconfig/selinuxsed -i "s/^SELINUX=permissive/SELINUX=disabled/g" /etc/selinux/config 2、docker安装1、下载设置源123sudo yum install -y yum-utils \ device-mapper-persistent-data \ lvm2 123yum-config-manager \--add-repo \https://download.daocloud.io/docker/linux/centos/docker-ce.repo 2、安装docker123yum list docker-ce --showduplicates | sort -r # 列出docker-ce的版本listyum install docker-ce-&lt;版本号&gt; -y # -y 安装docker需要的依赖，其中有个container-selinux的也可以单独下载# rpm -ivh container-selinux-2.33-1.git86f33cd.el7.noarch.rpm 3、启动docker12systemctl start dockerdocker version # 验证docker安装是否完成并启动成功 3、kubeadm/kubelet/kubectl安装 各节点安装 1234567891011121314cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOFyum install -y kubelet-1.13.5 kubeadm-1.13.5 kubectl-1.13.5 --disableexcludes=kubernetes #禁用除kubernetes之外的仓库,要用 -y 参数，会自动安装kube-cni等插件systemctl start kubeletsystemctl enable kubelet # kubeadm 要求kubelet保持开机自启状态 4、keepalive安装 master节点安装： 123456789101112131415161718192021222324252627282930313233343536373839404142yum install -y keepalivedsystemctl start keepalivedvim /etc/keepalived/keepalived.confglobal_defs &#123; router_id LVS_k8s&#125;vrrp_script CheckK8sMaster &#123; script &quot;curl -k https://10.70.49.130:6443&quot; interval 3 timeout 9 fall 2 rise 2&#125;vrrp_instance VI_1 &#123; state MASTER interface ens256 # 网卡 virtual_router_id 61 # 主节点权重最高 依次减少 priority 120 advert_int 1 #修改为本地IP mcast_src_ip x.x.x.2 nopreempt authentication &#123; auth_type PASS auth_pass sqP05dQgMSlzrxHj &#125; unicast_peer &#123; x.x.x.1 #x.x.x.2 x.x.x.3 &#125; virtual_ipaddress &#123; x.x.x.4 &#125; track_script &#123; CheckK8sMaster &#125;&#125; kubernetes部署1、master部署 部署之前请确保下载好相关image，翻墙下载或者国内dockerhub下载kubernetes镜像 1、初始化master1 创建master1的初始化配置文件,网络插件采用flannel，CIDR地址是 “10.244.0.0/16”，如下为1.13.5新版本配置文件。 1234567891011121314151617181920212223242526272829303132cat &gt; kubeadm-master.yaml &lt;&lt; EOFapiVersion: kubeadm.k8s.io/v1beta1kind: InitConfigurationnodeRegistration: name: 10.78.229.193 # 指定node name，kubectl get no会显示iplocalAPIEndpoint: advertiseAddress: x.x.x.1 # 本机ip，这里为x.x.x.1-3 bindPort: 6443---apiVersion: kubeadm.k8s.io/v1beta1kind: ClusterConfigurationkubernetesVersion: v1.13.5 # kubernetes版本 对应下载的imageimageRepository: k8s.gcr.io # 自己修改为自己的镜像库名apiServer: certSANs: - "master1" - "master2" - "master3" - "x.x.x.1" - "x.x.x.2" - "x.x.x.3" - "x.x.x.4" - "127.0.0.1"controlPlaneEndpoint: "x.x.x.4:8443" # 控制台ip指定，即vip 实现apiserver高可用etcd: local: dataDir: /var/lib/etcd # 随意指定即可networking: podSubnet: "10.244.0.0/16"EOF 123# 运行初始化命令即可，前提要把相关设置关闭，详情至准备工作# 其中在kubelet配置里加入--pod-infra-container-image 参数指定 pause私有镜像库镜像kubeadm init --config kubeadm-master.yaml 在初始化配置里，对于etcd有两种高可用的选项，一个使用内部etcd，一个使用外部etcd(独立搭建的etcd集群，而不是在初始化中搭建的)，两者初始化配置文件略有不同。 1234567891011121314151617# 使用内部etcd的话，初始化yaml文件中etcd配置如下：etcd: local: extraArgs: listen-client-urls: "https://127.0.0.1:2379,https://x.x.x.x:2379" advertise-client-urls: "https://x.x.x.x:2379" listen-peer-urls: "https://x.x.x.x:2380" initial-advertise-peer-urls: "https://x.x.x.x:2380" initial-cluster: "master1.hanli.com=https://x.x.x.x:2380"# 使用外部etcd的话，etcd: #ETCD的地址 external: endpoints: - https://x.x.x.1:2379 - https://x.x.x.2:2379 - https://x.x.x.3:2379 init初始化之后，如果成功会出现join，这时就可以运行一下命令 机器上的用户要使用kubectl来管理集群操作集群 123mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config 验证命令 123456789kubectl get cs # 如下信息NAME STATUS MESSAGE ERRORcontroller-manager Healthy okscheduler Healthy oketcd-0 Healthy &#123;"health": "true"&#125;kubectl get node # notReady 状态，是因为没有安装网络插件 Name成ip，可修改kubelet的启动参数即可NAME STATUS ROLES AGE VERSIONmaster1 NotReady master 1m v1.13.5 2、启动flannel服务123456789101112wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml# flannel 默认会使用主机的第一张网卡，如果你有多张网卡，需要通过配置单独指定。修改 kube-flannel.yml 中的以下部分cat kube-flannel.yml containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.11.0-amd64 # 修改下自己私有镜像库的flannel镜像名 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr - --iface=ens33 #添加 1234567891011121314kubectl apply -f kube-flannel.yml # 创建pod，因为是ds的所以后续集群里面加节点就会自动启动flannelkubectl get node # 会发现 node状态变成了 ReadyNAME STATUS ROLES AGE VERSIONmaster1 Ready master 10m v1.13.5kubectl get po --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system coredns-86c58d9df4-r59rv 1/1 Running 0 59mkube-system coredns-86c58d9df4-rbzx5 1/1 Running 0 59mkube-system kube-apiserver-master1 1/1 Running 0 58mkube-system kube-controller-manager-master1 1/1 Running 16 58mkube-system kube-flannel-ds-amd64-229j2 1/1 Running 0 42mkube-system kube-proxy-4wrg5 1/1 Running 0 59mkube-system kube-scheduler-master1 1/1 Running 13 58m 不是running状态，就说明出错了，通过查看 描述：kubectl describe pod kube-scheduler-master.hanli.com -n kube-system 日志：kubectl logs kube-scheduler-master.hanli.com -n kube-system flannel服务启动成功后，coredns也就会自动启动成功，状态为Running 3、初始化其他master节点 首先把master1上生成的ca证书等，拷贝到其他master节点上，最好免密，可使用pscp等批量任务 123456789101112131415161718192021&gt; #!/bin/bash&gt; #注意修改为自己的主机名&gt; export CONTROL_PLANE_IPS="master2 master3" &gt; &gt; # 保证节点有/etc/kubernetes/pki目录&gt; # 把以下证书复制到其他master节点&gt; for host in $&#123;CONTROL_PLANE_IPS&#125;; do&gt; # ！！！修正拷贝证书的时候请指定特定证书拷贝（之前的已经备注，新加的有etcd目录，没有用外部etcd集群），因为在证书拷贝的情况下吃过亏，因此修改此处！！！&gt; # 附issue：https://github.com/kubernetes/kubeadm/issues/1321 &gt; # 找了好久终于！！！哭&gt; #scp /etc/kubernetes/pki/*.crt $host:/etc/kubernetes/pki/&gt; #scp /etc/kubernetes/pki/*.key $host:/etc/kubernetes/pki/&gt; #scp /etc/kubernetes/pki/*.pub $host:/etc/kubernetes/pki/&gt; #scp /etc/kubernetes/admin.conf $host:/etc/kubernetes/admin.conf&gt; scp /etc/kubernetes/pki/ca.* "$&#123;USER&#125;"@$host:/etc/kubernetes/pki/&gt; scp /etc/kubernetes/pki/sa.* "$&#123;USER&#125;"@$host:/etc/kubernetes/pki/&gt; scp /etc/kubernetes/pki/front-proxy-ca.* "$&#123;USER&#125;"@$host:/etc/kubernetes/pki/&gt; scp /etc/kubernetes/pki/etcd/ca.* "$&#123;USER&#125;"@$host:/etc/kubernetes/pki/etcd/&gt; scp /etc/kubernetes/admin.conf "$&#123;USER&#125;"@$host:/etc/kubernetes/&gt; done&gt; 1234567891011121314151617&gt; tree /etc/kubernetes/pki/&gt; /etc/kubernetes/pki/&gt; ├── apiserver.crt&gt; ├── apiserver-etcd-client.crt&gt; ├── apiserver-etcd-client.key&gt; ├── apiserver.key&gt; ├── apiserver-kubelet-client.crt&gt; ├── apiserver-kubelet-client.key&gt; ├── ca.crt&gt; ├── ca.key&gt; ├── front-proxy-ca.crt&gt; ├── front-proxy-ca.key&gt; ├── front-proxy-client.crt&gt; ├── front-proxy-client.key&gt; ├── sa.key&gt; └── sa.pub&gt; 123456789101112131415161718192021222324252627282930# 分别修改对应ip 参数`--node-name`，在在master2、3执行master1出现的join命令即可 步骤如下：# 对于控制平面node 增加 `--experimental-control-plane` 即可，只有master node采用，更方便。当然还有复杂的通过原子操作来add etcd node 增加 etcd-clusterkubeadm join x.x.x.4:8443 --token bnnsb7.amapp1t78llxn54d --discovery-token-ca-cert-hash sha256:520ef89be84c30e480db6d441a7e4179634a9455f0009e249ebe8f35fa792087 --experimental-control-plane --node-name=x.x.x.2#(对应主机ip) # 等待kube-proxy flannel启动成功即可# 这种方案，kubectl get cs直观的看不到etcd集群的，可通过一下命令：kubectl exec -n kube-system etcd-x.x.x.1 -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=https://x.x.x.1:2379 member list# 这里把master2、3节点采用原子操作加入etcd集群步骤写下来：# 配置证书kubeadm init phase certs all --config kubernetes.yaml # etcd证书kubeadm init phase etcd local --config kubernetes.yaml # 生成kubelet配置文件kubeadm init phase kubeconfig kubelet --config kubernetes.yaml # 启动kubeletkubeadm init phase kubelet-start --config kubernetes.yaml # 配置kubectl命令mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config# etcd集群加节点kubectl exec -n kube-system etcd-x.x.x.1 -- etcdctl --ca-file /etc/kubernetes/pki/etcd/ca.crt --cert-file /etc/kubernetes/pki/etcd/peer.crt --key-file /etc/kubernetes/pki/etcd/peer.key --endpoints=https://x.x.x.1:2379 member add etcd2 https://x.x.x.2:2380# 启动 kube-apiserver、kube-controller-manager、kube-schedulerkubeadm init phase kubeconfig all --config kubernetes.yamlkubeadm init phase control-plane all --config kubernetes.yaml# 标记masterkubeadm init phase mark-control-plane --config kubernetes.yaml 1234567891011121314151617181920kubectl get po -n kube-systemNAME READY STATUS RESTARTS AGEcoredns-9f9d9c76-4zrt4 1/1 Running 0 2dcoredns-9f9d9c76-dqd4c 1/1 Running 0 2dkube-apiserver-zjjh-rq-k8s-1 1/1 Running 0 2dkube-apiserver-zjjh-rq-k8s-2 1/1 Running 0 2dkube-apiserver-zjjh-rq-k8s-3 1/1 Running 0 2dkube-controller-manager-zjjh-rq-k8s-1 1/1 Running 0 2dkube-controller-manager-zjjh-rq-k8s-2 1/1 Running 0 2dkube-controller-manager-zjjh-rq-k8s-3 1/1 Running 0 2dkube-flannel-ds-amd64-7ghn7 1/1 Running 1 2dkube-flannel-ds-amd64-9cqts 1/1 Running 0 2dkube-flannel-ds-amd64-f57nh 1/1 Running 1 2dkube-proxy-8fwts 1/1 Running 0 2dkube-proxy-95tjb 1/1 Running 0 2dkube-proxy-bls94 1/1 Running 0 2dkube-scheduler-zjjh-rq-k8s-1 1/1 Running 0 2dkube-scheduler-zjjh-rq-k8s-2 1/1 Running 0 2dkube-scheduler-zjjh-rq-k8s-3 1/1 Running 0 2dkubernetes-dashboard-67d49f7868-x79wf 1/1 Running 0 76m 2、Work节点加入集群 输入master节点初始化成功之后出现的join命令，出现kubectl get nodes即成功 12&gt; kubeadm join x.x.x.4:8443 --token bnnsb7.amapp1t78llxn54d --discovery-token-ca-cert-hash sha256:520ef89be84c30e480db6d441a7e4179634a9455f0009e249ebe8f35fa792087&gt; 3、集群验证1234567891011121314151617181920# 节点状态[root@master] ~$ kubectl get nodes# 组件状态[root@master] ~$ kubectl get cs# 服务账户[root@master] ~$ kubectl get serviceaccount# 集群信息[root@master] ~$ kubectl cluster-info# 验证dns功能[root@master] ~$ kubectl run curl --image=radial/busyboxplus:curl -it[ root@curl-66959f6557-r4crd:/ ]$ nslookup kubernetes.defaultServer: 10.96.0.10Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.localName: kubernetes.defaultAddress 1: 10.96.0.1 kubernetes.default.svc.cluster.local 附录： 打算把以前kubernetes二进制安装的步骤，也迁过来。kubeadm已经GA，之后也会一直用kubeadm了 kubeadm token create --print-join-command可以获取 kubeadm join命令，比以前真的方便很多了 =。= 12&gt; # kubelet.service配置文件 /use/lib/systemd/system/kubelet.service.d/10-kubeadm.conf&gt; 参考链接： kubeadm安装1.13.5：https://blog.csdn.net/fanren224/article/details/86573264#2master1_165 二进制安装：https://github.com/mritd/ktool 二进制安装kubernetes_v1.13.4：https://mritd.me/2019/03/16/set-up-kubernetes-1.13.4-cluster/]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Kubernetes_ha</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python_set集合操作]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2FPython-set%E9%9B%86%E5%90%88%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[Python_set集合类型小结下面来点简单的小例子说明把。 1234x = set('spam')y = set(['h','a','m'])x, y(set(['a', 'p', 's', 'm']), set(['a', 'h', 'm'])) 再来些小应用。 12345678&gt; &gt; &gt; x &amp; y # 交集&gt; &gt; &gt; set(['a', 'm'])&gt; &gt; &gt; x | y # 并集&gt; &gt; &gt; set(['a', 'p', 's', 'h', 'm'])&gt; &gt; &gt; x - y # 差集&gt; &gt; &gt; set(['p', 's']) 记得以前个网友提问怎么去除海量列表里重复元素，用hash来解决也行，只不过感觉在性能上不是很高，用set解决还是很不错的，示例如下： 1234567a = [11,22,33,44,11,22]b = set(a)bset([33, 11, 44, 22])c = [i for i in b]c[33, 11, 44, 22] 很酷把，几行就可以搞定。 集合集合用于包含一组无序的对象。要创建集合，可使用set()函数并像下面这样提供一系列的项： 12s = set([3,5,9,10]) #创建一个数值集合t = set("Hello") #创建一个唯一字符的集合 与列表和元组不同，集合是无序的，也无法通过数字进行索引。此外，集合中的元素不能重复。例如，如果检查前面代码中t集合的值，结果会是: 12&gt; &gt; &gt; tset(['H', 'e', 'l', 'o']) 注意只出现了一个’l’。 集合支持一系列标准操作，包括并集、交集、差集和对称差集，例如： 1234a = t | s # t 和 s的并集b = t &amp; s # t 和 s的交集c = t – s # 求差集（项在t中，但不在s中）d = t ^ s # 对称差集（项在t或s中，但不会同时出现在二者中） 基本操作： 123t.add('x') # 添加一项s.update([10,37,42]) # 在s中添加多项 1234567891011121314151617181920212223242526272829303132333435363738t.remove('H')# 使用remove()可以删除一项：len(s)# set 的长度x in s# 测试 x 是否是 s 的成员x not in s# 测试 x 是否不是 s 的成员s.issubset(t)s &lt;= t# 测试是否 s 中的每一个元素都在 t 中s.issuperset(t)s &gt;= t# 测试是否 t 中的每一个元素都在 s 中s.union(t)s | t# 返回一个新的 set 包含 s 和 t 中的每一个元素s.intersection(t)s &amp; t# 返回一个新的 set 包含 s 和 t 中的公共元素s.difference(t)s - t# 返回一个新的 set 包含 s 中有但是 t 中没有的元素s.symmetric_difference(t)s ^ t# 返回一个新的 set 包含 s 和 t 中不重复的元素s.copy()# 返回 set “s”的一个浅复制 请注意：union(), intersection(), difference() 和 symmetric_difference() 的非运算符（non-operator，就是形如 s.union()这样的）版本将会接受任何 iterable 作为参数。相反，它们的运算符版本（operator based counterparts）要求参数必须是 sets。这样可以避免潜在的错误，如：为了更可读而使用 set(‘abc’) &amp; ‘cbs’ 来替代 set(‘abc’).intersection(‘cbs’)。从 2.3.1 版本中做的更改：以前所有参数都必须是 sets。 另外，Set 和 ImmutableSet 两者都支持 set 与 set 之间的比较。两个 sets 在也只有在这种情况下是相等的：每一个 set 中的元素都是另一个中的元素（二者互为subset）。一个 set 比另一个 set 小，只有在第一个 set 是第二个 set 的 subset 时（是一个 subset，但是并不相等）。一个 set 比另一个 set 打，只有在第一个 set 是第二个 set 的 superset 时（是一个 superset，但是并不相等） 子 set 和相等比较并不产生完整的排序功能。例如：任意两个 sets 都不相等也不互为子 set，因此以下的运算都会返回 False：a&lt;b, a==b, 或者a&gt;b。因此，sets 不提供 cmp 方法。 运算符123456789101112131415161718192021222324252627282930313233hash(s)# 返回 s 的 hash 值s.update(t)s |= t# 返回增加了 set “t”中元素后的 set “s”s.intersection_update(t)s &amp;= t# 返回只保留含有 set “t”中元素的 set “s”s.difference_update(t)s -= t# 返回删除了 set “t”中含有的元素后的 set “s”s.symmetric_difference_update(t)s ^= t# 返回含有 set “t”或者 set “s”中有而不是两者都有的元素的 set “s”s.add(x)# 向 set “s”中增加元素 xs.remove(x)# 从 set “s”中删除元素 x, 如果不存在则引发 KeyErrors.discard(x)# 如果在 set “s”中存在元素 x, 则删除s.pop()# 删除并且返回 set “s”中的一个不确定的元素, 如果为空则引发 KeyError，随机删除并打印s.clear()# 删除 set “s”中的所有元素 请注意：非运算符版本的 update(), intersection_update(), difference_update()和symmetric_difference_update()将会接受任意 iterable 作为参数。从 2.3.1 版本做的更改：以前所有参数都必须是 sets。 还请注意：这个模块还包含一个 union_update() 方法，它是 update() 方法的一个别名。包含这个方法是为了向后兼容。程序员们应该多使用 update() 方法，因为这个方法也被内置的 set() 和 frozenset() 类型支持。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Python_碎片化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python_list或dict合并]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2FPython-list%E6%88%96dict%E5%90%88%E5%B9%B6%2F</url>
    <content type="text"><![CDATA[对于两个list、两个dict合并数据1.两个字典：12a= &#123;'a':1,'b':2,'c':3&#125; b= &#123;'aa':11,'bb':22,'cc':33&#125; 合并1：dict(a,**b) 操作如下： 1c = dict(a,**b) 合并2：dict(a.items()+b.items()) 如下： 1c = dict(a.item() + b.items()) 合并3：c = {} c.update(a) c.update(b) 输出c 如下： 123c = &#123;&#125;c.update(a)c.update(b) 2.两个list合并：12a = [1,2,3,4,5,6] b = ['a','b','c','d'] 合并1：a+b 如下： 1c = a+b 合并2：a+=b 这时a的值变成了合并后的结果，如下： 1a += b # ====&gt; a = a+b 合并3：a.extend(b) 和+=结果一样，输出a 如下： 1a.extend(b) # a += b 合并4：a.append(b)将b看成list一个元素和a合并成一个新的list，合并后的结果输入a 如下： 1a.append(b) # list add other list &gt;&gt; [1,2,3,[1,2,3]] 合并5：a[0:0] = b 使用切片，如下： 1a[0:0] = b 附录： 123456789101112131415161718192021&gt; # 方法一：&gt; list1 = ['k1','k2','k3']&gt; list2 = ['v1','v2','v3']&gt; dic = dict(map(lambda x,y:[x,y],list1,list2))&gt; &gt;&gt;&gt; print(dic)&gt; &#123;'k3': 'v3', 'k2': 'v2', 'k1': 'v1'&#125;&gt; &gt; # 方法二：&gt; &gt;&gt;&gt; dict(zip(list1,list2))&gt; &#123;'k3': 'v3', 'k2': 'v2', 'k1': 'v1'&#125;&gt; &gt; # 方法三：&gt; &gt;&gt;&gt; l1=[1,2,3,4,5,6]&gt; &gt;&gt;&gt; l2=[4,5,6,7,8,9]&gt; &gt;&gt;&gt; &#123;k:v for k,v in zip(l1,l2)&#125;&gt; &#123;1: 4, 2: 5, 3: 6, 4: 7, 5: 8, 6: 9&#125;&gt; &gt; &gt;&gt;&gt; x = &#123;1: 4, 2: 5, 3: 6, 4: 7, 5: 8, 6: 9&#125;&gt; &gt;&gt;&gt; &#123;v:k for k,v in x.items()&#125; #反过来 将字典中的v和k调换&gt; &#123;4: 1, 5: 2, 6: 3, 7: 4, 8: 5, 9: 6&#125;&gt;]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Python_碎片化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python_list交集、差集和并集]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2FPython-list%E4%BA%A4%E9%9B%86%E3%80%81%E5%B7%AE%E9%9B%86%E5%92%8C%E5%B9%B6%E9%9B%86%2F</url>
    <content type="text"><![CDATA[举例说明 a_list = [1,2,3,4]b_list = [1,4,5] 一. 差集很明显结果是[2,3,5]，下面我们说一下具体方法。方法a.正常法： 1234ret_list = []for item in a_list: if item not in b_list: ret_list.append(item) 方法b.简化版： 1ret_list = [item for item in a_list if item not in b_list] 方法c.高级版： 1ret_list = list(set(a_list)^set(b_list)) 二. 并集很明显结果是[1,2,3,4,5],下面是具体方法： 1ret_list = list(set(a_list).union(set(b_list))) 三. 交集很明显结果是[1,4]，下面是具体方法： 1ret_list = list( ( set(a_list).union(set(b_list)) )^( set(a_list)^set(b_list) ) )]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Python_碎片化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python_问题小结(2)]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2FPython-%E9%97%AE%E9%A2%98%E5%B0%8F%E7%BB%93-2%2F</url>
    <content type="text"><![CDATA[环境：未特别注明的话，均为Python 3.6.5，IDE：VS code 遇到的都是基础问题，分了两篇。 担心一篇写多了会看不下去 https://blog.csdn.net/xinzechen/article/details/89446322 vscode的问题 『1』关于python无法显示中文的问题：SyntaxError: Non-ASCII character ‘\xe4’ in file test.py on line 3, but no encoding declared。python：2.7.15 解决办法： 在以后的每一个需要显示汉字的python文件中， 可以采用如下方法在 #!/usr/bin/python的下一行加上一句话来定义编码格式， 我以utf-8编码为例。 1234567891011### 第一种：#!/usr/bin/python#coding:utf-8### 第二种：#!/usr/bin/python#-*-coding:utf-8 -*-### 第三种：#!/usr/bin/python#vim: set fileencoding:utf-8 『2』有时候在去数据的时候会遇到数据内有中文，但是取出来的是unicode码，用了很多方法也转不了中文解决: “反编码”我自己起的名字，大概意思就是我得到一串字符，是unicode码，如：‘\u53eb\u6211’，进行反编码后得到其对应的汉字。 12345678910111213141516f='\u53eb\u6211'print fprint(f.decode('unicode-escape'))list_words为列表类型str(list_words).decode( 'string_escape' )#encoding=utf-8 import json list_words = [ '你', '我', '他' ]print( list_words ) print( str(list_words).decode( 'string_escape' ) ) # 正常显示汉字（可靠性不高，原因尚不明确） list_words_result = json.dumps( list_words, encoding='UTF-8', ensure_ascii=False ) # 正常显示汉字 （经实验可靠性较高） 结果为: 12\u53eb\u6211 叫我 『3』在编译时会出现这样的错：IndentationError:expected an indented block说明此处需要缩进，你只要在出现错误的那一行，按空格或Tab（但不能混用）键缩进就行。 『4』SyntaxError:invalid syntax ：语法错误。最常见的，最基础的错误 英文是“非法语句”的意思。漏标点符号（例如漏分号，漏&amp;号），多标点符号，拼写错，等等都会造成这种错 『5』url连接超时解决：1）使用的是urllib不是urllib2，所以无法直接在urlopen里面加timeout参数，只能是设置全局脚本的超时时间 12import socketsocket.setdefaulttimeout(60) 设置全局的超时时间为60s。 2）urllib，urllib2常会一起使用（两者分别提供不同的功能） 1234html = urllib.urlopen(url).read()# 修改为import urllib2html = urllib2.urlopen(url, timeout=60).read() 『6』UnboundLocalError: local variable ‘appname’ referenced before assignment：123456&#123;u&apos;batchrealnameverifytask&apos;: 99.41, u&apos;bptask100000188408&apos;: 98.77, u&apos;clsexpflowmaintask576&apos;: 101.28&#125;Traceback (most recent call last): File &quot;get_app_cpu_from_influxdb.py&quot;, line 89, in &lt;module&gt; appname = get_sql_info(key) File &quot;get_app_cpu_from_influxdb.py&quot;, line 28, in get_sql_info return appname 变量问题：变量在被赋值之前被使用定义 『7』TypeError: sequence item 1: expected string or Unicode, NoneType found期望值未发现：str或者unicode类型 12345message = u"应用的CPU利用率超过%s,请检查应用:%s"%(str(threshold)+'%',",".join(app_name))# =====================&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;&gt;message = "应用的CPU利用率超过%s,请检查应用:%s"%(str(threshold)+'%',",".join(app_name)) 『8』UnicodeDecodeError: ‘ascii’ codec can’t decode byte 0xe5 in position 1: ordinal not in range(128)1234# 关于编码问题：import sysreload(sys)sys.setdefaultencoding( "utf-8" ) 『9』IndentationError: unindent does not match any outer indentation level【问题】 一个python脚本，本来都运行好好的，然后写了几行代码，而且也都确保每行都对齐了，但是运行的时候，却出现语法错误： 1IndentationError: unindent does not match any outer indentation level 【解决过程】 1.对于此错误，最常见的原因是，的确没有对齐。但是我根据错误提示的行数，去代码中看了下，没啥问题啊。 都是用TAB键，对齐好了的，没有不对齐的行数啊。 2.以为是前面的注释的内容影响后面的语句的语法了，所以把前面的注释也删除了。 结果还是此语法错误。 3.后来折腾了半天，突然想到了，把当前python脚本的所有字符都显示出来看看有没有啥特殊的字符。tab与空格混用了 『10』request https报错：12# 忽略警告：InsecureRequestWarning: Unverified HTTPS request is being made. Adding certificate verification is strongly advised.requests.packages.urllib3.disable_warnings(） 『11』python安装MySQLdb的时候一直有问题在Linux下 1pip install MySQL-python 如果安装之后仍不能正常运行，尝试用yum install MySQL-python，因为这个模块需要一些第三方程序来运行的。]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Python_problem</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python_问题小结(1)]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2FPython-%E9%97%AE%E9%A2%98%E5%B0%8F%E7%BB%93-1%2F</url>
    <content type="text"><![CDATA[环境：未特别注明的话，均为Python 3.6.5，IDE：VS code 『1』TypeError: unbound method 问题今天执行了下之前写的Python接口文件，源码如下: 12345678910__author__ = 'Administrator'#coding:utf-8from readData import dictionaryreadIt = &#123;&#125;readIt = dictionary.onlyCellValue("E:\python\API\eadData.xls", "Sheet1", 1)print readItfor key in readIt: temp_list = readIt[key] for i in range(0, len(temp_list)): print "第"+(i+1)+"个参数为"+temp_list[i] 在运行时报错：TypeError: unbound method onlyCellValue() must be called with dictionary instance as first argument (got str instance instead)，后经在网上查看，发现时由于调用其他类时，未在后面添加括号，添加括号后，运行正常。这是由于未添加括号情况下，未被认为是类的实例，故报此错； 改正后的：readIt = dictionary().onlyCellValue(“E:\python\API\eadData.xls”, “Sheet1”, 1) 『2』Python访问MySQL听说很多人都使用著名的MySQLdb来访问MySQL(当然还有pymysql)，但是它并不支持Python 3.x的版本。所以要另寻出路。那就是mysql-connector-python，这个貌似是MySQL官方提供的，并且它不依赖于MySQL C客户端library，我下载的是1.1.4版本，下载解压之后，会看到setup.py文件，这样安装： 1sudo python3.2 setup.py install 然后在程序中就可以用了。下面给出一段查询MySQL记录的示例代码： 1234567891011121314151617181920212223242526272829303132import mysql.connectorimport sys __author__ = 'codelast' username = 'root'password = 'xxx'host = '127.0.0.1'db = 'mydb' connection = mysql.connector.connect(user=username, password=password, host=host, database=db)cursor = connection.cursor() sql = "SELECT * FROM my_table WHERE id = 9"try: cursor.execute(sql) # 打印查询到的记录的行数 data = cursor.fetchall() print(len(data)) # 输出所有记录 for (ID, name) in data: print("name:[%s]" % (name)) except mysql.connector.Error as err: print("Failed to query table, detail: &#123;&#125;".format(err.msg)) sys.exit() connection.commit()cursor.close()connection.close() 上面的代码很简单，无非就是从my_table表里查询一些记录，再打印出来。注意 for (ID, name) 中的括号里要写全该表中，你查询的所有字段名，否则会报错。还有其他遍历查询结果的方法，后面会继续陈述。 『3』逆序遍历list123myList = [1, 2, 3]for item in reversed(myList): print(item) 输出： 123321 注意是 reversed 不是 reverse。这只是逆序遍历，myList中的数据顺序并不会改变。 『4』使用MySQLdb访问数据库时，“TypeError: execute() takes at most 3 arguments (4 given)”错误的解决办法Python版本：2.7.3使用2.7.3版本的Python时，访问MySQL的最佳方案应该数使用MySQLdb了。如果在执行SQL时，你遇到了上面所说的问题，那么你可能是像下面这样写导致的： 12sql = "INSERT INTO my_table (field1, field2) VALUES (%s, %s)"cursor.execute(sql, "a", "b") 这是错误的，其实这货根本不是这样用的，当参数多于一个时，你要把它们放在一个tuple里传进去： 12sql = "INSERT INTO my_table (field1, field2) VALUES (%s, %s)"cursor.execute(sql, ("a", "b")) 例如这个链接有个例子。 『5』使用 lxml 库生成XML（字符串）Python版本：2.7.3直接看代码： 12345678910111213141516171819202122232425#coding:UTF-8"""XML生成器。文件名：xmlGenerator.py""" __author__ = 'Darran Zhang @ codelast.com' from lxml import etree class XMLGenerator: def __init__(self): pass def generate_xml(self): commands = etree.Element('Commands') command = etree.SubElement(commands, 'Command') from_user = etree.SubElement(command, 'FromUser') from_user.text = u'abc' cmd = etree.SubElement(command, 'Cmd') cmd.text = u'mmmmmmmmmmmmmm' cmd_extra_data = etree.SubElement(command, 'CmdExtraData') cmd_extra_data.text = u'eeeeeeeeee' return etree.tostring(commands, pretty_print=True, xml_declaration=True, encoding='utf-8') 测试代码： 1234from xmlGenerator import XMLGenerator xmlGen = XMLGenerator()print(xmlGen.generate_xml()) 12345678&lt;?xml version=&apos;1.0&apos; encoding=&apos;utf-8&apos;?&gt;&lt;Commands&gt; &lt;Command&gt; &lt;FromUser&gt;abc&lt;/FromUser&gt; &lt;Cmd&gt;mmmmmmmmmmmmmm&lt;/Cmd&gt; &lt;CmdExtraData&gt;eeeeeeeeee&lt;/CmdExtraData&gt; &lt;/Command&gt;&lt;/Commands&gt; 可见非常简单。 『6』在PyCharm中无法安装MySQL-python这个package的一个解决办法在PyCharm中可以直接搜索Python package并安装，我遇到的一个问题是：无法安装，错误提示的其中一段为： EnvironmentError: mysql_config not found 你需要保证你的Ubuntu已经安装了以下这些东西： 1sudo apt-get install mysql-server mysql-client libmysqlclient-dev 然后再试，一切OK。 『7』”ValueError: zero length field name in format”错误的解决办法Python版本：2.6.6错误出在如下语句： 1"&#123;&#125;\t&#123;&#125;".format(123, 456) 我遇到的出问题的Python版本为2.6.6（不知道是不是2.6.*及低版本都有此问题），反正Python 2.7.3就没有这个问题了。上面的代码无非主就是把123和456代替两个大括号。在旧版本的Python上（具体从哪个版本开始不用这样写，我不确定），你要有多个需要format的field时，需要指定它们的顺序： 1"&#123;0&#125;\t&#123;1&#125;".format(123, 456) 这样就没问题了。 『8』对一个字典(dict)，按value进行排序Python版本：2.6.6，2.6.9均测试可用（Python3里没有cmp方法了，所以不能用） 123sortedList = sorted(myDict.items(), lambda x, y: cmp(x[1], y[1]), reverse=True)for (k, v) in sortedList: print("&#123;0&#125;\t&#123;1&#125;".format(k,v)) 其中，myDict是你要将其排序的字典，sortedList是排序之后的结果，变成了一个list，里面是若干个tuple，每个tuple里是一对(key,value)，所以后面用那样的方式对它进行了遍历。 『9』判断一个字典（dict）中是否包含指定的key12345678910d = dict()# 向字典中添加两个元素d[1] = 'abc'd[2] = 'def' # 检查 3 这个key是否在字典中，结果输出的是“NO”if 3 in d: print('YES')else: print('NO') 『10』Python正则简单示例先看代码： 12345code = 'String url = "http://item.jd.com/1148104.html?erpad_source=abc";'pattern = re.compile('.*\"(.*)\"')match = pattern.match(code)if match: print(match.group(1)) 输出： 1http://item.jd.com/1148104.html?erpad_source=abc 说明：上面的代码是想把字符串“code”中的双引号里的那个URL打印出来。正则表达式 .\”(.)\” 中的小括号就是第1个group，匹配上的话可以用 group(1) 获取之。 『11』try…except的示例参考123456try: # 可能会出现异常的一段代码 command_1 # 如果command_1出现异常，则不执行command_1以及之后的语句 command_2 # command_1如果正常，则会执行except: # try中任意一行语句出现异常，直接跳转至except，程序继续运行 command_3 command_4 『』To be added…]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Python_problem</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker的问题总结]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2FDocker%E7%9A%84%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[问题：1、Docker18.06.1-ce安装(RHEL7)后启动失败, 原因(xfsprogs版本过低)* 问题分析: 使用/usr/bin/dockerd启动测试发现是文件系统无法创建 1 WARN[2018-09-15T10:47:39.886191433+08:00] Usage of loopback devices is strongly discouraged for production use. Please use --storage-opt dm.thinpooldev or use man dockerd to refer to dm.thinpooldev section. storage-driver=devicemapper 2 INFO[2018-09-15T10:47:40.220046362+08:00] Creating filesystem xfs on device docker-253:4-17133027-base, mkfs args: [-m crc=0,finobt=0 /dev/mapper/docker-253:4-17133027-base] storage-driver=devicemapper 3 INFO[2018-09-15T10:47:40.221230038+08:00] Error while creating filesystem xfs on device docker-253:4-17133027-base: exit status 1 storage-driver=devicemapper 4 ERRO[2018-09-15T10:47:40.221258767+08:00] [graphdriver] prior storage driver devicemapper failed: exit status 1 5 Error starting daemon: error initializing graphdriver: exit status 1 第三行日志可以找到mkfs的执行参数, 执行发现参数不正确 1 2 INFO[2018-09-15T10:47:40.220046362+08:00] Creating filesystem xfs on device docker-253:4-17133027-base, mkfs args: [-m crc=0,finobt=0 /dev/mapper/docker-253:4-17133027-base] storage-driver=devicemapper * 解决问题 1234yum install xfsprogs # 但是内核版本过低，操作系统7.0# 18.06最好升级系统版本到7.4，只能装会1.12版本,问题来了，私有镜像库登入不了# 17.12.0兼容 2、docker rm container 失败现象： docker version：17.12.0-ce 偶尔出现docker跟daemon守护进程失去联系 docker rm不掉容器 解决方法：docker守护进程重启了，现在只能重启解决 链接： https://github.com/moby/moby/issues/36002 3、指北：一、 原因：版本不兼容 7.2版本 升级系统7.4可解决 二、no matching entries in passwd file 进不去容器 重启docker 三、standard_init_linux.go:195: exec user process caused “exec format error” 进不去容器； 启动脚本错误 启动脚本=======》 #！/bin/bash 四、内核：enterd disabled state 被禁用状态 主机内核有问题]]></content>
      <categories>
        <category>Docker</category>
      </categories>
      <tags>
        <tag>Docker</tag>
        <tag>Docker_problem</tag>
        <tag>Docker的一些东西</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[安静写点东西]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2F%E5%AE%89%E9%9D%99%E5%86%99%E7%82%B9%E4%B8%9C%E8%A5%BF%2F</url>
    <content type="text"><![CDATA[Come on ٩(๑.◡.๑)۶ ，Please ecter Password (*ﾉ▽ﾉ). Incorrect Password! No content to display! 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]]></content>
      <tags>
        <tag>生活需要一点方式</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes_填坑]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2FKubernetes-%E5%A1%AB%E5%9D%91%2F</url>
    <content type="text"><![CDATA[更新19.4.5一、etcd 单点问题默认kubeadm创建的集群会在内部启动一个单点的 etcd，当然大部分情况下 etcd 还是很稳定的，但是一但 etcd 由于某种原因挂掉，这个问题会非常严重，会导致整个集群不可用。具体原因是 etcd 存储着 kubernetes 各种元数据信息；包括 kubectl get pod等等基础命令实际上全部是调用 RESTful API 从 etcd 中获取的信息；所以一但 etcd 挂掉以后，基本等同于kubectl命令不可用，集群各节点也会因无法从 etcd 获取数据而出现无法调度，最终挂掉。 解决办法：是在使用kubeadm创建集群时使用 –external-etcd-endpoints 参数指定外部 etcd 集群，此时kubeadm 将不会在内部创建 etcd，转而使用外部我们指定的 etcd 集群，如果外部 etcd 集群配置了 SSL 加密，那么还需要配合 –external-etcd-cafile、–external-etcd-certfile、–external-etcd-keyfile 三个参数指定 etcd 的 CA证书、CA签发的使用证书和私钥文件，命令如下 12345# 非 SSL# kubeadm init --external-etcd-endpoints http://x.x.x.1:2379# etcd SSL# kubeadm init --external-etcd-endpoints https://x.x.x.1:2379 --external-etcd-cafile /path/to/ca --external-etcd-certfile /path/to/cert --external-etcd-keyfile /path/to/privatekey 二、etcd 不可与 master 同在‘愿上帝与你同在’……这个坑是由于kubeadm的 check 机制的 bug 造成的，目前还没有修复；表现为 当 etcd 与 master 在同一节点时，kubeadm init 会失败，同时报错信息提示’已经存在了 /var/lib/etcd 目录，或者 2379 端口被占用’；因为默认kubeadm会创建 etcd，而默认的 etcd 会占用这个目录和 2379 端口，即使你加了--external-etcd-endpoints参数，kubeadm仍然会检测这两项条件是否满足，不满足则禁止 init 操作 解决办法：就是要么外部的 etcd 更换数据目录(/var/lib/etcd)和端口，要么干脆不要和 master 放在同一主机即可 三、巨大的日志熟悉的小伙伴应该清楚，基本上每个 kubernetes 组件都会有个通用的参数 --v；这个参数用于控制 kubernetes 各个组件的日志级别，在早期(alpha)的 kubeadm 版本中，如果不进行调整，默认创建集群所有组件日志级别全部为 --v=4 即最高级别输出，这会导致在业务量大的时候磁盘空间以 ‘我去尼玛’ 的速度增长，尤其是 kube-proxy 组件的容器，会疯狂吃掉你的磁盘空间，然后剩下懵逼的你不知为何。在后续的版本中(beta)发现日志级别已经降到了 --v=2，不过对于完全不怎么看日志的我来说还是无卵用…… 解决办法有两种方案: 1、如果已经 –v=4 跑起来了(检查方法就是随便 describe 一个 kube-proxy 的容器，看下 command 字段就能看到)，并且无法停止重建集群，那么最简单的办法就是使用kubectl edit ds xxx方式编译一下相关 ds 文件等，然后手动杀掉相关 pod，让 kubernetes 自动重建即可，如果命令行用着不爽也可以通过 dashboard 更改 2、如果还没开始搭建，或者可以停掉重建，那么只需在kubeadm init之前export KUBE_COMPONENT_LOGLEVEL=&#39;--v=0&#39;即可 四、新节点加入 dns 要你命当 kubeadm 创建好集群以后，如果有需要增加新节点，那么在 kubeadm join 之后务必检查 kube-dns 组件，dns 在某些(weave 启动不完整或不正常)情况下，会由于新节点加入而挂掉，此时整个集群 dns 失效，所以最好 join 完观察一会 dns 状态，如果发现不正常马上杀掉 dns pod，让 kubernetes 自动重建；如果情况允许最好全部 join 完成后直接干掉 dns 让 kubernetes 重建一下 五、单点的 dns 浪起来让你怕（更新-19.4.5）kubeadm 创建的 dns 默认也是单点的，而 dns 至关重要，只要一挂瞬间整个集群全部 game over；不过暂时还是没有发现能在 init 时候创建多个 dns 的方法；不过在集群创建后可以通过kubectl edit deploy kube-dns的方式修改其副本数量，让其创建多个副本即可，目前新版本kubernetes所使用的dns已经不是单pod三container，而是启动两个pod的coredns，解决单点问题 六、coredns一直处于ContainerCreating状态 Node加入集群中后一直处于NotReady状态，查看kube-system的状态，发现coredns一直处于ContainerCreating状态，flannel启动正常 12345# kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster1.rsq.com NotReady master 16h v1.13.0node01.rsq.com NotReady &lt;none&gt; 16h v1.13.0node02.rsq.com NotReady &lt;none&gt; 8m39s v1.13.0 查看kubelet服务状态，看最后几行的报错 1234567891011121314151617181920212223242526# systemctl status kubelet● kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: disabled) Drop-In: /etc/systemd/system/kubelet.service.d └─10-kubeadm.conf Active: active (running) since Wed 2018-12-12 09:24:44 CST; 6min ago Docs: https://kubernetes.io/docs/ Main PID: 123631 (kubelet) Memory: 35.2M CGroup: /system.slice/kubelet.service └─123631 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --cgrou...Dec 12 09:30:57 master1.rsq.com kubelet[123631]: E1212 09:30:57.187292 123631 pod_workers.go:190] Error syncing pod cea84a11-fd24-11e8-a282-000c291e37c2 ("coredns-86c58d9df4-fzs9l_kube-...Dec 12 09:30:57 master1.rsq.com kubelet[123631]: E1212 09:30:57.187480 123631 pod_workers.go:190] Error syncing pod cea7ebef-fd24-11e8-a282-000c291e37c2 ("coredns-86c58d9df4-hrwvk_kube-...Dec 12 09:30:59 master1.rsq.com kubelet[123631]: E1212 09:30:59.187419 123631 pod_workers.go:190] Error syncing pod cea84a11-fd24-11e8-a282-000c291e37c2 ("coredns-86c58d9df4-fzs9l_kube-...Dec 12 09:30:59 master1.rsq.com kubelet[123631]: E1212 09:30:59.187607 123631 pod_workers.go:190] Error syncing pod cea7ebef-fd24-11e8-a282-000c291e37c2 ("coredns-86c58d9df4-hrwvk_kube-...Dec 12 09:31:00 master1.rsq.com kubelet[123631]: W1212 09:31:00.454147 123631 cni.go:203] Unable to update cni config: No networks found in /etc/cni/net.dDec 12 09:31:00 master1.rsq.com kubelet[123631]: E1212 09:31:00.454242 123631 kubelet.go:2192] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNot...initializedDec 12 09:31:01 master1.rsq.com kubelet[123631]: E1212 09:31:01.188877 123631 pod_workers.go:190] Error syncing pod cea7ebef-fd24-11e8-a282-000c291e37c2 ("coredns-86c58d9df4-hrwvk_kube-...Dec 12 09:31:01 master1.rsq.com kubelet[123631]: E1212 09:31:01.189259 123631 pod_workers.go:190] Error syncing pod cea84a11-fd24-11e8-a282-000c291e37c2 ("coredns-86c58d9df4-fzs9l_kube-...Dec 12 09:31:03 master1.rsq.com kubelet[123631]: E1212 09:31:03.187200 123631 pod_workers.go:190] Error syncing pod cea84a11-fd24-11e8-a282-000c291e37c2 ("coredns-86c58d9df4-fzs9l_kube-...Dec 12 09:31:03 master1.rsq.com kubelet[123631]: E1212 09:31:03.187730 123631 pod_workers.go:190] Error syncing pod cea7ebef-fd24-11e8-a282-000c291e37c2 ("coredns-86c58d9df4-hrwvk_kube-...Hint: Some lines were ellipsized, use -l to show in full.# 附加PS# 报错10.244.0.1 网络已存在（更新） 产生问题原因： 一直报网络NotReady，我就感觉flannel组件出了问题， 最后网上搜了一些资料解决参考博客：coreDNS一直处于创建中解决解决办法：所有节点执行（我只在master节点先执行就解决问题了） 12# rm -rf /var/lib/cni/flannel/* &amp;&amp; rm -rf /var/lib/cni/networks/cbr0/* &amp;&amp; ip link delete cni0# rm -rf /var/lib/cni/networks/cni0/* 删除flannel组件，重新下载 12# docker rmi quay.io/coreos/flannel:v0.10.0-amd64# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 查看节点状态已经处于Ready状态 12345# kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster1.rsq.com Ready master 16h v1.13.0node01.rsq.com Ready &lt;none&gt; 16h v1.13.0node02.rsq.com Ready &lt;none&gt; 39m v1.13.0 七、kubeadm join报错12345678910在部署服务过程中，初始化之后重启了master节点，然后node节点在join进群的时候报错，提示证书是否过期等问题，报错信息如下：# kubeadm join 10.0.0.100:6443 --token qxl5b3.5b78nwu3gm1r4u6o --discovery-token-ca-cert-hash sha256:3e20fa8054cbc9000cf3d3586a05a01d8af5721b577856e93c7e243877393d21 --ignore-preflight-errors=Swap[preflight] Running pre-flight checks[WARNING Swap]: running with swap on is not supported. Please disable swap [WARNING SystemVerification]: this Docker version is not on the list of validated versions: 18.09.0. Latest validated version: 18.06[discovery] Trying to connect to API Server "10.0.0.100:6443"[discovery] Created cluster-info discovery client, requesting info from "https://10.0.0.100:6443"[discovery] Failed to request cluster info, will try again: [Get https://10.0.0.100:6443/api/v1/namespaces/kube-public/configmaps/cluster-info: dial tcp 10.0.0.100:6443: connect: connection refused][discovery] Failed to request cluster info, will try again: [Get https://10.0.0.100:6443/api/v1/namespaces/kube-public/configmaps/cluster-info: dial tcp 10.0.0.100:6443: connect: connection refused] 产生原因：有可能是时间不同步造成的，在初始化后重启master，重启后会报错 找了好多资料，没有找到可行的，最后kubeadm reset完美解决参考博文： k8s踩坑记 - kubeadm join 之 token 失效reset之后重新初始化 12# kubeadm reset # kubeadm init --kubernetes-version=v1.13.0 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 --token-ttl=0 --ignore-preflight-errors=Swap 创建所需文件 123# mkdir -p $HOME/.kube# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config# chown $(id -u):$(id -g) $HOME/.kube/config 查看节点 123# kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster1.rsq.com NotReady master 2m49s v1.13.0 八、Etcd二进制安装目录也会报错哦etcd服务启动后报错etcd cluster ID mismatch：检查service配置cluster选项有无问题，若无问题，则可能是此前的etcd bootstrap加速启动缓存残留导致，坑爹的是rm -rf /var/lib/etcd/*删除完了之后还是报错，必须rm -rf /var/lib/etcd/才能彻底清除，删除完成后记得再创建该路径mkdir /var/lib/etcd，否则会有类似报错： 1etcd.service: Failed at step CHDIR spawning /usr/local/bin/etcd: No such file or directory 九、二进制安装后重建相关组件会出现secrets报错123May 27 10:34:45 kube-node3 journal: E0527 02:34:45.767392 1 config.go:322] Expected to load root CA config from /var/run/secrets/kubernetes.io/serviceaccount/ca.crt, but got err: open /var/run/secrets/kubernetes.io/serviceaccount/ca.crt: no such file or directory May 27 10:34:45 kube-node3 journal: E0527 02:34:45.767392 1 config.go:322] Expected to load root CA config from /var/run/secrets/kubernetes.io/serviceaccount/ca.crt, but got err: open /var/run/secrets/kubernetes.io/serviceaccount/ca.crt: no such file or directory 分析：产生这个错误是因为Kubernetes默认创建的secrets资源不包含用于访问kube-apiserver的根证书 需要给apiserver设置安全证书，然后删除默认secrets，系统会自动产生新的secrets secrets一般集群安装时默认自动创建 1# kubectl delete secret secretname -n Xxx 十、kubeadm生成集群，加入节点时发现忘记了join token 怎么办？ 1.生成一条永久有效的token 12345678&gt; # kubeadm token create --ttl 0 &gt; o4avtg.65ji6b778nyacw68 &gt; &gt; # kubeadm token list &gt; TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS &gt; dxnj79.rnj561a137ri76ym &lt;invalid&gt; 2018-11-\#02T14:06:43+08:00 authentication,signing &lt;none&gt; system:bootstrappers:kubeadm:default-node-token &gt; o4avtg.65ji6b778nyacw68 &lt;forever&gt; &lt;never&gt; authentication,signing &lt;none&gt; system:bootstrappers:kubeadm:default-node-token &gt; 2.获取ca证书sha256编码hash值 1234&gt; # openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //' &gt; &gt; 2cc3029123db737f234186636330e87b5510c173c669f513a9c0e0da395515b0 &gt; 3.node节点加入 12&gt; # kubeadm join x.x.x.x:6443 --token o4avtg.65ji6b778nyacw68 --discovery-token-ca-cert-hash sha256:2cc3029123db737f234186636330e87b5510c173c669f513a9c0e0da395515b0 &gt; ###十一、]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
        <tag>kubernetes_problem</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes安装-kubeadm_1.13.5]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2FKubernetes%E5%AE%89%E8%A3%85-kubeadm-1-13-5%2F</url>
    <content type="text"><![CDATA[环境描述 kubernetes version：1.13.5 docker version：18.6.3 Redhat：7.6 x.x.x.1 master-1 kube-apiserver、kube-controller-manager、kube-scheduler、etcd、keepalive、docker x.x.x.2 master-2 kube-apiserver、kube-controller-manager、kube-scheduler、etcd、keepalive、docker x.x.x.3 master-3 kube-apiserver、kube-controller-manager、kube-scheduler、etcd、keepalive、docker x.x.x.4 vip x.x.x.5 node-1 kubelet、docker x.x.x.6 node-2 kubelet、docker 因为担心kubeadm起来的etcd不稳定，这里用到的etcd对于kubernetes来说作为外部etcd集群。即使用二进制安装etcd集群，其余组件用kubeadm来完成安装。 环境准备1、准备工作12345678910111213141516echo "1" &gt; /proc/sys/net/bridge/bridge-nf-call-iptables# 停防火墙systemctl stop firewalldsystemctl disable firewalldsystemctl disable firewalld# 关闭Swapswapoff -ased 's/.*swap.*/#&amp;/' /etc/fstab# 关闭防火墙systemctl disable firewalld &amp;&amp; systemctl stop firewalld &amp;&amp; systemctl status firewalld# 关闭Selinuxsetenforce 0sed -i "s/^SELINUX=enforcing/SELINUX=disabled/g" /etc/sysconfig/selinuxsed -i "s/^SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/configsed -i "s/^SELINUX=permissive/SELINUX=disabled/g" /etc/sysconfig/selinuxsed -i "s/^SELINUX=permissive/SELINUX=disabled/g" /etc/selinux/config 2、docker安装1、下载设置源123sudo yum install -y yum-utils \ device-mapper-persistent-data \ lvm2 123yum-config-manager \--add-repo \https://download.daocloud.io/docker/linux/centos/docker-ce.repo 2、安装docker123yum list docker-ce --showduplicates | sort -r # 列出docker-ce的版本listyum install docker-ce-&lt;版本号&gt; -y # -y 安装docker需要的依赖，其中有个container-selinux的也可以单独下载# rpm -ivh container-selinux-2.33-1.git86f33cd.el7.noarch.rpm 3、启动docker12systemctl start dockerdocker version # 验证docker安装是否完成并启动成功 3、kubeadm/kubelet/kubectl安装 各节点安装 1234567891011121314cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOFyum install -y kubelet-1.13.5 kubeadm-1.13.5 kubectl-1.13.5 --disableexcludes=kubernetes #禁用除kubernetes之外的仓库,要用 -y 参数，会自动安装kube-cni等插件systemctl start kubeletsystemctl enable kubelet # kubeadm 要求kubelet保持开机自启状态 4、keepalive安装 master节点安装： 123456789101112131415161718192021222324252627282930313233343536373839404142yum install -y keepalivedsystemctl start keepalivedvim /etc/keepalived/keepalived.confglobal_defs &#123; router_id LVS_k8s&#125;vrrp_script CheckK8sMaster &#123; script &quot;curl -k https://10.70.49.130:6443&quot; interval 3 timeout 9 fall 2 rise 2&#125;vrrp_instance VI_1 &#123; state MASTER interface ens256 # 网卡 virtual_router_id 61 # 主节点权重最高 依次减少 priority 120 advert_int 1 #修改为本地IP mcast_src_ip x.x.x.2 nopreempt authentication &#123; auth_type PASS auth_pass sqP05dQgMSlzrxHj &#125; unicast_peer &#123; x.x.x.1 #x.x.x.2 x.x.x.3 &#125; virtual_ipaddress &#123; x.x.x.4 &#125; track_script &#123; CheckK8sMaster &#125;&#125; kubernetes部署1、etcd二进制部署1234567891011# 1.cfssl签发证书wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64chmod +x cfssl_linux-amd64mv cfssl_linux-amd64 /usr/local/bin/cfsslwget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64chmod +x cfssljson_linux-amd64mv cfssljson_linux-amd64 /usr/local/bin/cfssljsonwget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64chmod +x cfssl-certinfo_linux-amd64mv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfoexport PATH=/usr/local/bin:$PATH 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 2.根据config.json文件的格式创建如下的ca-config.json文件,过期时间设置成了 87600hmkdir /root/sslcd /root/sslcfssl print-defaults config &gt; config.jsoncfssl print-defaults csr &gt; csr.jsoncat &gt; ca-config.json &lt;&lt;EOF&#123; "signing": &#123; "default": &#123; "expiry": "87600h" &#125;, "profiles": &#123; "kubernetes": &#123; "usages": [ "signing", "key encipherment", "server auth", "client auth" ], "expiry": "87600h" &#125; &#125; &#125;&#125;EOFcat &gt; ca-csr.json &lt;&lt;EOF&#123; "CN": "kubernetes", "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "k8s", "OU": "System" &#125; ]&#125;EOFcfssl gencert -initca ca-csr.json | cfssljson -bare ca 12345678910111213141516171819202122232425262728293031323334# 3.创建kubernetes-etcd证书cat &gt; kubernetes-etcd-csr.json &lt;&lt;EOF&#123; "CN": "kubernetes", "hosts": [ "x.x.x.1", "x.x.x.2", "x.x.x.3", "x.x.x.4", "127.0.0.1", "10.254.0.1", "kubernetes", "kubernetes.default", "kubernetes.default.svc", "kubernetes.default.svc.cluster", "kubernetes.default.svc.cluster.local" ], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "k8s", "OU": "System" &#125; ]&#125;EOFcfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubernetes-etcd-csr.json | cfssljson -bare etcd 123456# 4.目录证书文件如下etcd-key.pem etcd.pem ca.pem ca.key# 保证三节点证书一致scp /etc/etcd/ssl/* master2:/etc/etcd/ssl/scp /etc/etcd/ssl/* master3:/etc/etcd/ssl/ 证书生成完毕之后，将CA证书ca.pem, etcd秘钥etcd-key.pem, etcd证书etcd.pem拷贝到各节点的/etc/etcd/ssl目录中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# 这里就用etcd 3.3.10版本# 一：wget https://github.com/coreos/etcd/releases/download/v3.3.10/etcd-v3.3.10-linux-amd64.tar.gz# 解压缩etcd-v3.3.10-linux-amd64.tar.gz，将其中的etcd和etcdctl两个可执行文件复制到各节点的/usr/bin和/usr/local/bin目录。tar zxvf etcd-v3.3.10-linux-amd64.tar.gzcp etcd-v3.3.10-linux-amd64/etcd* /usr/local/bin/cp etcd-v3.3.10-linux-amd64/etcd* /usr/bin/mkdir /var/lib/etcd #etcd的数据目录mkdir /etc/etcd #etcd的配置文件目录# 二：yum -y install etcd# 安装好etcd之后，就可以修改etcd配置以及启动service，文件路径为：/etc/etcd/etcd.conf和/usr/lib/systemd/system/etcd.servicecat &gt; /etc/etcd/etcd.conf &lt;&lt; EOF# [member]ETCD_NAME=etcd1 # etcd名字 三节点的话为 etcd1 etcd2 etcd3ETCD_DATA_DIR="/data/etcd" # etcd数据目录指定ETCD_LISTEN_PEER_URLS="https://x.x.x.1:2380" # 修改每个master节点的ipETCD_LISTEN_CLIENT_URLS="https://x.x.x.1:2379"#[cluster]ETCD_INITIAL_ADVERTISE_PEER_URLS="https://x.x.x.1:2380"ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"ETCD_ADVERTISE_CLIENT_URLS="https://x.x.x.1:2379"EOFcat &gt; /usr/lib/systemd/system/etcd.service &lt;&lt; EOF[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.targetDocumentation=https://github.com/coreos[Service]Type=notifyWorkingDirectory=/var/lib/etcd/EnvironmentFile=-/etc/etcd/etcd.confExecStart=/usr/local/bin/etcd \ --name $&#123;ETCD_NAME&#125; \ --cert-file=/etc/etcd/ssl/etcd.pem \ --key-file=/etc/etcd/ssl/etcd-key.pem \ --peer-cert-file=/etc/etcd/ssl/etcd.pem \ --peer-key-file=/etc/etcd/ssl/etcd-key.pem \ --trusted-ca-file=/etc/etcd/ssl/ca.pem \ --peer-trusted-ca-file=/etc/etcd/ssl/ca.pem \ --initial-advertise-peer-urls $&#123;ETCD_INITIAL_ADVERTISE_PEER_URLS&#125; \ --listen-peer-urls $&#123;ETCD_LISTEN_PEER_URLS&#125; \ --listen-client-urls $&#123;ETCD_LISTEN_CLIENT_URLS&#125;,http://127.0.0.1:2379 \ --advertise-client-urls $&#123;ETCD_ADVERTISE_CLIENT_URLS&#125; \ --initial-cluster-token $&#123;ETCD_INITIAL_CLUSTER_TOKEN&#125; \ --initial-cluster etcd1=https://x.x.x.1:2380,etcd2=https://x.x.x.2:2380,etcd3=https://x.x.x.3:2380 \ --initial-cluster-state new \ --data-dir=$&#123;ETCD_DATA_DIR&#125;Restart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF# 上面在启动参数中指定了etcd的工作目录和数据目录分别是/var/lib/etcd和/data/etcd# –cert-file和–key-file分别指定etcd的公钥证书和私钥# –peer-cert-file和–peer-key-file分别指定了etcd的Peers通信的公钥证书和私钥。# –trusted-ca-file指定了客户端的CA证书# –peer-trusted-ca-file指定了Peers的CA证书# –initial-cluster-state new表示这是新初始化集群，–name指定的参数值必须在–initial-cluster中# 高可用etcd启动需要多节点同时启动才能起来服务# 分别在master节点同时启动etcdsystemctl start etcd.servicesystemctl enable etcd.service 至此，etcd高可用集群搭建完成，可用一下命令验证etcd集群 123456etcdctl \ --ca-file=/etc/etcd/ssl/ca.pem \ --cert-file=/etc/etcd/ssl/etcd.pem \ --key-file=/etc/etcd/ssl/etcd-key.pem \ --endpoints=https://x.x.x.1:2379,https://x.x.x.2:2379,https://x.x.x.3:2379 \ cluster-health 2、kubeadm部署1、master部署 部署之前请确保下载好相关image，翻墙下载或者国内dockerhub下载kubernetes镜像 1、初始化master1 创建master1的初始化配置文件,网络插件采用flannel，CIDR地址是 “10.244.0.0/16”，如下为1.13.5新版本配置文件。 123456789101112131415161718192021222324252627282930313233cat &gt; kubeadm-master.yaml &lt;&lt; EOFapiVersion: kubeadm.k8s.io/v1beta1kind: InitConfigurationlocalAPIEndpoint: advertiseAddress: x.x.x.1 # 本机ip，这里为x.x.x.1-3 bindPort: 6443---apiVersion: kubeadm.k8s.io/v1beta1kind: ClusterConfigurationkubernetesVersion: v1.13.5 # kubernetes版本 对应下载的imageimageRepository: k8s.gcr.io # 自己修改为自己的镜像库名apiServer: certSANs: - "master1" - "master2" - "master3" - "x.x.x.1" - "x.x.x.2" - "x.x.x.3" - "x.x.x.4" - "127.0.0.1"controlPlaneEndpoint: "x.x.x.4:8443" # 控制台ip指定，即vip 实现apiserver高可用etcd: external: endpoints: - https://x.x.x.1:2379 - https://x.x.x.2:2379 - https://x.x.x.3:2379networking: podSubnet: "10.244.0.0/16"EOF 123# 运行初始化命令即可，前提要把相关设置关闭，详情至准备工作# 其中在kubelet配置里加入--pod-infra-container-image 参数指定 pause私有镜像库镜像kubeadm init --config kubeadm-master.yaml 在初始化配置里，对于etcd有两种高可用的选项，一个使用内部etcd，一个使用外部etcd(独立搭建的etcd集群，而不是在初始化中搭建的)，两者初始化配置文件略有不同。 1234567891011121314151617# 使用内部etcd的话，初始化yaml文件中etcd配置如下：etcd: local: extraArgs: listen-client-urls: "https://127.0.0.1:2379,https://x.x.x.x:2379" advertise-client-urls: "https://x.x.x.x:2379" listen-peer-urls: "https://x.x.x.x:2380" initial-advertise-peer-urls: "https://x.x.x.x:2380" initial-cluster: "master1.hanli.com=https://x.x.x.x:2380"# 使用外部etcd的话，etcd: #ETCD的地址 external: endpoints: - https://x.x.x.1:2379 - https://x.x.x.2:2379 - https://x.x.x.3:2379 init初始化之后，如果成功会出现join，这时就可以运行一下命令 机器上的用户要使用kubectl来管理集群操作集群 123mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config 验证命令 1234567891011kubectl get cs # 如下信息NAME STATUS MESSAGE ERRORcontroller-manager Healthy okscheduler Healthy oketcd-1 Healthy &#123;"health": "true"&#125;etcd-0 Healthy &#123;"health": "true"&#125;etcd-2 Healthy &#123;"health": "true"&#125;kubectl get node # notReady 状态，是因为没有安装网络插件 Name成ip，可修改kubelet的启动参数即可NAME STATUS ROLES AGE VERSIONmaster1 NotReady master 1m v1.13.5 2、启动flannel服务123456789101112wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml# flannel 默认会使用主机的第一张网卡，如果你有多张网卡，需要通过配置单独指定。修改 kube-flannel.yml 中的以下部分cat kube-flannel.yml containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.11.0-amd64 # 修改下自己私有镜像库的flannel镜像名 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr - --iface=ens33 #添加 1234567891011121314kubectl apply -f kube-flannel.yml # 创建pod，因为是ds的所以后续集群里面加节点就会自动启动flannelkubectl get node # 会发现 node状态变成了 ReadyNAME STATUS ROLES AGE VERSIONmaster1 Ready master 10m v1.13.5kubectl get po --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system coredns-86c58d9df4-r59rv 1/1 Running 0 59mkube-system coredns-86c58d9df4-rbzx5 1/1 Running 0 59mkube-system kube-apiserver-master1 1/1 Running 0 58mkube-system kube-controller-manager-master1 1/1 Running 16 58mkube-system kube-flannel-ds-amd64-229j2 1/1 Running 0 42mkube-system kube-proxy-4wrg5 1/1 Running 0 59mkube-system kube-scheduler-master1 1/1 Running 13 58m 不是running状态，就说明出错了，通过查看 描述：kubectl describe pod kube-scheduler-master.hanli.com -n kube-system 日志：kubectl logs kube-scheduler-master.hanli.com -n kube-system flannel服务启动成功后，coredns也就会自动启动成功，状态为Running 3、初始化其他master节点 首先把master1上生成的ca证书等，拷贝到其他master节点上，最好免密，可使用pscp等批量任务 123456789101112131415161718192021&gt; #!/bin/bash&gt; #注意修改为自己的主机名&gt; export CONTROL_PLANE_IPS="master2 master3" &gt; &gt; # 保证节点有/etc/kubernetes/pki目录&gt; # 把以下证书复制到其他master节点&gt; for host in $&#123;CONTROL_PLANE_IPS&#125;; do&gt; # ！！！修正拷贝证书的时候请指定特定证书拷贝（之前的已经备注，新加的有etcd目录，没有用外部etcd集群），因为在证书拷贝的情况下吃过亏，因此修改此处！！！&gt; # 附issue：https://github.com/kubernetes/kubeadm/issues/1321 &gt; # 找了好久终于！！！哭&gt; #scp /etc/kubernetes/pki/*.crt $host:/etc/kubernetes/pki/&gt; #scp /etc/kubernetes/pki/*.key $host:/etc/kubernetes/pki/&gt; #scp /etc/kubernetes/pki/*.pub $host:/etc/kubernetes/pki/&gt; #scp /etc/kubernetes/admin.conf $host:/etc/kubernetes/admin.conf&gt; scp /etc/kubernetes/pki/ca.* "$&#123;USER&#125;"@$host:/etc/kubernetes/pki/&gt; scp /etc/kubernetes/pki/sa.* "$&#123;USER&#125;"@$host:/etc/kubernetes/pki/&gt; scp /etc/kubernetes/pki/front-proxy-ca.* "$&#123;USER&#125;"@$host:/etc/kubernetes/pki/&gt; scp /etc/kubernetes/pki/etcd/ca.* "$&#123;USER&#125;"@$host:/etc/kubernetes/pki/etcd/&gt; scp /etc/kubernetes/admin.conf "$&#123;USER&#125;"@$host:/etc/kubernetes/&gt; done&gt; 1234567891011121314151617&gt; tree /etc/kubernetes/pki/&gt; /etc/kubernetes/pki/&gt; ├── apiserver.crt&gt; ├── apiserver-etcd-client.crt&gt; ├── apiserver-etcd-client.key&gt; ├── apiserver.key&gt; ├── apiserver-kubelet-client.crt&gt; ├── apiserver-kubelet-client.key&gt; ├── ca.crt&gt; ├── ca.key&gt; ├── front-proxy-ca.crt&gt; ├── front-proxy-ca.key&gt; ├── front-proxy-client.crt&gt; ├── front-proxy-client.key&gt; ├── sa.key&gt; └── sa.pub&gt; In v1.8.0, kubeadm introduced the kubeadm alpha phase command with the aim of making kubeadm more modular. In v1.13.0 this command graduated to kubeadm init phase. This modularity enables you to invoke atomic sub-steps of the bootstrap process. Hence, you can let kubeadm do some parts and fill in yourself where you need customizations. kubeadm init phase is consistent with the kubeadm init workflow, and behind the scene both use the same code. 在v1.8.0中，kubeadm引入了该kubeadm alpha phase命令，目的是使kubeadm更加模块化。在v1.13.0中，此命令逐渐变为kubeadm init phase。此模块化使您可以调用引导过程的原子子步骤。因此，您可以让kubeadm执行某些操作，并在需要自定义的位置填写您自己的位置。 kubeadm init phase与kubeadm init工作流程一致，并且在场景后面都使用相同的代码。 123kubectl init --config kube-master.yaml# 分别修改对应ip，在在master2-3并执行即可 步骤如master1# 等待kube-proxy flannel启动成功即可 1234567891011121314151617181920kubectl get po -n kube-systemNAME READY STATUS RESTARTS AGEcoredns-9f9d9c76-4zrt4 1/1 Running 0 2dcoredns-9f9d9c76-dqd4c 1/1 Running 0 2dkube-apiserver-zjjh-rq-k8s-1 1/1 Running 0 2dkube-apiserver-zjjh-rq-k8s-2 1/1 Running 0 2dkube-apiserver-zjjh-rq-k8s-3 1/1 Running 0 2dkube-controller-manager-zjjh-rq-k8s-1 1/1 Running 0 2dkube-controller-manager-zjjh-rq-k8s-2 1/1 Running 0 2dkube-controller-manager-zjjh-rq-k8s-3 1/1 Running 0 2dkube-flannel-ds-amd64-7ghn7 1/1 Running 1 2dkube-flannel-ds-amd64-9cqts 1/1 Running 0 2dkube-flannel-ds-amd64-f57nh 1/1 Running 1 2dkube-proxy-8fwts 1/1 Running 0 2dkube-proxy-95tjb 1/1 Running 0 2dkube-proxy-bls94 1/1 Running 0 2dkube-scheduler-zjjh-rq-k8s-1 1/1 Running 0 2dkube-scheduler-zjjh-rq-k8s-2 1/1 Running 0 2dkube-scheduler-zjjh-rq-k8s-3 1/1 Running 0 2dkubernetes-dashboard-67d49f7868-x79wf 1/1 Running 0 76m 2、Work节点加入集群 输入master节点初始化成功之后出现的join命令，出现kubectl get nodes即成功 12&gt; kubeadm join x.x.x.4:8443 --token bnnsb7.amapp1t78llxn54d --discovery-token-ca-cert-hash sha256:520ef89be84c30e480db6d441a7e4179634a9455f0009e249ebe8f35fa792087&gt; 3、集群验证1234567891011121314151617181920# 节点状态[root@master] ~$ kubectl get nodes# 组件状态[root@master] ~$ kubectl get cs# 服务账户[root@master] ~$ kubectl get serviceaccount# 集群信息[root@master] ~$ kubectl cluster-info# 验证dns功能[root@master] ~$ kubectl run curl --image=radial/busyboxplus:curl -it[ root@curl-66959f6557-r4crd:/ ]$ nslookup kubernetes.defaultServer: 10.96.0.10Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.localName: kubernetes.defaultAddress 1: 10.96.0.1 kubernetes.default.svc.cluster.local 附录： 12&gt;# kubelet.service配置文件 /use/lib/systemd/system/kubelet.service.d/10-kubeadm.conf&gt; 参考链接： kubeadm安装1.13.5：https://blog.csdn.net/fanren224/article/details/86573264#2master1_165 二进制安装：https://github.com/mritd/ktool 二进制安装kubernetes_v1.13.4：https://mritd.me/2019/03/16/set-up-kubernetes-1.13.4-cluster/]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
        <tag>Kubernetes_ha</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[记一点Haproxy参数的文档]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2F%E8%AE%B0%E4%B8%80%E7%82%B9Haproxy%E5%8F%82%E6%95%B0%E7%9A%84%E6%96%87%E6%A1%A3%2F</url>
    <content type="text"><![CDATA[Haproxy的一些配置参数，/etc/haproxy/haproxy.cfg 主要是配置项的超时参数，如下： 1234567891011121314151617181920&gt; # 反映连接haproxy集群报错，查看应用服务器进程大量的端口连接处于colse_wait状态。&gt; # 这个haproxy的redis集群类似配置已经好几组了，配置都相同问啥别的都好好的这个却有问题。&gt; # 自己尝试使用redis-cli连接（haproxy做的redis分片集群），确实能够正常连接操作，但是连接状态马上就会变成close_wait状态，重新执行命令这个close_wait状态会恢复ESTABLISHED，但是细心看端口号已经变化，也就是redis-cli发生了重新连接。于是问题清晰了：用户客户端工具建立链接后，没有自动重连来保持连接，导致超时，服务器主动断开连接，客户端再次链接的时候socket已经损坏不可用了，导致报错&gt; # 解决办法：调整haproxy的超时参数：&gt; timeout connect 5s&gt; timeout queue 5s&gt; timeout client 30s&gt; timeout server 30s&gt; timeout client-fin 30s&gt; timeout server-fin 30s&gt; timeout tunnel 1h&gt; # 参数解释：&gt; timeout connect 连接尝试成功连接到server的超时时间&gt; timeout queue 在队列等待连接槽释放的超时时间&gt; timeout server server端非活动状态超时时间&gt; timeout client 客户端非活动状态超时时间&gt; timeout server-fin 半关闭状态连接，server端非活动超时时间&gt; timeout client-fin 半关闭状态连接，client端非活动超时时间&gt; timeout tunnel 客户端和服务器端通道非活动超时时间&gt;]]></content>
      <tags>
        <tag>Haproxy</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python_list去重]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2FPython-list%E5%8E%BB%E9%87%8D%2F</url>
    <content type="text"><![CDATA[Python_list去重集中记录一下方法怎么快速的对列表进行去重呢，去重之后原来的顺序会不会改变呢？1.以下的几种情况结果是一样的，去重之后顺序会改变:1234567# 最常用ids = [1,2,3,3,4,2,3,4,5,6,1]news_ids = []for id in ids: if id not in news_ids: news_ids.append(id)print news_ids 或用set123# 使用set如果列表里面有字典活着其他数据结构，貌似不会去重。ids = [1,4,3,3,4,2,3,4,5,6,1]ids = list(set(ids)) 或使用itertools.grouby123456import itertoolsids = [1,4,3,3,4,2,3,4,5,6,1]ids.sort()it = itertools.groupby(ids)for k, g in it: print k 关于itertools.groupby的原理可以看这里： (1) http://docs.python.org/2/library/itertools.html#itertools.groupby (2)https://www.liaoxuefeng.com/wiki/001374738125095c955c1e6d8bb493182103fac9270762a000/001415616001996f6b32d80b6454caca3d33c965a07611f000 2.怎么能不改变原来的顺序呢？(要用到reduce 关于reduce的介绍 http://docs.python.org/2/library/functions.html#reduce) 关于lambda的文章:http://www.cnblogs.com/nyist-xsk/p/7404675.html 关于reduce的文章: (1) http://www.cnblogs.com/XXCXY/p/5180245.html (2) http://www.pythoner.com/46.html 1234In [5]: ids = [1,4,3,3,4,2,3,4,5,6,1]In [6]: func = lambda x,y:x if y in x else x + [y]In [7]: reduce(func, [[], ] + ids)Out[7]: [1, 4, 3, 2, 5, 6] 其中的 lambda x,y:x if y in x else x + [y] 等价于 lambda x,y: y in x and x or x+[y] 。思路其实就是先把ids变为[[], 1,4,3,……] ,然后在利用reduce的特性. 去列表去重，不改变原来的顺序，还可以使用一个空列表把原列表里面不重复的数据”装起来”，例如：123456789101112131415161718list2 = []list1 = [1,2,3,2,2,2,4,6,5]for i in list1: if i not in list2: list2.append(i)list2[1, 2, 3, 4, 6, 5]或者使用删除元素索引的方法对列表去重，并且不改变原列表的顺序# python for删除的时候会往前移(垃圾回收机制)，未遍历到的后一个占了前一个被删除的"位置"，导致这个数不会被遍历到，而使最后的结果错误# 局部变量在栈内存中存在,当for循环语句结束,那么变量会及时被gc(垃圾回收器)及时的释放掉,不浪费空间；# 如果使用循环之后还想去访问循环语句中控制那个变量,使用while循环。# 所以使用while循环删除nums中的Val(的下标)nums = [1,2,3,3,4,2,3,4,5,6,1]val = 3while val in nums: nums.pop(nums.index(val))print numsreturn len(nums)]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>Python</tag>
        <tag>Python_碎片化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang_问题]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2FGolang-%E9%97%AE%E9%A2%98%2F</url>
    <content type="text"><![CDATA[（持续更新） 一、gorm遇到invalid connection报错 (/data/dcops_workspace/src/x.x.x.x/xxxx/model/containerappid_info.go:76)[2019-04-09 14:41:40] invalid connection 现象：今天使用gorm时，总遇到invalid connection报错，导致过一段时间调一下接口服务容器就会重启 网上查了文档看了gorm的mysql连接池的参数： dcosdb.DB().SetMaxIdleConns(10) // SetMaxIdleConns设置idle connection pool的最大连接数。如果MaxOpenConns的值 &gt; 0，但是小于这里设置的MaxIdleConns，则MaxIdleConns将自动降到与MaxOpenConns的限制相同。如果 &lt;= 0, 则没有空闲连接会被保留。最大空闲连接数 dcosdb.DB().SetMaxOpenConns(100) //SetMaxOpenConns用于设置Database最大可以打开的连接数。如果 &lt;= 0, 则没有连接限制。且默认值为0（无限制）。数据库最大连接数 dcosdb.DB().SetConnMaxLifetime(time.Second 5) // SetConnMaxLifetime用于设置连接可被重新使用的最大时间间隔。如果超时，则连接会在重新使用前被关闭。如果 d &lt;= 0, 则连接将被永久保留。**连接最长存活期，超过这个时间连接将不再被复用*** 原因是：程序在重复使用数据库tcp连接池中的某个连接时，该命中连接可能已被服务器过期丢弃，而客户端这边认为该连接为过期，还有效。此时会报错 invalid connection。随后将该连接重连接池中丢弃 原因参考：https://blog.csdn.net/dghpgyss/article/details/86480837 1234// 发现我把dcosdb.DB().SetConnMaxLifetime(time.Second * 5)这个参数设置成了dcosdb.DB().SetConnMaxLifetime(time.Hour)导致了每次连接丢弃，但是客户端不知道。服务端主动关闭了连接，因为服务端对连接的保持超时时间到了也关闭的，所以可以设置客户端连接超时时间小于服务端DB.SetConnMaxLifetime(time.Second)// SetConnMaxLifetime sets the maximum amount of time a connection may be reused.dcosdb.DB().SetConnMaxLifetime(time.Second * 5)// dcosdb.DB().SetConnMaxLifetime(time.Hour) 设置成了1小时 二、]]></content>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang并发之原子操作]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2FGolang%E5%B9%B6%E5%8F%91%E4%B9%8B%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[转自：http://ifeve.com/go-concurrency-atomic/ 原理多，代码量偏少。 我们已经知道，原子操作即是进行过程中不能被中断的操作。也就是说，针对某个值的原子操作在被进行的过程当中，CPU绝不会再去进行其它的针对该值的操作。无论这些其它的操作是否为原子操作都会是这样。为了实现这样的严谨性，原子操作仅会由一个独立的CPU指令代表和完成。只有这样才能够在并发环境下保证原子操作的绝对安全。Go语言提供的原子操作都是非侵入式的。它们由标准库代码包sync/atomic中的众多函数代表。我们可以通过调用这些函数对几种简单的类型的值进行原子操作。这些类型包括int32、int64、uint32、uint64、uintptr和unsafe.Pointer类型，共6个。这些函数提供的原子操作共有5种，即：增或减、比较并交换、载入、存储和交换。它们分别提供了不同的功能，且适用的场景也有所区别。下面，我们就根据这些种类对Go语言提供的原子操作进行逐一的讲解。 1、增或减被用于进行增或减的原子操作（以下简称原子增/减操作）的函数名称都以“Add”为前缀，并后跟针对的具体类型的名称。例如，实现针对uint32类型的原子增/减操作的函数的名称为AddUint32。事实上，sync/atomic包中的所有函数的命名都遵循此规则。顾名思义，原子增/减操作即可实现对被操作值的增大或减小。因此，被操作值的类型只能是数值类型。更具体的讲，它只能是我们在前面提到的int32、int64、uint32、uint64和uintptr类型。例如，我们如果想原子的把一个int32类型的变量i32的值增大3的话，可以这样做： 1newi32 := atomic.AddInt32(&amp;i32, 3) 我们将指向i32变量的值的指针值和代表增减的差值3作为参数传递给了atomic.AddInt32函数。之所以要求第一个参数值必须是一个指针类型的值，是因为该函数需要获得到被操作值在内存中的存放位置，以便施加特殊的CPU指令。从另一个角度看，对于一个不能被取址的数值，我们是无法进行原子操作的。此外，这类函数的第二个参数的类型被操作值的类型总是相同的。因此，在前面那个调用表达式被求值的时候，字面量3会被自动转换为一个int32类型的值。函数atomic.AddInt32在被执行结束之时会返回经过原子操作后的新值。不过不要误会，我们无需把这个新值再赋给原先的变量i32。因为它的值已经在atomic.AddInt32函数返回之前被原子的修改了。与该函数类似的还有atomic.AddInt64函数、atomic.AddUint32函数、atomic.AddUint64函数和atomic.AddUintptr函数。这些函数也可以被用来原子的增/减对应类型的值。例如，如果我们要原子的将int64类型的变量i64的值减小3话，可以这样编写代码： 12var i64 int64atomic.AddInt64(&amp;i64, -3) 不过，由于atomic.AddUint32函数和atomic.AddUint64函数的第二个参数的类型分别是uint32和uint64，所以我们无法通过传递一个负的数值来减小被操作值。那么，这是不是就意味着我们无法原子的减小uint32或uint64类型的值了呢？幸好，不是这样。Go语言为我们提供了一个可以迂回的达到此目的办法。如果我们想原子的把uint32类型的变量ui32的值增加NN（NN代表了一个负整数），那么我们可以这样调用atomic.AddUint32函数： 1atomic.AddUint32(&amp;ui32, ^uint32(-NN-1)) 对于uint64类型的值来说也是这样。调用表达式 1atomic.AddUint64(&amp;ui64, ^uint64(-NN-1)) 表示原子的把uint64类型的变量ui64的值增加NN（或者说减小-NN）。之所以这种方式可以奏效，是因为它利用了二进制补码的特性。我们知道，一个负整数的补码可以通过对它按位（除了符号位之外）求反码并加一得到。我们还知道，一个负整数可以由对它的绝对值减一并求补码后得到的数值的二进制表示来代表。例如，如果NN是一个int类型的变量且其值为-35，那么表达式 1uint32(int32(NN)) 和 1^uint32(-NN-1) 的结果值就都会是11111111111111111111111111011101。由此，我们使用^uint32(-NN-1)和^uint64(-NN-1)来分别表示uint32类型和uint64类型的NN就顺理成章了。这样，我们就可以合理的绕过uint32类型和uint64类型对值的限制了。以上是官方提供一种通用解决方案。除此之外，我们还有两个非通用的方案可供选择。首先，需要明确的是，对于一个代表负数的字面常量来说，它们是无法通过简单的类型转换将其转换为uint32类型或uint64类型的值的。例如，表达式uint32(-35)和uint64(-35)都是不合法的。它们都不能通过编译。但是，如果我们事先把这个字面量赋给一个变量然后再对这个变量进行类型转换，那么就可以得到Go语言编译器的认可。我们依然以值为-35的变量NN为例，下面这条语句可以通过编译并被正常执行： 1fmt.Printf("The variable: %b.\n", uint32(NN)) 其输出内容为： 1The variable: 11111111111111111111111111011101. 可以看到，表达式uint32(NN)的结果值的二进制表示与前面的uint32(int32(NN))表达式以及^uint32(-NN-1)表达式的结果值是一致的。它们都可以被用来表示uint32类型的-35。因此，我们也可以使用下面的调用表达式来原子的把变量ui32的值减小-NN：atomic.AddUint32(&amp;ui32, uint32(NN))不过，这样的编写方式仅在NN是数值类型的变量的时候才可以通过编译。如果NN是一个常量，那么也会使表达式uint32(NN)不合法并无法通过编译。它与表达式uint32(-35)造成的编译错误是一致的。在这种情况下，我们可以这样来达到上述目的： 1atomic.AddUint32(&amp;ui32, NN&amp;math.MaxUint32) 其中，我们用到了标准库代码包math中的常量MaxUint32。math.MaxUint32常量表示的是一个32位的、所有二进制位上均为1的数值。我们把NN和math.MaxUint32进行按位与操作的意义是使前者的值能够被视为一个uint32类型的数值。实际上，对于表达式NN&amp;math.MaxUint32来说，其结果值的二进制表示与前面uint32(int32(NN))表达式以及^uint32(-NN-1)表达式的结果值也是一致的。我们在这里介绍的这两种非官方的解决方案是不能混用的。更具体地说，如果NN是一个常量，那么表达式uint32(NN)是无法通过编译的。而如果NN是一个变量，那么表达式NN&amp;math.MaxUint32就无法通过编译。前者的错误在于代表负整数的字面常量不能被转换为uint32类型的值。后者的错误在于这个按位与运算的结果值的类型不是uint32类型而是int类型，从而导致数据溢出的错误。相比之下，官方给出的那个解决方案的适用范围更广。有些读者可能会有这样的疑问：为什么如此曲折的实现这一功能？直接声明出atomic.SubUint32()函数和atomic.SubUint64()函数不好吗？作者理解，不这样做是为了让这些原子操作的API可以整齐划一，并且避免在扩充它们的时候使sync/atomic包中声明的程序实体成倍增加。（作者向Go语言官方提出了这个问题并引发了一些讨论，他们也许会使用投票的方式来选取更好一些的方案）注意，并不存在名为atomic.AddPointer的函数，因为unsafe.Pointer类型值之间既不能被相加也不能被相减。2、比较并交换有些读者可能很熟悉比较并交换操作的英文称谓——Compare And Swap，简称CAS。在sync/atomic包中，这类原子操作由名称以“CompareAndSwap”为前缀的若干个函数代表。我们依然以针对int32类型值的函数为例。该函数名为CompareAndSwapInt32。其声明如下： 1func CompareAndSwapInt32(addr *int32, old, new int32) (swapped bool) 可以看到，CompareAndSwapInt32函数接受三个参数。第一个参数的值应该是指向被操作值的指针值。该值的类型即为*int32。后两个参数的类型都是int32类型。它们的值应该分别代表被操作值的旧值和新值。CompareAndSwapInt32函数在被调用之后会先判断参数addr指向的被操作值与参数old的值是否相等。仅当此判断得到肯定的结果之后，该函数才会用参数new代表的新值替换掉原先的旧值。否则，后面的替换操作就会被忽略。这正是“比较并交换”这个短语的由来。CompareAndSwapInt32函数的结果swapped被用来表示是否进行了值的替换操作。与我们前面讲到的锁相比，CAS操作有明显的不同。它总是假设被操作值未曾被改变（即与旧值相等），并一旦确认这个假设的真实性就立即进行值替换。而使用锁则是更加谨慎的做法。我们总是先假设会有并发的操作要修改被操作值，并使用锁将相关操作放入临界区中加以保护。我们可以说，使用锁的做法趋于悲观，而CAS操作的做法则更加乐观。CAS操作的优势是，可以在不形成临界区和创建互斥量的情况下完成并发安全的值替换操作。这可以大大的减少同步对程序性能的损耗。当然，CAS操作也有劣势。在被操作值被频繁变更的情况下，CAS操作并不那么容易成功。有些时候，我们可能不得不利用for循环以进行多次尝试。示例如下： 123456789var value int32func addValue(delta int32) &#123; for &#123; v := value if atomic.CompareAndSwapInt32(&amp;value, v, (v + delta)) &#123; break &#125; &#125;&#125; 可以看到，为了保证CAS操作的成功完成，我们仅在CompareAndSwapInt32函数的结果值为true时才会退出循环。这种做法与自旋锁的自旋行为相似。addValue函数会不断的尝试原子的更新value的值，直到这一操作成功为止。操作失败的缘由总会是value的旧值已不与v的值相等了。如果value的值会被并发的修改的话，那么发生这种情况是很正常的。CAS操作虽然不会让某个Goroutine阻塞在某条语句上，但是仍可能会使流程的执行暂时停滞。不过，这种停滞的时间大都极其短暂。请记住，当想并发安全的更新一些类型（更具体的讲是，前文所述的那6个类型）的值的时候，我们总是应该优先选择CAS操作。与此对应，被用来进行原子的CAS操作的函数共有6个。除了我们已经讲过的CompareAndSwapInt32函数之外，还有CompareAndSwapInt64、CompareAndSwapPointer、CompareAndSwapUint32、CompareAndSwapUint64 和CompareAndSwapUintptr函数。这些函数的结果声明列表与CompareAndSwapInt32函数的完全一致。而它们的参数声明列表与后者也非常类似。虽然其中的那三个参数的类型不同，但其遵循的规则是一致的，即：第二个和第三个参数的类型均为与第一个参数的类型（即某个指针类型）紧密相关的那个类型。例如，如果第一个参数的类型为*unsafe.Pointer，那么后两个参数的类型就一定是unsafe.Pointer。这也是由这三个参数的含义决定的。3、载入在前面示例的for循环中，我们使用语句v := value为变量v赋值。但是，要注意，其中的读取value的值的操作并不是并发安全的。在该读取操作被进行的过程中，其它的对此值的读写操作是可以被同时进行的。它们并不会受到任何限制。在第7章的第1节的最后，我们举过这样一个例子：在32位计算架构的计算机上写入一个64位的整数。如果在这个写操作未完成的时候有一个读操作被并发的进行了，那么这个读操作很可能会读取到一个只被修改了一半的数据。这种结果是相当糟糕的。为了原子的读取某个值，sync/atomic代码包同样为我们提供了一系列的函数。这些函数的名称都以“Load”为前缀，意为载入。我们依然以针对int32类型值的那个函数为例。我们下面利用LoadInt32函数对上一个示例稍作修改： 12345678func addValue(delta int32) &#123; for &#123; v := atomic.LoadInt32(&amp;value) if atomic.CompareAndSwapInt32(&amp;value, v, (v + delta)) &#123; break &#125; &#125;&#125; 函数atomic.LoadInt32接受一个int32类型的指针值，并会返回该指针值指向的那个值。在该示例中，我们使用调用表达式atomic.LoadInt32(&amp;value)替换掉了标识符value。替换后，那条赋值语句的含义就变为：原子的读取变量value的值并把它赋给变量v。有了“原子的”这个形容词就意味着，在这里读取value的值的同时，当前计算机中的任何CPU都不会进行其它的针对此值的读或写操作。这样的约束是受到底层硬件的支持的。注意，虽然我们在这里使用atomic.LoadInt32函数原子的载入value的值，但是其后面的CAS操作仍然是有必要的。因为，那条赋值语句和if语句并不会被原子的执行。在它们被执行期间，CPU仍然可能进行其它的针对value的值的读或写操作。也就是说，value的值仍然有可能被并发的改变。与atomic.LoadInt32函数的功能类似的函数有atomic.LoadInt64、atomic.LoadPointer、atomic.LoadUint32、atomic.LoadUint64和atomic.LoadUintptr。4、存储与读取操作相对应的是写入操作。而sync/atomic包也提供了与原子的值载入函数相对应的原子的值存储函数。这些函数的名称均以“Store”为前缀。在原子的存储某个值的过程中，任何CPU都不会进行针对同一个值的读或写操作。如果我们把所有针对此值的写操作都改为原子操作，那么就不会出现针对此值的读操作因被并发的进行而读到修改了一半的值的情况了。原子的值存储操作总会成功，因为它并不会关心被操作值的旧值是什么。显然，这与前面讲到的CAS操作是有着明显的区别的。因此，我们并不能把前面展示的addValue函数中的调用atomic.CompareAndSwapInt32函数的表达式替换为对atomic.StoreInt32函数的调用表达式。函数atomic.StoreInt32会接受两个参数。第一个参数的类型是int 32类型的，其含义同样是指向被操作值的指针。而第二个参数则是int32类型的，它的值应该代表欲存储的新值。其它的同类函数也会有类似的参数声明列表。5、交换在sync/atomic代码包中还存在着一类函数。它们的功能与前文所讲的CAS操作和原子载入操作都有些类似。这样的功能可以被称为原子交换操作。这类函数的名称都以“Swap”为前缀。与CAS操作不同，原子交换操作不会关心被操作值的旧值。它会直接设置新值。但它又比原子载入操作多做了一步。作为交换，它会返回被操作值的旧值。此类操作比CAS操作的约束更少，同时又比原子载入操作的功能更强。以atomic.SwapInt32函数为例。它接受两个参数。第一个参数是代表了被操作值的内存地址的int32类型值，而第二个参数则被用来表示新值。注意，该函数是有结果值的。该值即是被新值替换掉的旧值。atomic.SwapInt32函数被调用后，会把第二个参数值置于第一个参数值所表示的内存地址上（即修改被操作值），并将之前在该地址上的那个值作为结果返回。其它的同类函数的声明和作用都与此类似。至此，我们快速且简要地介绍了sync/atomic代码包中的所有函数的功能和用法。这些函数都被用来对特定类型的值进行原子性的操作。如果我们想以并发安全的方式操作单一的特定类型（int32、int64、uint32、uint64、uintptr或unsafe.Pointer）的值的话，应该首先考虑使用这些函数来实现。请注意，原子的减小一些特定类型（确切地说，是uint32类型和uint64类型）的值的实现方式并不那么直观。在Go语言官方对此进行改进之前，我们应该按照他们为我们提供的那种方式来进行此类操作。6、应用于实际下面，我们就使用刚刚介绍的知识再次对在前面示例中创建的myDataFile类型进行改造。在myDataFile类型的第二个版本中，我们仍然使用两个互斥锁来对与roffset字段和woffset字段相关的操作进行保护。myDataFile类型的方法中的绝大多数都包含了这些操作。首先，我们来看对roffset字段的操作。在*myDataFile类型的Read方法中有这样一段代码： 1234567// 读取并更新读偏移量// 原子操作偏移量var offset int64df.rmutex.Lock()offset = df.roffsetdf.roffset += int64(df.dataLen)df.rmutex.Unlock() 这段代码的含义是读取读偏移量的值并把它存入到局部变量中，然后增加读偏移量的值以使其它的并发的读操作能够被正确、有效的进行。为了使程序能够在并发环境下有序的对roffset字段进行操作，我们为这段代码应用了互斥锁rmutex。字段roffset和变量offset都是int64类型的。后者代表了前者的旧值。而字段roffset的新值即为其旧值与dataLen字段的值的和。实际上，这正是原子的CAS操作的适用场景。我们现在用CAS操作来实现该段代码的功能： 123456789// 读取并更新读偏移量 var offset int64 for &#123; offset = df.roffset if atomic.CompareAndSwapInt64(&amp;df.roffset, offset, (offset + int64(df.dataLen))) &#123; break &#125; &#125; 根据roffset和offset的类型，我们选用atomic.CompareAndSwapInt64来进行CAS操作。我们在调用该函数的时候传入了三个参数，分别代表了被操作值的地址、旧值和新值。如果该函数的结果值是true，那么我们就退出for循环。这时，变量offset即是我们需要的读偏移量的值。另一方面，如果该函数的结果值是false，那么就说明在从完成读取到开始更新roffset字段的值的期间内有其它的并发操作对该值进行了更改。当遇到这种情况，我们就需要再次尝试。只要尝试失败，我们就会重新读取roffset字段的值并试图对该值进行CAS操作，直到成功为止。具体的尝试次数与具体的并发环境有关。我们在前面说过，在32位计算架构的计算机上写入一个64位的整数也会存在并发安全方面的隐患。因此，我们还应该将这段代码中的offset = df.roffset语句修改为offset = atomic.LoadInt64(&amp;df.roffset)。除了这里，在*myDataFile类型的Rsn方法中也有针对roffset字段的读操作： 123df.rmutex.Lock()defer df.rmutex.Unlock()return df.roffset / int64(df.dataLen) 我们现在去掉施加在上面的锁定和解锁操作，转而使用原子操作来实现它。修改后的代码如下： 12offset := atomic.LoadInt64(&amp;df.roffset)return offset / int64(df.dataLen) 这样，我们就在依然保证相关操作的并发安全的前提下去除了对互斥锁rmutex的使用。对于字段woffset和互斥锁wmutex，我们也应该如法炮制。读者可以试着按照上面的方法修改与之相关的Write方法和Wsn方法。在修改完成之后，我们就可以把代表互斥锁的rmutex字段和wmutex字段从myDataFile类型的基本结构中去掉了。这样，该类型的基本结构会显得精简了不少。通过本次改造，我们减少了myDataFile类型及其方法对互斥锁的使用。这对该程度的性能和可伸缩性都会有一定的提升。其主要原因是，原子操作由底层硬件支持，而锁则由操作系统提供的API实现。若实现相同的功能，前者通常会更有效率。读者可以为前面展示的这三个版本的*myDataFile类型的实现编写性能测试，以验证上述观点的正确性。总之，我们要善用原子操作。因为它比锁更加简练和高效。不过，由于原子操作自身的限制，锁依然常用且重要。]]></content>
      <categories>
        <category>Golang并发</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang并发之sync.WaitGroup]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2FGolang%E5%B9%B6%E5%8F%91%E4%B9%8Bsync-WaitGroup%2F</url>
    <content type="text"><![CDATA[声明1var wg sync.WaitGroup sync.WaitGroup/ Golang内置sync包 Wg变量：该类型有三个指针方法，即Add、Done和Wait。 类型sync.WaitGroup是一个结构体类型。当一个sync.WaitGroup类型的变量被声明之后，其值中的那个计数值将会是0。 我们可以通过该值的Add方法增大或减少其中的计数值。 如下： wg.Add(1) 与wg.Add(-1)的执行效果是一致的： wg.Done() 总结: 1、使用Done方法禁忌与Add方法的一样——不要让相应的计数值变为负数。 例如，这段代码中的第5条语句会引发一个运行时恐慌： var wg sync.WaitGroup wg.Add(2) wg.Done() wg.Done() wg.Done() 2、当我们调用sync.WaitGroup类型值的Wait方法的时候，它会去检查该值中的计数值。如果这个计数值为0，那么该方法会立即返回，且不会对程序的运行产生任何影响。 但是，如果这个计数值大于0，那么该方法的调用方所属的那个Goroutine就会被阻塞。直到该计数值重新变为0之时，为此而被阻塞的所有Goroutine才会被唤醒。 代码案例1234567891011121314151617181920212223242526272829303132333435// 协调多个Goroutine的运行。假设，在我们的程序中启用了4个Goroutine，分别是G1、G2、G3和G4。其中，G2、G3和G4是由G1中的代码启用并被用于执行某些特定任务的。G1在启用这3个Goroutine之后要等待这些特定任务的完成。// 方案1（channel通道）sign := make(chan byte, 3)go func() &#123; //G2 sign &lt;- 2&#125;()go func() &#123; //G3 sign &lt;- 3&#125;()go func() &#123; //G4 sign &lt;- 4&#125;()for i := 0; i &lt; 3; i++ &#123;fmt.Printf("G%d is ended.\n", &lt;-sign)&#125;// 方案2（sync.WaitGroup）var wg sync.WaitGroupwg.Add(3)go func() &#123; //G2 wg.Done()&#125;()go func() &#123; //G3 wg.Done()&#125;()go func() &#123; //G4 wg.Done()&#125;()wg.Wait()fmt.Println("Finish") 12345678910111213141516171819202122package mainimport ( "fmt" "sync" "time")func main() &#123; var wg sync.WaitGroup for i := 0; i &lt; 5; i = i + 1 &#123; wg.Add(1) go func(n int) &#123; // defer wg.Done() defer wg.Add(-1) EchoNumber(n) &#125;(i) // n = i &#125; wg.Wait()&#125;func EchoNumber(i int) &#123; time.Sleep(3e9) fmt.Println(i)&#125;]]></content>
      <categories>
        <category>Golang并发</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo入坑-主题篇]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2FHexo%E5%85%A5%E5%9D%91-%E4%B8%BB%E9%A2%98%E7%AF%87%2F</url>
    <content type="text"><![CDATA[]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo入坑-安装篇]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2FHexo%E5%85%A5%E5%9D%91-%E5%AE%89%E8%A3%85%E7%AF%87%2F</url>
    <content type="text"><![CDATA[准备 有一个github账号，没有的话去注册一个 安装了git、node.js、npm，并了解相关基础知识 12node -vgit --version 安装了git for Mac（或者其它git客户端） 安装1. 创建github仓库​ github上新建一个名为用户名.github.io的项目，例如：sheldon-lu.github.io， 这个项目名是用来做域名用的，当然也可以去申请一个属于自己的域名，一般推荐godaddy 2. 配置SSH key免密上传不详细讲了，命令如下： 1234567891011cd ~/.ssh# 检查本机已存在的ssh密钥# 如果提示：No such file or directory 说明你是第一次使用git。ssh-keygen -t rsa -C "邮件地址"# 然后连续3次回车，最终会生成一个文件在用户目录下，打开用户目录，找到.ssh\id_rsa.pub文件，记事本打开并复制里面的内容，打开你的github主页，进入个人设置 -&gt; SSH and GPG keys -&gt; New SSH key# 测试是否成功，验证命令：ssh -T git@GitHub.com## Hi cnfeat! You've successfully authenticated, but GitHub does not provide shell access.出现这个就是成功了，之后可以测试一下push是否免密上传。# 此时你可能还需要配置，这个属于全局config，对于git项目完全可以git clone下来之后进行push等等操作：git config --global user.name "liuxianan"// 你的github用户名，非昵称git config --global user.email "xxx@qq.com"// 填写你的github注册邮箱 3. Hexo安装及介绍官网： http://hexo.io github: https://github.com/hexojs/hexo 由于github pages存放的都是静态文件，博客存放的不只是文章内容，还有文章列表、分类、标签、翻页等动态内容，假如每次写完一篇文章都要手动更新博文目录和相关链接信息，相信谁都会疯掉，所以hexo所做的就是将这些md文件都放在本地，每次写完文章后调用写好的命令来批量完成相关页面的生成，然后再将有改动的页面提交到github。 安装之前先来说几个注意事项： 很多命令既可以用Windows的cmd来完成，也可以使用git bash来完成，但是部分命令会有一些问题，为避免不必要的问题，建议全部使用git bash来执行（对于windows用户而言）； hexo不同版本差别比较大，网上很多文章的配置信息都是基于2.x的，所以注意不要被误导； hexo有2种_config.yml文件，一个是根目录下的全局的_config.yml，一个是各个theme下的； 这里给官网上的部署命令： 1234567891011mkdir &lt;Your blog&gt; # 创建blog文档文件夹npm install hexo-cli -g # 安装hexo以及hexo命令# 初始化一个blog项目，其中'blog'为你的blog名字，hexo会自动下载一些文件到这个目录，包括node_modules.hexo init blogcd blognpm installhexo server## 这两个命令是经常会用到的hexo s -g #生成并本地预览hexo d -g #生成并上传 对于mac用户，如果遇到以下报错： 123456789101112131415161718安装npm install hexo-cli -g时报错npm install hexo -gnpm WARN locking Error: EACCES, open &apos;/Users/lushenneng/.npm/_locks/hexo-4ded2cf5ea4a8daa.lock&apos;npm WARN locking at Error (native)npm WARN locking /Users/lushenneng/.npm/_locks/hexo-4ded2cf5ea4a8daa.lock failed &#123; [Error: EACCES, open &apos;/Users/lushenneng/.npm/_locks/hexo-4ded2cf5ea4a8daa.lock&apos;]npm WARN locking errno: -13,npm WARN locking code: &apos;EACCES&apos;,npm WARN locking path: &apos;/Users/lushenneng/.npm/_locks/hexo-4ded2cf5ea4a8daa.lock&apos; &#125;npm ERR! Darwin 14.0.0npm ERR! argv &quot;node&quot; &quot;/usr/bin/npm&quot; &quot;install&quot; &quot;hexo&quot; &quot;-g&quot;npm ERR! node v0.12.3npm ERR! npm v2.9.1npm ERR! Attempt to unlock /usr/lib/node_modules/hexo, which hasn&apos;t been lockednpm ERR!npm ERR! If you need help, you may report this error at:npm ERR! https://github.com/npm/npm/issuesnpm ERR! Please include the following file with any support request:npm ERR! /Users/lushenneng/blog/npm-debug.log 解决： 12345# 可以先用这个命令：sudo npm install hexo-cli -g# 如果还是报错可以用这个命令：sudo npm install --unsafe-perm --verbose -g hexo# 加上sudo一般能解决很多问题 4. START Hexo在blog目录下的_config.yml配置修改如下：(大概是在最后一行) 12345deploy: type: git repository: github: git@github.com:sheldon-lu/sheldon_blog.git branch: master 我这里不指定用的github.io这个域名是因为我用sheldon-lu.github.io作主域名，sheldon_blog作文根，当然这样的话这边hexo内设置如下：(大概在14行左右开始) 1234567# URL## If your site is put in a subdirectory, set url as &apos;http://yoursite.com/child&apos; and root as &apos;/child/&apos;url: https://sheldon-lu.github.io/root: /sheldon_blog# permalink: :year/:month/:day/:title/permalink: passages/:title/permalink_defaults: 至此，就可以使用了，浏览器输入域名：https://sheldon-lu.github.io/sheldon_blog即可 1、这里要说明一下，使用 hexo d 上传至github时如果需要用到 gh-pages 的话，记得在自己github的blog项目下创建个gh-pages的分支，即可; 2、_config.yml中配置repository时一定配置的要是ssh，别弄的什么https://github之类的 参考地址：https://ngwind.github.io/2018/07/27/%E4%BD%BF%E7%94%A8gh-pages+Hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E9%A1%B5%E6%95%99%E7%A8%8B/]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
</search>
