<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Kubernetes_填坑]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2FKubernetes-%E5%A1%AB%E5%9D%91%2F</url>
    <content type="text"><![CDATA[更新19.4.5一、etcd 单点问题默认kubeadm创建的集群会在内部启动一个单点的 etcd，当然大部分情况下 etcd 还是很稳定的，但是一但 etcd 由于某种原因挂掉，这个问题会非常严重，会导致整个集群不可用。具体原因是 etcd 存储着 kubernetes 各种元数据信息；包括 kubectl get pod等等基础命令实际上全部是调用 RESTful API 从 etcd 中获取的信息；所以一但 etcd 挂掉以后，基本等同于kubectl命令不可用，集群各节点也会因无法从 etcd 获取数据而出现无法调度，最终挂掉。 解决办法：是在使用kubeadm创建集群时使用 –external-etcd-endpoints 参数指定外部 etcd 集群，此时kubeadm 将不会在内部创建 etcd，转而使用外部我们指定的 etcd 集群，如果外部 etcd 集群配置了 SSL 加密，那么还需要配合 –external-etcd-cafile、–external-etcd-certfile、–external-etcd-keyfile 三个参数指定 etcd 的 CA证书、CA签发的使用证书和私钥文件，命令如下 12345# 非 SSL# kubeadm init --external-etcd-endpoints http://x.x.x.1:2379# etcd SSL# kubeadm init --external-etcd-endpoints https://x.x.x.1:2379 --external-etcd-cafile /path/to/ca --external-etcd-certfile /path/to/cert --external-etcd-keyfile /path/to/privatekey 二、etcd 不可与 master 同在‘愿上帝与你同在’……这个坑是由于kubeadm的 check 机制的 bug 造成的，目前还没有修复；表现为 当 etcd 与 master 在同一节点时，kubeadm init 会失败，同时报错信息提示’已经存在了 /var/lib/etcd 目录，或者 2379 端口被占用’；因为默认kubeadm会创建 etcd，而默认的 etcd 会占用这个目录和 2379 端口，即使你加了--external-etcd-endpoints参数，kubeadm仍然会检测这两项条件是否满足，不满足则禁止 init 操作 解决办法：就是要么外部的 etcd 更换数据目录(/var/lib/etcd)和端口，要么干脆不要和 master 放在同一主机即可 三、巨大的日志熟悉的小伙伴应该清楚，基本上每个 kubernetes 组件都会有个通用的参数 --v；这个参数用于控制 kubernetes 各个组件的日志级别，在早期(alpha)的 kubeadm 版本中，如果不进行调整，默认创建集群所有组件日志级别全部为 --v=4 即最高级别输出，这会导致在业务量大的时候磁盘空间以 ‘我去尼玛’ 的速度增长，尤其是 kube-proxy 组件的容器，会疯狂吃掉你的磁盘空间，然后剩下懵逼的你不知为何。在后续的版本中(beta)发现日志级别已经降到了 --v=2，不过对于完全不怎么看日志的我来说还是无卵用…… 解决办法有两种方案: 1、如果已经 –v=4 跑起来了(检查方法就是随便 describe 一个 kube-proxy 的容器，看下 command 字段就能看到)，并且无法停止重建集群，那么最简单的办法就是使用kubectl edit ds xxx方式编译一下相关 ds 文件等，然后手动杀掉相关 pod，让 kubernetes 自动重建即可，如果命令行用着不爽也可以通过 dashboard 更改 2、如果还没开始搭建，或者可以停掉重建，那么只需在kubeadm init之前export KUBE_COMPONENT_LOGLEVEL=&#39;--v=0&#39;即可 四、新节点加入 dns 要你命当 kubeadm 创建好集群以后，如果有需要增加新节点，那么在 kubeadm join 之后务必检查 kube-dns 组件，dns 在某些(weave 启动不完整或不正常)情况下，会由于新节点加入而挂掉，此时整个集群 dns 失效，所以最好 join 完观察一会 dns 状态，如果发现不正常马上杀掉 dns pod，让 kubernetes 自动重建；如果情况允许最好全部 join 完成后直接干掉 dns 让 kubernetes 重建一下 五、单点的 dns 浪起来让你怕（更新-19.4.5）kubeadm 创建的 dns 默认也是单点的，而 dns 至关重要，只要一挂瞬间整个集群全部 game over；不过暂时还是没有发现能在 init 时候创建多个 dns 的方法；不过在集群创建后可以通过kubectl edit deploy kube-dns的方式修改其副本数量，让其创建多个副本即可，目前新版本kubernetes所使用的dns已经不是单pod三container，而是启动两个pod的coredns，解决单点问题 六、coredns一直处于ContainerCreating状态 Node加入集群中后一直处于NotReady状态，查看kube-system的状态，发现coredns一直处于ContainerCreating状态，flannel启动正常 12345# kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster1.rsq.com NotReady master 16h v1.13.0node01.rsq.com NotReady &lt;none&gt; 16h v1.13.0node02.rsq.com NotReady &lt;none&gt; 8m39s v1.13.0 查看kubelet服务状态，看最后几行的报错 1234567891011121314151617181920212223242526# systemctl status kubelet● kubelet.service - kubelet: The Kubernetes Node Agent Loaded: loaded (/etc/systemd/system/kubelet.service; enabled; vendor preset: disabled) Drop-In: /etc/systemd/system/kubelet.service.d └─10-kubeadm.conf Active: active (running) since Wed 2018-12-12 09:24:44 CST; 6min ago Docs: https://kubernetes.io/docs/ Main PID: 123631 (kubelet) Memory: 35.2M CGroup: /system.slice/kubelet.service └─123631 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.yaml --cgrou...Dec 12 09:30:57 master1.rsq.com kubelet[123631]: E1212 09:30:57.187292 123631 pod_workers.go:190] Error syncing pod cea84a11-fd24-11e8-a282-000c291e37c2 ("coredns-86c58d9df4-fzs9l_kube-...Dec 12 09:30:57 master1.rsq.com kubelet[123631]: E1212 09:30:57.187480 123631 pod_workers.go:190] Error syncing pod cea7ebef-fd24-11e8-a282-000c291e37c2 ("coredns-86c58d9df4-hrwvk_kube-...Dec 12 09:30:59 master1.rsq.com kubelet[123631]: E1212 09:30:59.187419 123631 pod_workers.go:190] Error syncing pod cea84a11-fd24-11e8-a282-000c291e37c2 ("coredns-86c58d9df4-fzs9l_kube-...Dec 12 09:30:59 master1.rsq.com kubelet[123631]: E1212 09:30:59.187607 123631 pod_workers.go:190] Error syncing pod cea7ebef-fd24-11e8-a282-000c291e37c2 ("coredns-86c58d9df4-hrwvk_kube-...Dec 12 09:31:00 master1.rsq.com kubelet[123631]: W1212 09:31:00.454147 123631 cni.go:203] Unable to update cni config: No networks found in /etc/cni/net.dDec 12 09:31:00 master1.rsq.com kubelet[123631]: E1212 09:31:00.454242 123631 kubelet.go:2192] Container runtime network not ready: NetworkReady=false reason:NetworkPluginNot...initializedDec 12 09:31:01 master1.rsq.com kubelet[123631]: E1212 09:31:01.188877 123631 pod_workers.go:190] Error syncing pod cea7ebef-fd24-11e8-a282-000c291e37c2 ("coredns-86c58d9df4-hrwvk_kube-...Dec 12 09:31:01 master1.rsq.com kubelet[123631]: E1212 09:31:01.189259 123631 pod_workers.go:190] Error syncing pod cea84a11-fd24-11e8-a282-000c291e37c2 ("coredns-86c58d9df4-fzs9l_kube-...Dec 12 09:31:03 master1.rsq.com kubelet[123631]: E1212 09:31:03.187200 123631 pod_workers.go:190] Error syncing pod cea84a11-fd24-11e8-a282-000c291e37c2 ("coredns-86c58d9df4-fzs9l_kube-...Dec 12 09:31:03 master1.rsq.com kubelet[123631]: E1212 09:31:03.187730 123631 pod_workers.go:190] Error syncing pod cea7ebef-fd24-11e8-a282-000c291e37c2 ("coredns-86c58d9df4-hrwvk_kube-...Hint: Some lines were ellipsized, use -l to show in full.# 附加PS# 报错10.244.0.1 网络已存在（更新） 产生问题原因： 一直报网络NotReady，我就感觉flannel组件出了问题， 最后网上搜了一些资料解决参考博客：coreDNS一直处于创建中解决解决办法：所有节点执行（我只在master节点先执行就解决问题了） 12# rm -rf /var/lib/cni/flannel/* &amp;&amp; rm -rf /var/lib/cni/networks/cbr0/* &amp;&amp; ip link delete cni0# rm -rf /var/lib/cni/networks/cni0/* 删除flannel组件，重新下载 12# docker rmi quay.io/coreos/flannel:v0.10.0-amd64# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml 查看节点状态已经处于Ready状态 12345# kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster1.rsq.com Ready master 16h v1.13.0node01.rsq.com Ready &lt;none&gt; 16h v1.13.0node02.rsq.com Ready &lt;none&gt; 39m v1.13.0 七、kubeadm join报错12345678910在部署服务过程中，初始化之后重启了master节点，然后node节点在join进群的时候报错，提示证书是否过期等问题，报错信息如下：# kubeadm join 10.0.0.100:6443 --token qxl5b3.5b78nwu3gm1r4u6o --discovery-token-ca-cert-hash sha256:3e20fa8054cbc9000cf3d3586a05a01d8af5721b577856e93c7e243877393d21 --ignore-preflight-errors=Swap[preflight] Running pre-flight checks[WARNING Swap]: running with swap on is not supported. Please disable swap [WARNING SystemVerification]: this Docker version is not on the list of validated versions: 18.09.0. Latest validated version: 18.06[discovery] Trying to connect to API Server "10.0.0.100:6443"[discovery] Created cluster-info discovery client, requesting info from "https://10.0.0.100:6443"[discovery] Failed to request cluster info, will try again: [Get https://10.0.0.100:6443/api/v1/namespaces/kube-public/configmaps/cluster-info: dial tcp 10.0.0.100:6443: connect: connection refused][discovery] Failed to request cluster info, will try again: [Get https://10.0.0.100:6443/api/v1/namespaces/kube-public/configmaps/cluster-info: dial tcp 10.0.0.100:6443: connect: connection refused] 产生原因：有可能是时间不同步造成的，在初始化后重启master，重启后会报错 找了好多资料，没有找到可行的，最后kubeadm reset完美解决参考博文： k8s踩坑记 - kubeadm join 之 token 失效reset之后重新初始化 12# kubeadm reset # kubeadm init --kubernetes-version=v1.13.0 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 --token-ttl=0 --ignore-preflight-errors=Swap 创建所需文件 123# mkdir -p $HOME/.kube# cp -i /etc/kubernetes/admin.conf $HOME/.kube/config# chown $(id -u):$(id -g) $HOME/.kube/config 查看节点 123# kubectl get nodesNAME STATUS ROLES AGE VERSIONmaster1.rsq.com NotReady master 2m49s v1.13.0 八、Etcd二进制安装目录也会报错哦etcd服务启动后报错etcd cluster ID mismatch：检查service配置cluster选项有无问题，若无问题，则可能是此前的etcd bootstrap加速启动缓存残留导致，坑爹的是rm -rf /var/lib/etcd/*删除完了之后还是报错，必须rm -rf /var/lib/etcd/才能彻底清除，删除完成后记得再创建该路径mkdir /var/lib/etcd，否则会有类似报错： 1etcd.service: Failed at step CHDIR spawning /usr/local/bin/etcd: No such file or directory 九、二进制安装后重建相关组件会出现secrets报错123May 27 10:34:45 kube-node3 journal: E0527 02:34:45.767392 1 config.go:322] Expected to load root CA config from /var/run/secrets/kubernetes.io/serviceaccount/ca.crt, but got err: open /var/run/secrets/kubernetes.io/serviceaccount/ca.crt: no such file or directory May 27 10:34:45 kube-node3 journal: E0527 02:34:45.767392 1 config.go:322] Expected to load root CA config from /var/run/secrets/kubernetes.io/serviceaccount/ca.crt, but got err: open /var/run/secrets/kubernetes.io/serviceaccount/ca.crt: no such file or directory 分析：产生这个错误是因为Kubernetes默认创建的secrets资源不包含用于访问kube-apiserver的根证书 需要给apiserver设置安全证书，然后删除默认secrets，系统会自动产生新的secrets secrets一般集群安装时默认自动创建 1# kubectl delete secret secretname -n Xxx 十、kubeadm生成集群，加入节点时发现忘记了join token 怎么办？ 1.生成一条永久有效的token 12345678&gt; # kubeadm token create --ttl 0 &gt; o4avtg.65ji6b778nyacw68 &gt; &gt; # kubeadm token list &gt; TOKEN TTL EXPIRES USAGES DESCRIPTION EXTRA GROUPS &gt; dxnj79.rnj561a137ri76ym &lt;invalid&gt; 2018-11-\#02T14:06:43+08:00 authentication,signing &lt;none&gt; system:bootstrappers:kubeadm:default-node-token &gt; o4avtg.65ji6b778nyacw68 &lt;forever&gt; &lt;never&gt; authentication,signing &lt;none&gt; system:bootstrappers:kubeadm:default-node-token &gt; 2.获取ca证书sha256编码hash值 1234&gt; # openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2&gt;/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //' &gt; &gt; 2cc3029123db737f234186636330e87b5510c173c669f513a9c0e0da395515b0 &gt; 3.node节点加入 12&gt; # kubeadm join x.x.x.x:6443 --token o4avtg.65ji6b778nyacw68 --discovery-token-ca-cert-hash sha256:2cc3029123db737f234186636330e87b5510c173c669f513a9c0e0da395515b0 &gt;]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
        <tag>kubernetes_problem</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kubernetes安装—kubeadm_1.13.5]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2FKubernetes%E5%AE%89%E8%A3%85%E2%80%94kubeadm-1-13-5%2F</url>
    <content type="text"><![CDATA[环境描述 kubernetes version：1.13.5 docker version：18.6.3 Redhat：7.6 linux 内核：4.2.0 x.x.x.1 master-1 kube-apiserver、kube-controller-manager、kube-scheduler、etcd、keepalive、docker x.x.x.2 master-2 kube-apiserver、kube-controller-manager、kube-scheduler、etcd、keepalive、docker x.x.x.3 master-3 kube-apiserver、kube-controller-manager、kube-scheduler、etcd、keepalive、docker x.x.x.4 vip x.x.x.5 node-1 kubelet、docker x.x.x.6 node-2 kubelet、docker 因为担心kubeadm起来的etcd不稳定，这里用到的etcd对于kubernetes来说作为外部etcd集群。即使用二进制安装etcd集群，其余组件用kubeadm来完成安装。 环境准备1、准备工作12345678910111213141516echo "1" &gt; /proc/sys/net/bridge/bridge-nf-call-iptables# 停防火墙systemctl stop firewalldsystemctl disable firewalldsystemctl disable firewalld# 关闭Swapswapoff -ased 's/.*swap.*/#&amp;/' /etc/fstab# 关闭防火墙systemctl disable firewalld &amp;&amp; systemctl stop firewalld &amp;&amp; systemctl status firewalld# 关闭Selinuxsetenforce 0sed -i "s/^SELINUX=enforcing/SELINUX=disabled/g" /etc/sysconfig/selinuxsed -i "s/^SELINUX=enforcing/SELINUX=disabled/g" /etc/selinux/configsed -i "s/^SELINUX=permissive/SELINUX=disabled/g" /etc/sysconfig/selinuxsed -i "s/^SELINUX=permissive/SELINUX=disabled/g" /etc/selinux/config 2、docker安装1、下载设置源123sudo yum install -y yum-utils \ device-mapper-persistent-data \ lvm2 123yum-config-manager \--add-repo \https://download.daocloud.io/docker/linux/centos/docker-ce.repo 2、安装docker123yum list docker-ce --showduplicates | sort -r # 列出docker-ce的版本listyum install docker-ce-&lt;版本号&gt; -y # -y 安装docker需要的依赖，其中有个container-selinux的也可以单独下载# rpm -ivh container-selinux-2.33-1.git86f33cd.el7.noarch.rpm 3、启动docker12systemctl start dockerdocker version # 验证docker安装是否完成并启动成功 3、kubeadm/kubelet/kubectl安装 各节点安装 1234567891011121314cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/enabled=1gpgcheck=0repo_gpgcheck=0gpgkey=https://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg https://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpgEOFyum install -y kubelet-1.13.5 kubeadm-1.13.5 kubectl-1.13.5 --disableexcludes=kubernetes #禁用除kubernetes之外的仓库,要用 -y 参数，会自动安装kube-cni等插件systemctl start kubeletsystemctl enable kubelet # kubeadm 要求kubelet保持开机自启状态 4、keepalive安装 master节点安装： 123456789101112131415161718192021222324252627282930313233343536373839404142yum install -y keepalivedsystemctl start keepalivedvim /etc/keepalived/keepalived.confglobal_defs &#123; router_id LVS_k8s&#125;vrrp_script CheckK8sMaster &#123; script &quot;curl -k https://10.70.49.130:6443&quot; interval 3 timeout 9 fall 2 rise 2&#125;vrrp_instance VI_1 &#123; state MASTER interface ens256 # 网卡 virtual_router_id 61 # 主节点权重最高 依次减少 priority 120 advert_int 1 #修改为本地IP mcast_src_ip x.x.x.2 nopreempt authentication &#123; auth_type PASS auth_pass sqP05dQgMSlzrxHj &#125; unicast_peer &#123; x.x.x.1 #x.x.x.2 x.x.x.3 &#125; virtual_ipaddress &#123; x.x.x.4 &#125; track_script &#123; CheckK8sMaster &#125;&#125; kubernetes部署1、etcd二进制部署1234567891011# 1.cfssl签发证书wget https://pkg.cfssl.org/R1.2/cfssl_linux-amd64chmod +x cfssl_linux-amd64mv cfssl_linux-amd64 /usr/local/bin/cfsslwget https://pkg.cfssl.org/R1.2/cfssljson_linux-amd64chmod +x cfssljson_linux-amd64mv cfssljson_linux-amd64 /usr/local/bin/cfssljsonwget https://pkg.cfssl.org/R1.2/cfssl-certinfo_linux-amd64chmod +x cfssl-certinfo_linux-amd64mv cfssl-certinfo_linux-amd64 /usr/local/bin/cfssl-certinfoexport PATH=/usr/local/bin:$PATH 12345678910111213141516171819202122232425262728293031323334353637383940414243444546# 2.根据config.json文件的格式创建如下的ca-config.json文件,过期时间设置成了 87600hmkdir /root/sslcd /root/sslcfssl print-defaults config &gt; config.jsoncfssl print-defaults csr &gt; csr.jsoncat &gt; ca-config.json &lt;&lt;EOF&#123; "signing": &#123; "default": &#123; "expiry": "87600h" &#125;, "profiles": &#123; "kubernetes": &#123; "usages": [ "signing", "key encipherment", "server auth", "client auth" ], "expiry": "87600h" &#125; &#125; &#125;&#125;EOFcat &gt; ca-csr.json &lt;&lt;EOF&#123; "CN": "kubernetes", "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "k8s", "OU": "System" &#125; ]&#125;EOFcfssl gencert -initca ca-csr.json | cfssljson -bare ca 12345678910111213141516171819202122232425262728293031323334# 3.创建kubernetes-etcd证书cat &gt; kubernetes-etcd-csr.json &lt;&lt;EOF&#123; "CN": "kubernetes", "hosts": [ "x.x.x.1", "x.x.x.2", "x.x.x.3", "x.x.x.4", "127.0.0.1", "10.254.0.1", "kubernetes", "kubernetes.default", "kubernetes.default.svc", "kubernetes.default.svc.cluster", "kubernetes.default.svc.cluster.local" ], "key": &#123; "algo": "rsa", "size": 2048 &#125;, "names": [ &#123; "C": "CN", "ST": "BeiJing", "L": "BeiJing", "O": "k8s", "OU": "System" &#125; ]&#125;EOFcfssl gencert -ca=ca.pem -ca-key=ca-key.pem -config=ca-config.json -profile=kubernetes kubernetes-etcd-csr.json | cfssljson -bare etcd 123456# 4.目录证书文件如下etcd-key.pem etcd.pem ca.pem ca.key# 保证三节点证书一致scp /etc/etcd/ssl/* master2:/etc/etcd/ssl/scp /etc/etcd/ssl/* master3:/etc/etcd/ssl/ 证书生成完毕之后，将CA证书ca.pem, etcd秘钥etcd-key.pem, etcd证书etcd.pem拷贝到各节点的/etc/etcd/ssl目录中。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869# 这里就用etcd 3.3.10版本# 一：wget https://github.com/coreos/etcd/releases/download/v3.3.10/etcd-v3.3.10-linux-amd64.tar.gz# 解压缩etcd-v3.3.10-linux-amd64.tar.gz，将其中的etcd和etcdctl两个可执行文件复制到各节点的/usr/bin和/usr/local/bin目录。tar zxvf etcd-v3.3.10-linux-amd64.tar.gzcp etcd-v3.3.10-linux-amd64/etcd* /usr/local/bin/cp etcd-v3.3.10-linux-amd64/etcd* /usr/bin/mkdir /var/lib/etcd #etcd的数据目录mkdir /etc/etcd #etcd的配置文件目录# 二：yum -y install etcd# 安装好etcd之后，就可以修改etcd配置以及启动service，文件路径为：/etc/etcd/etcd.conf和/usr/lib/systemd/system/etcd.servicecat &gt; /etc/etcd/etcd.conf &lt;&lt; EOF# [member]ETCD_NAME=etcd1 # etcd名字 三节点的话为 etcd1 etcd2 etcd3ETCD_DATA_DIR="/data/etcd" # etcd数据目录指定ETCD_LISTEN_PEER_URLS="https://x.x.x.1:2380" # 修改每个master节点的ipETCD_LISTEN_CLIENT_URLS="https://x.x.x.1:2379"#[cluster]ETCD_INITIAL_ADVERTISE_PEER_URLS="https://x.x.x.1:2380"ETCD_INITIAL_CLUSTER_TOKEN="etcd-cluster"ETCD_ADVERTISE_CLIENT_URLS="https://x.x.x.1:2379"EOFcat &gt; /usr/lib/systemd/system/etcd.service &lt;&lt; EOF[Unit]Description=Etcd ServerAfter=network.targetAfter=network-online.targetWants=network-online.targetDocumentation=https://github.com/coreos[Service]Type=notifyWorkingDirectory=/var/lib/etcd/EnvironmentFile=-/etc/etcd/etcd.confExecStart=/usr/local/bin/etcd \ --name $&#123;ETCD_NAME&#125; \ --cert-file=/etc/etcd/ssl/etcd.pem \ --key-file=/etc/etcd/ssl/etcd-key.pem \ --peer-cert-file=/etc/etcd/ssl/etcd.pem \ --peer-key-file=/etc/etcd/ssl/etcd-key.pem \ --trusted-ca-file=/etc/etcd/ssl/ca.pem \ --peer-trusted-ca-file=/etc/etcd/ssl/ca.pem \ --initial-advertise-peer-urls $&#123;ETCD_INITIAL_ADVERTISE_PEER_URLS&#125; \ --listen-peer-urls $&#123;ETCD_LISTEN_PEER_URLS&#125; \ --listen-client-urls $&#123;ETCD_LISTEN_CLIENT_URLS&#125;,http://127.0.0.1:2379 \ --advertise-client-urls $&#123;ETCD_ADVERTISE_CLIENT_URLS&#125; \ --initial-cluster-token $&#123;ETCD_INITIAL_CLUSTER_TOKEN&#125; \ --initial-cluster etcd1=https://x.x.x.1:2380,etcd2=https://x.x.x.2:2380,etcd3=https://x.x.x.3:2380 \ --initial-cluster-state new \ --data-dir=$&#123;ETCD_DATA_DIR&#125;Restart=on-failureRestartSec=5LimitNOFILE=65536[Install]WantedBy=multi-user.targetEOF# 上面在启动参数中指定了etcd的工作目录和数据目录分别是/var/lib/etcd和/data/etcd# –cert-file和–key-file分别指定etcd的公钥证书和私钥# –peer-cert-file和–peer-key-file分别指定了etcd的Peers通信的公钥证书和私钥。# –trusted-ca-file指定了客户端的CA证书# –peer-trusted-ca-file指定了Peers的CA证书# –initial-cluster-state new表示这是新初始化集群，–name指定的参数值必须在–initial-cluster中# 高可用etcd启动需要多节点同时启动才能起来服务# 分别在master节点同时启动etcdsystemctl start etcd.servicesystemctl enable etcd.service 至此，etcd高可用集群搭建完成，可用一下命令验证etcd集群 123456etcdctl \ --ca-file=/etc/etcd/ssl/ca.pem \ --cert-file=/etc/etcd/ssl/etcd.pem \ --key-file=/etc/etcd/ssl/etcd-key.pem \ --endpoints=https://x.x.x.1:2379,https://x.x.x.2:2379,https://x.x.x.3:2379 \ cluster-health 2、kubeadm部署1、master部署 部署之前请确保下载好相关image，翻墙下载或者国内dockerhub下载kubernetes镜像 1、初始化master1 创建master1的初始化配置文件,网络插件采用flannel，CIDR地址是 “10.244.0.0/16”，如下为1.13.5新版本配置文件。 123456789101112131415161718192021222324252627282930313233cat &gt; kubeadm-master.yaml &lt;&lt; EOFapiVersion: kubeadm.k8s.io/v1beta1kind: InitConfigurationlocalAPIEndpoint: advertiseAddress: x.x.x.1 # 本机ip，这里为x.x.x.1-3 bindPort: 6443---apiVersion: kubeadm.k8s.io/v1beta1kind: ClusterConfigurationkubernetesVersion: v1.13.5 # kubernetes版本 对应下载的imageimageRepository: k8s.gcr.io # 自己修改为自己的镜像库名apiServer: certSANs: - "master1" - "master2" - "master3" - "x.x.x.1" - "x.x.x.2" - "x.x.x.3" - "x.x.x.4" - "127.0.0.1"controlPlaneEndpoint: "x.x.x.4:8443" # 控制台ip指定，即vip 实现apiserver高可用etcd: external: endpoints: - https://x.x.x.1:2379 - https://x.x.x.2:2379 - https://x.x.x.3:2379networking: podSubnet: "10.244.0.0/16"EOF 123# 运行初始化命令即可，前提要把相关设置关闭，详情至准备工作# 其中在kubelet配置里加入--pod-infra-container-image 参数指定 pause私有镜像库镜像kubeadm init --config kubeadm-master.yaml 在初始化配置里，对于etcd有两种高可用的选项，一个使用内部etcd，一个使用外部etcd(独立搭建的etcd集群，而不是在初始化中搭建的)，两者初始化配置文件略有不同。 1234567891011121314151617# 使用内部etcd的话，初始化yaml文件中etcd配置如下：etcd: local: extraArgs: listen-client-urls: "https://127.0.0.1:2379,https://x.x.x.x:2379" advertise-client-urls: "https://x.x.x.x:2379" listen-peer-urls: "https://x.x.x.x:2380" initial-advertise-peer-urls: "https://x.x.x.x:2380" initial-cluster: "master1.hanli.com=https://x.x.x.x:2380"# 使用外部etcd的话，etcd: #ETCD的地址 external: endpoints: - https://x.x.x.1:2379 - https://x.x.x.2:2379 - https://x.x.x.3:2379 init初始化之后，如果成功会出现join，这时就可以运行一下命令 机器上的用户要使用kubectl来管理集群操作集群 123mkdir -p $HOME/.kubesudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/configsudo chown $(id -u):$(id -g) $HOME/.kube/config 验证命令 1234567891011kubectl get cs # 如下信息NAME STATUS MESSAGE ERRORcontroller-manager Healthy okscheduler Healthy oketcd-1 Healthy &#123;"health": "true"&#125;etcd-0 Healthy &#123;"health": "true"&#125;etcd-2 Healthy &#123;"health": "true"&#125;kubectl get node # notReady 状态，是因为没有安装网络插件 Name成ip，可修改kubelet的启动参数即可NAME STATUS ROLES AGE VERSIONmaster1 NotReady master 1m v1.13.5 2、启动flannel服务123456789101112wget https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml# flannel 默认会使用主机的第一张网卡，如果你有多张网卡，需要通过配置单独指定。修改 kube-flannel.yml 中的以下部分cat kube-flannel.yml containers: - name: kube-flannel image: quay.io/coreos/flannel:v0.11.0-amd64 # 修改下自己私有镜像库的flannel镜像名 command: - /opt/bin/flanneld args: - --ip-masq - --kube-subnet-mgr - --iface=ens33 #添加 1234567891011121314kubectl apply -f kube-flannel.yml # 创建pod，因为是ds的所以后续集群里面加节点就会自动启动flannelkubectl get node # 会发现 node状态变成了 ReadyNAME STATUS ROLES AGE VERSIONmaster1 Ready master 10m v1.13.5kubectl get po --all-namespacesNAMESPACE NAME READY STATUS RESTARTS AGEkube-system coredns-86c58d9df4-r59rv 1/1 Running 0 59mkube-system coredns-86c58d9df4-rbzx5 1/1 Running 0 59mkube-system kube-apiserver-master1 1/1 Running 0 58mkube-system kube-controller-manager-master1 1/1 Running 16 58mkube-system kube-flannel-ds-amd64-229j2 1/1 Running 0 42mkube-system kube-proxy-4wrg5 1/1 Running 0 59mkube-system kube-scheduler-master1 1/1 Running 13 58m 不是running状态，就说明出错了，通过查看 描述：kubectl describe pod kube-scheduler-master.hanli.com -n kube-system 日志：kubectl logs kube-scheduler-master.hanli.com -n kube-system flannel服务启动成功后，coredns也就会自动启动成功，状态为Running 3、初始化其他master节点 首先把master1上生成的ca证书等，拷贝到其他master节点上，最好免密，可使用pscp等批量任务 12345678910111213&gt; #!/bin/bash&gt; #注意修改为自己的主机名&gt; export CONTROL_PLANE_IPS="master2 master3" &gt; &gt; # 保证节点有/etc/kubernetes/pki目录&gt; # 把以下证书复制到其他master节点&gt; for host in $&#123;CONTROL_PLANE_IPS&#125;; do&gt; scp /etc/kubernetes/pki/*.crt $host:/etc/kubernetes/pki/&gt; scp /etc/kubernetes/pki/*.key $host:/etc/kubernetes/pki/&gt; scp /etc/kubernetes/pki/*.pub $host:/etc/kubernetes/pki/&gt; scp /etc/kubernetes/admin.conf $host:/etc/kubernetes/admin.conf&gt; done&gt; 1234567891011121314151617&gt; tree /etc/kubernetes/pki/&gt; /etc/kubernetes/pki/&gt; ├── apiserver.crt&gt; ├── apiserver-etcd-client.crt&gt; ├── apiserver-etcd-client.key&gt; ├── apiserver.key&gt; ├── apiserver-kubelet-client.crt&gt; ├── apiserver-kubelet-client.key&gt; ├── ca.crt&gt; ├── ca.key&gt; ├── front-proxy-ca.crt&gt; ├── front-proxy-ca.key&gt; ├── front-proxy-client.crt&gt; ├── front-proxy-client.key&gt; ├── sa.key&gt; └── sa.pub&gt; In v1.8.0, kubeadm introduced the kubeadm alpha phase command with the aim of making kubeadm more modular. In v1.13.0 this command graduated to kubeadm init phase. This modularity enables you to invoke atomic sub-steps of the bootstrap process. Hence, you can let kubeadm do some parts and fill in yourself where you need customizations. kubeadm init phase is consistent with the kubeadm init workflow, and behind the scene both use the same code. 在v1.8.0中，kubeadm引入了该kubeadm alpha phase命令，目的是使kubeadm更加模块化。在v1.13.0中，此命令逐渐变为kubeadm init phase。此模块化使您可以调用引导过程的原子子步骤。因此，您可以让kubeadm执行某些操作，并在需要自定义的位置填写您自己的位置。 kubeadm init phase与kubeadm init工作流程一致，并且在场景后面都使用相同的代码。 123kubectl init --config kube-master.yaml# 分别修改对应ip，在在master2-3并执行即可 步骤如master1# 等待kube-proxy flannel启动成功即可 1234567891011121314151617181920kubectl get po -n kube-systemNAME READY STATUS RESTARTS AGEcoredns-9f9d9c76-4zrt4 1/1 Running 0 2dcoredns-9f9d9c76-dqd4c 1/1 Running 0 2dkube-apiserver-zjjh-rq-k8s-1 1/1 Running 0 2dkube-apiserver-zjjh-rq-k8s-2 1/1 Running 0 2dkube-apiserver-zjjh-rq-k8s-3 1/1 Running 0 2dkube-controller-manager-zjjh-rq-k8s-1 1/1 Running 0 2dkube-controller-manager-zjjh-rq-k8s-2 1/1 Running 0 2dkube-controller-manager-zjjh-rq-k8s-3 1/1 Running 0 2dkube-flannel-ds-amd64-7ghn7 1/1 Running 1 2dkube-flannel-ds-amd64-9cqts 1/1 Running 0 2dkube-flannel-ds-amd64-f57nh 1/1 Running 1 2dkube-proxy-8fwts 1/1 Running 0 2dkube-proxy-95tjb 1/1 Running 0 2dkube-proxy-bls94 1/1 Running 0 2dkube-scheduler-zjjh-rq-k8s-1 1/1 Running 0 2dkube-scheduler-zjjh-rq-k8s-2 1/1 Running 0 2dkube-scheduler-zjjh-rq-k8s-3 1/1 Running 0 2dkubernetes-dashboard-67d49f7868-x79wf 1/1 Running 0 76m 2、Work节点加入集群 输入master节点初始化成功之后出现的join命令，出现kubectl get nodes即成功 12&gt; kubeadm join x.x.x.4:8443 --token bnnsb7.amapp1t78llxn54d --discovery-token-ca-cert-hash sha256:520ef89be84c30e480db6d441a7e4179634a9455f0009e249ebe8f35fa792087&gt; 3、集群验证1234567891011121314151617181920# 节点状态[root@master] ~$ kubectl get nodes# 组件状态[root@master] ~$ kubectl get cs# 服务账户[root@master] ~$ kubectl get serviceaccount# 集群信息[root@master] ~$ kubectl cluster-info# 验证dns功能[root@master] ~$ kubectl run curl --image=radial/busyboxplus:curl -it[ root@curl-66959f6557-r4crd:/ ]$ nslookup kubernetes.defaultServer: 10.96.0.10Address 1: 10.96.0.10 kube-dns.kube-system.svc.cluster.localName: kubernetes.defaultAddress 1: 10.96.0.1 kubernetes.default.svc.cluster.local 附录： 12&gt;# kubelet.service配置文件 /use/lib/systemd/system/kubelet.service.d/10-kubeadm.conf&gt; 参考链接： kubeadm安装1.13.5：https://blog.csdn.net/fanren224/article/details/86573264#2master1_165 二进制安装：https://github.com/mritd/ktool 二进制安装kubernetes_v1.13.4：https://mritd.me/2019/03/16/set-up-kubernetes-1.13.4-cluster/]]></content>
      <categories>
        <category>Kubernetes</category>
      </categories>
      <tags>
        <tag>Kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang并发之sync.WaitGroup]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2FGolang%E5%B9%B6%E5%8F%91%E4%B9%8Bsync-WaitGroup%2F</url>
    <content type="text"><![CDATA[声明1var wg sync.WaitGroup sync.WaitGroup/ Golang内置sync包 Wg变量：该类型有三个指针方法，即Add、Done和Wait。 类型sync.WaitGroup是一个结构体类型。当一个sync.WaitGroup类型的变量被声明之后，其值中的那个计数值将会是0。 我们可以通过该值的Add方法增大或减少其中的计数值。 如下： wg.Add(1) 与wg.Add(-1)的执行效果是一致的： wg.Done() 总结: 1、使用Done方法禁忌与Add方法的一样——不要让相应的计数值变为负数。 例如，这段代码中的第5条语句会引发一个运行时恐慌： var wg sync.WaitGroup wg.Add(2) wg.Done() wg.Done() wg.Done() 2、当我们调用sync.WaitGroup类型值的Wait方法的时候，它会去检查该值中的计数值。如果这个计数值为0，那么该方法会立即返回，且不会对程序的运行产生任何影响。 但是，如果这个计数值大于0，那么该方法的调用方所属的那个Goroutine就会被阻塞。直到该计数值重新变为0之时，为此而被阻塞的所有Goroutine才会被唤醒。 代码案例1234567891011121314151617181920212223242526272829303132333435// 协调多个Goroutine的运行。假设，在我们的程序中启用了4个Goroutine，分别是G1、G2、G3和G4。其中，G2、G3和G4是由G1中的代码启用并被用于执行某些特定任务的。G1在启用这3个Goroutine之后要等待这些特定任务的完成。// 方案1（channel通道）sign := make(chan byte, 3)go func() &#123; //G2 sign &lt;- 2&#125;()go func() &#123; //G3 sign &lt;- 3&#125;()go func() &#123; //G4 sign &lt;- 4&#125;()for i := 0; i &lt; 3; i++ &#123;fmt.Printf("G%d is ended.\n", &lt;-sign)&#125;// 方案2（sync.WaitGroup）var wg sync.WaitGroupwg.Add(3)go func() &#123; //G2 wg.Done()&#125;()go func() &#123; //G3 wg.Done()&#125;()go func() &#123; //G4 wg.Done()&#125;()wg.Wait()fmt.Println("Finish") 12345678910111213141516171819202122package mainimport ( "fmt" "sync" "time")func main() &#123; var wg sync.WaitGroup for i := 0; i &lt; 5; i = i + 1 &#123; wg.Add(1) go func(n int) &#123; // defer wg.Done() defer wg.Add(-1) EchoNumber(n) &#125;(i) // n = i &#125; wg.Wait()&#125;func EchoNumber(i int) &#123; time.Sleep(3e9) fmt.Println(i)&#125;]]></content>
      <categories>
        <category>Golang并发</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Golang并发之原子操作]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2FGolang%E5%B9%B6%E5%8F%91%E4%B9%8B%E5%8E%9F%E5%AD%90%E6%93%8D%E4%BD%9C%2F</url>
    <content type="text"><![CDATA[1、]]></content>
      <categories>
        <category>Golang并发</category>
      </categories>
      <tags>
        <tag>Golang</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo入坑-主题篇]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2FHexo%E5%85%A5%E5%9D%91-%E4%B8%BB%E9%A2%98%E7%AF%87%2F</url>
    <content type="text"></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hexo入坑-安装篇]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2FHexo%E5%85%A5%E5%9D%91-%E5%AE%89%E8%A3%85%E7%AF%87%2F</url>
    <content type="text"><![CDATA[准备 有一个github账号，没有的话去注册一个 安装了git、node.js、npm，并了解相关基础知识 12node -vgit --version 安装了git for Mac（或者其它git客户端） 安装1. 创建github仓库​ github上新建一个名为用户名.github.io的项目，例如：sheldon-lu.github.io， 这个项目名是用来做域名用的，当然也可以去申请一个属于自己的域名，一般推荐godaddy 2. 配置SSH key免密上传不详细讲了，命令如下： 1234567891011cd ~/.ssh# 检查本机已存在的ssh密钥# 如果提示：No such file or directory 说明你是第一次使用git。ssh-keygen -t rsa -C "邮件地址"# 然后连续3次回车，最终会生成一个文件在用户目录下，打开用户目录，找到.ssh\id_rsa.pub文件，记事本打开并复制里面的内容，打开你的github主页，进入个人设置 -&gt; SSH and GPG keys -&gt; New SSH key# 测试是否成功，验证命令：ssh -T git@GitHub.com## Hi cnfeat! You've successfully authenticated, but GitHub does not provide shell access.出现这个就是成功了，之后可以测试一下push是否免密上传。# 此时你可能还需要配置，这个属于全局config，对于git项目完全可以git clone下来之后进行push等等操作：git config --global user.name "liuxianan"// 你的github用户名，非昵称git config --global user.email "xxx@qq.com"// 填写你的github注册邮箱 3. Hexo安装及介绍官网： http://hexo.io github: https://github.com/hexojs/hexo 由于github pages存放的都是静态文件，博客存放的不只是文章内容，还有文章列表、分类、标签、翻页等动态内容，假如每次写完一篇文章都要手动更新博文目录和相关链接信息，相信谁都会疯掉，所以hexo所做的就是将这些md文件都放在本地，每次写完文章后调用写好的命令来批量完成相关页面的生成，然后再将有改动的页面提交到github。 安装之前先来说几个注意事项： 很多命令既可以用Windows的cmd来完成，也可以使用git bash来完成，但是部分命令会有一些问题，为避免不必要的问题，建议全部使用git bash来执行（对于windows用户而言）； hexo不同版本差别比较大，网上很多文章的配置信息都是基于2.x的，所以注意不要被误导； hexo有2种_config.yml文件，一个是根目录下的全局的_config.yml，一个是各个theme下的； 这里给官网上的部署命令： 1234567891011mkdir &lt;Your blog&gt; # 创建blog文档文件夹npm install hexo-cli -g # 安装hexo以及hexo命令# 初始化一个blog项目，其中'blog'为你的blog名字，hexo会自动下载一些文件到这个目录，包括node_modules.hexo init blogcd blognpm installhexo server## 这两个命令是经常会用到的hexo s -g #生成并本地预览hexo d -g #生成并上传 对于mac用户，如果遇到以下报错： 123456789101112131415161718安装npm install hexo-cli -g时报错npm install hexo -gnpm WARN locking Error: EACCES, open &apos;/Users/lushenneng/.npm/_locks/hexo-4ded2cf5ea4a8daa.lock&apos;npm WARN locking at Error (native)npm WARN locking /Users/lushenneng/.npm/_locks/hexo-4ded2cf5ea4a8daa.lock failed &#123; [Error: EACCES, open &apos;/Users/lushenneng/.npm/_locks/hexo-4ded2cf5ea4a8daa.lock&apos;]npm WARN locking errno: -13,npm WARN locking code: &apos;EACCES&apos;,npm WARN locking path: &apos;/Users/lushenneng/.npm/_locks/hexo-4ded2cf5ea4a8daa.lock&apos; &#125;npm ERR! Darwin 14.0.0npm ERR! argv &quot;node&quot; &quot;/usr/bin/npm&quot; &quot;install&quot; &quot;hexo&quot; &quot;-g&quot;npm ERR! node v0.12.3npm ERR! npm v2.9.1npm ERR! Attempt to unlock /usr/lib/node_modules/hexo, which hasn&apos;t been lockednpm ERR!npm ERR! If you need help, you may report this error at:npm ERR! https://github.com/npm/npm/issuesnpm ERR! Please include the following file with any support request:npm ERR! /Users/lushenneng/blog/npm-debug.log 解决： 12345# 可以先用这个命令：sudo npm install hexo-cli -g# 如果还是报错可以用这个命令：sudo npm install --unsafe-perm --verbose -g hexo# 加上sudo一般能解决很多问题 4. START Hexo在blog目录下的_config.yml配置修改如下：(大概是在最后一行) 12345deploy: type: git repository: github: git@github.com:sheldon-lu/sheldon_blog.git branch: master 我这里不指定用的github.io这个域名是因为我用sheldon-lu.github.io作主域名，sheldon_blog作文根，当然这样的话这边hexo内设置如下：(大概在14行左右开始) 1234567# URL## If your site is put in a subdirectory, set url as &apos;http://yoursite.com/child&apos; and root as &apos;/child/&apos;url: https://sheldon-lu.github.io/root: /sheldon_blog# permalink: :year/:month/:day/:title/permalink: passages/:title/permalink_defaults: 至此，就可以使用了，浏览器输入域名：https://sheldon-lu.github.io/sheldon_blog即可 1、这里要说明一下，使用 hexo d 上传至github时如果需要用到 gh-pages 的话，记得在自己github的blog项目下创建个gh-pages的分支，即可; 2、_config.yml中配置repository时一定配置的要是ssh，别弄的什么https://github之类的 参考地址：https://ngwind.github.io/2018/07/27/%E4%BD%BF%E7%94%A8gh-pages+Hexo%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E9%A1%B5%E6%95%99%E7%A8%8B/]]></content>
      <categories>
        <category>Hexo</category>
      </categories>
      <tags>
        <tag>Hexo</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fsheldon_blog%2Fpassages%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Please enter the password to read the blog. Incorrect Password! No content to display! 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]]></content>
      <tags>
        <tag>每天一种方式</tag>
      </tags>
  </entry>
</search>
