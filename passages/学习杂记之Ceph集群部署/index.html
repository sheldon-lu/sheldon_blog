<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"www.sheldonlu.com","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="CentOS7下安装Ceph供Kubernetes使用环境说明1234系统：CentOS7，一个非系统分区分配给cephdocker：18.06-cekubernetes：1.11.3ceph：luminous    Ceph部署准备">
<meta property="og:type" content="article">
<meta property="og:title" content="学习杂记之Ceph集群部署">
<meta property="og:url" content="https://www.sheldonlu.com/passages/%E5%AD%A6%E4%B9%A0%E6%9D%82%E8%AE%B0%E4%B9%8BCeph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/index.html">
<meta property="og:site_name" content="Sheldon_Lu">
<meta property="og:description" content="CentOS7下安装Ceph供Kubernetes使用环境说明1234系统：CentOS7，一个非系统分区分配给cephdocker：18.06-cekubernetes：1.11.3ceph：luminous    Ceph部署准备">
<meta property="og:locale">
<meta property="article:published_time" content="2019-09-03T15:31:00.000Z">
<meta property="article:modified_time" content="2020-06-30T13:40:33.248Z">
<meta property="article:author" content="Sheldon Lu">
<meta property="article:tag" content="学习杂记">
<meta property="article:tag" content="Ceph">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://www.sheldonlu.com/passages/%E5%AD%A6%E4%B9%A0%E6%9D%82%E8%AE%B0%E4%B9%8BCeph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-Hans'
  };
</script>

  <title>学习杂记之Ceph集群部署 | Sheldon_Lu</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Sheldon_Lu</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>About</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="https://www.sheldonlu.com/passages/%E5%AD%A6%E4%B9%A0%E6%9D%82%E8%AE%B0%E4%B9%8BCeph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Sheldon Lu">
      <meta itemprop="description" content="静下心来写点东西">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sheldon_Lu">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          学习杂记之Ceph集群部署
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2019-09-03 23:31:00" itemprop="dateCreated datePublished" datetime="2019-09-03T23:31:00+08:00">2019-09-03</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-06-30 21:40:33" itemprop="dateModified" datetime="2020-06-30T21:40:33+08:00">2020-06-30</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Study/" itemprop="url" rel="index"><span itemprop="name">Study</span></a>
                </span>
            </span>

          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
      <span class="post-meta-item-text">Valine: </span>
    
    <a title="valine" href="/passages/%E5%AD%A6%E4%B9%A0%E6%9D%82%E8%AE%B0%E4%B9%8BCeph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/#valine-comments" itemprop="discussionUrl">
      <span class="post-comments-count valine-comment-count" data-xid="/passages/%E5%AD%A6%E4%B9%A0%E6%9D%82%E8%AE%B0%E4%B9%8BCeph%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/" itemprop="commentCount"></span>
    </a>
  </span>
  
  <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>38k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>34 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h3 id="CentOS7下安装Ceph供Kubernetes使用"><a href="#CentOS7下安装Ceph供Kubernetes使用" class="headerlink" title="CentOS7下安装Ceph供Kubernetes使用"></a>CentOS7下安装Ceph供Kubernetes使用</h3><h4 id="环境说明"><a href="#环境说明" class="headerlink" title="环境说明"></a>环境说明</h4><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">系统：CentOS7，一个非系统分区分配给ceph</span><br><span class="line">docker：18.06-ce</span><br><span class="line">kubernetes：1.11.3</span><br><span class="line">ceph：luminous</span><br></pre></td></tr></table></figure>



<h4 id="Ceph部署准备"><a href="#Ceph部署准备" class="headerlink" title="Ceph部署准备"></a>Ceph部署准备<span id="more"></span></h4><h5 id="节点规划"><a href="#节点规划" class="headerlink" title="节点规划"></a>节点规划</h5><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">192.168.105.92 lab1  # master1</span><br><span class="line">192.168.105.93 lab2  # master2</span><br><span class="line">192.168.105.94 lab3  # master3</span><br><span class="line">192.168.105.95 lab4  # node4</span><br><span class="line">192.168.105.96 lab5  # node5</span><br><span class="line">192.168.105.97 lab6  # node6</span><br><span class="line">192.168.105.98 lab7  # node7</span><br></pre></td></tr></table></figure>

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">监控节点：lab1、lab2、lab3</span><br><span class="line">OSD节点：lab4、lab5、lab6、lab7</span><br><span class="line">MDS节点：lab4</span><br></pre></td></tr></table></figure>



<h5 id="添加yum源"><a href="#添加yum源" class="headerlink" title="添加yum源"></a>添加yum源</h5><p>我们使用阿里云yum源：(CeontOS和epel也是阿里云yum源)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/yum.repos.d/ceph.repo</span><br><span class="line">[Ceph]</span><br><span class="line">name=Ceph packages for $basearch</span><br><span class="line">baseurl=https://mirrors.aliyun.com/ceph/rpm-luminous/el7/$basearch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br><span class="line"></span><br><span class="line">[Ceph-noarch]</span><br><span class="line">name=Ceph noarch packages</span><br><span class="line">baseurl=https://mirrors.aliyun.com/ceph/rpm-luminous/el7/noarch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br><span class="line"></span><br><span class="line">[ceph-source]</span><br><span class="line">name=Ceph source packages</span><br><span class="line">baseurl=https://mirrors.aliyun.com/ceph/rpm-luminous/el7/SRPMS</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br></pre></td></tr></table></figure>

<p>注意：ceph集群中节点都需要添加该yum源</p>
<h5 id="安装Ceph部署工具"><a href="#安装Ceph部署工具" class="headerlink" title="安装Ceph部署工具"></a>安装Ceph部署工具</h5><p><strong>以下操作在lab1上root用户操作</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">安装 ceph-deploy</span><br><span class="line">yum -y install ceph-deploy</span><br></pre></td></tr></table></figure>



<h5 id="安装时间同步工具chrony"><a href="#安装时间同步工具chrony" class="headerlink" title="安装时间同步工具chrony"></a>安装时间同步工具chrony</h5><p>以下操作在所有ceph节点root用户操作</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">yum -y install chrony</span><br><span class="line">systemctl start chronyd</span><br><span class="line">systemctl enable chronyd</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">修改/etc/chrony.conf前面的server段为如下</span></span><br><span class="line">server ntp1.aliyun.com iburst</span><br><span class="line">server ntp2.aliyun.com iburst</span><br><span class="line">server ntp3.aliyun.com iburst</span><br><span class="line">server ntp4.aliyun.com iburst</span><br></pre></td></tr></table></figure>



<h5 id="安装SSH服务"><a href="#安装SSH服务" class="headerlink" title="安装SSH服务"></a>安装SSH服务</h5><p>默认已正常运行，略</p>
<h5 id="创建部署-CEPH-的用户"><a href="#创建部署-CEPH-的用户" class="headerlink" title="创建部署 CEPH 的用户"></a>创建部署 CEPH 的用户</h5><p><strong>以下操作在所有ceph节点root用户操作</strong></p>
<p>ceph-deploy 工具必须以普通用户登录 Ceph 节点，且此用户拥有无密码使用 sudo 的权限，因为它需要在安装软件及配置文件的过程中，不必输入密码。</p>
<p>较新版的 ceph-deploy 支持用 –username 选项提供可无密码使用 sudo 的用户名（包括 root ，虽然不建议这样做）。使用 ceph-deploy –username {username} 命令时，指定的用户必须能够通过无密码 SSH 连接到 Ceph 节点，因为 ceph-deploy 中途不会提示输入密码。</p>
<p>建议在集群内的所有 Ceph 节点上给 ceph-deploy 创建一个特定的用户，但不要用 “ceph” 这个名字。全集群统一的用户名可简化操作（非必需），然而你应该避免使用知名用户名，因为黑客们会用它做暴力破解（如 root 、 admin 、 {productname} ）。后续步骤描述了如何创建无 sudo 密码的用户，你要用自己取的名字替换 {username} 。</p>
<p>注意：<br>从 Infernalis 版起，用户名 “ceph” 保留给了 Ceph 守护进程。如果 Ceph 节点上已经有了 “ceph” 用户，升级前必须先删掉这个用户。</p>
<p><strong>我们使用用户名ceph-admin</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">username=&quot;ceph-admin&quot;</span><br><span class="line">useradd $&#123;username&#125; &amp;&amp; echo &#x27;PASSWORD&#x27; | passwd $&#123;ceph-admin&#125; --stdinecho &quot;$&#123;username&#125; ALL = (root) NOPASSWD:ALL&quot; | sudo tee /etc/sudoers.d/$&#123;username&#125;</span><br><span class="line">chmod 0440 /etc/sudoers.d/$&#123;username&#125;</span><br><span class="line">chmod a+x /etc/sudoers.d/</span><br></pre></td></tr></table></figure>



<h5 id="允许无密码-SSH-登录"><a href="#允许无密码-SSH-登录" class="headerlink" title="允许无密码 SSH 登录"></a>允许无密码 SSH 登录</h5><p>以下操作在lab1节点ceph-admin用户操作</p>
<p>正因为 ceph-deploy 不支持输入密码，你必须在管理节点上生成 SSH 密钥并把其公钥分发到各 Ceph 节点。 ceph-deploy 会尝试给初始 monitors 生成 SSH 密钥对。</p>
<p>生成 SSH 密钥对，但不要用 sudo 或 root 用户。提示 “Enter passphrase” 时，直接回车，口令即为空：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">su - ceph-admin  # 切换到此用户，因为ceph-deploy也用此用户</span><br><span class="line">ssh-keygen</span><br><span class="line"></span><br><span class="line">Generating public/private rsa key pair.</span><br><span class="line">Enter file in which to save the key (/home/ceph-admin/.ssh/id_rsa): </span><br><span class="line">/home/ceph-admin/.ssh/id_rsa already exists.</span><br><span class="line">Overwrite (y/n)? y</span><br><span class="line">Enter passphrase (empty for no passphrase): </span><br><span class="line">Enter same passphrase again: </span><br><span class="line">Your identification has been saved in /home/ceph-admin/.ssh/id_rsa.</span><br><span class="line">Your public key has been saved in /home/ceph-admin/.ssh/id_rsa.pub.</span><br><span class="line">The key fingerprint is:</span><br></pre></td></tr></table></figure>

<p>把公钥拷贝到各 Ceph 节点，把下列命令中的 {username} 替换成前面创建部署 Ceph 的用户里的用户名。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">username=&quot;ceph-admin&quot;</span><br><span class="line">ssh-copy-id $&#123;username&#125;@lab1</span><br><span class="line">ssh-copy-id $&#123;username&#125;@lab2</span><br><span class="line">ssh-copy-id $&#123;username&#125;@lab3</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>（推荐做法）修改 ceph-deploy 管理节点上的 ~&#x2F;.ssh&#x2F;config 文件，这样 ceph-deploy 就能用你所建的用户名登录 Ceph 节点了，而无需每次执行 ceph-deploy 都要指定 –username {username} 。这样做同时也简化了 ssh 和 scp 的用法。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Host lab1</span><br><span class="line">   Hostname lab1</span><br><span class="line">   User ceph-admin</span><br><span class="line">Host lab2</span><br><span class="line">   Hostname lab2</span><br><span class="line">   User ceph-admin</span><br><span class="line">Host lab3</span><br><span class="line">   Hostname lab3</span><br><span class="line">   User ceph-admin   </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Bad owner or permissions on /home/ceph-admin/.ssh/config</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">需要用<span class="built_in">chmod</span> 600 ~/.ssh/config解决。</span></span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h5 id="开放所需端口"><a href="#开放所需端口" class="headerlink" title="开放所需端口"></a>开放所需端口</h5><blockquote>
<p>以下操作在所有监视器节点<code>root</code>用户操作</p>
</blockquote>
<p>Ceph Monitors 之间默认使用 <code>6789</code> 端口通信， OSD 之间默认用 6800:7300 这个范围内的端口通信。</p>
<p><code>firewall-cmd --zone=public --add-port=6789/tcp --permanent &amp;&amp; firewall-cmd --reload</code></p>
<h5 id="终端（-TTY-）"><a href="#终端（-TTY-）" class="headerlink" title="终端（ TTY ）"></a>终端（ TTY ）</h5><blockquote>
<p>以下操作在所有ceph节点root用户操作</p>
</blockquote>
<p>在 CentOS 和 RHEL 上执行 ceph-deploy 命令时可能会报错。如果你的 Ceph 节点默认设置了 requiretty ，执行 sudo visudo 禁用它，并找到 Defaults requiretty 选项，把它改为 Defaults:ceph !requiretty 或者直接注释掉，这样 ceph-deploy 就可以用之前创建的用户（创建部署 Ceph 的用户 ）连接了。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed -i -r &#x27;s@Defaults(.*)!visiblepw@Defaults:ceph-admin\1!visiblepw@g&#x27; /etc/sudoers</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h5 id="SELINUX"><a href="#SELINUX" class="headerlink" title="SELINUX"></a>SELINUX</h5><blockquote>
<p>以下操作在所有ceph节点root用户操作</p>
</blockquote>
<p>如果原来是开启的，需要重启生效。</p>
<p>永久关闭 修改&#x2F;etc&#x2F;sysconfig&#x2F;selinux文件设置</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sed -i &#x27;s/SELINUX=.*/SELINUX=disabled/&#x27; /etc/sysconfig/selinux</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h5 id="整理以上所有ceph节点操作"><a href="#整理以上所有ceph节点操作" class="headerlink" title="整理以上所有ceph节点操作"></a>整理以上所有ceph节点操作</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">yum -y install chrony</span><br><span class="line">systemctl start chronyd</span><br><span class="line">systemctl enable chronyd</span><br><span class="line"></span><br><span class="line">username=&quot;ceph-admin&quot;</span><br><span class="line">useradd $&#123;username&#125; &amp;&amp; echo &#x27;PASSWORD&#x27; | passwd $&#123;username&#125; --stdin</span><br><span class="line">echo &quot;$&#123;username&#125; ALL = (root) NOPASSWD:ALL&quot; | sudo tee /etc/sudoers.d/$&#123;username&#125;</span><br><span class="line">chmod 0440 /etc/sudoers.d/$&#123;username&#125;</span><br><span class="line">chmod a+x /etc/sudoers.d/</span><br><span class="line">sed -i -r &#x27;s@Defaults(.*)!visiblepw@Defaults:ceph-admin\1!visiblepw@g&#x27; /etc/sudoers</span><br><span class="line">sed -i &#x27;s/SELINUX=.*/SELINUX=disabled/&#x27; /etc/sysconfig/selinux</span><br><span class="line">yum -y install ceph</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h4 id="存储集群部署"><a href="#存储集群部署" class="headerlink" title="存储集群部署"></a>存储集群部署</h4><blockquote>
<p>以下操作在lab节点<code>ceph-admin</code>用户操作</p>
</blockquote>
<p>我们创建一个 Ceph 存储集群，它有一个 Monitor 和两个 OSD 守护进程。一旦集群达到 active + clean 状态，再扩展它：增加第三个 OSD 、增加元数据服务器和两个 Ceph Monitors。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">su - ceph-admin</span><br><span class="line">mkdir ceph-cluster</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>如果在某些地方碰到麻烦，想从头再来，可以用下列命令清除配置：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy purgedata &#123;ceph-node&#125; [&#123;ceph-node&#125;]</span><br><span class="line">ceph-deploy forgetkeys</span><br><span class="line">rm -rf /etc/ceph/*</span><br><span class="line">rm -rf /var/lib/ceph/*/*</span><br><span class="line">rm -rf /var/log/ceph/*</span><br><span class="line">rm -rf /var/run/ceph/*</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h5 id="创建集群并准备配置"><a href="#创建集群并准备配置" class="headerlink" title="创建集群并准备配置"></a>创建集群并准备配置</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd ceph-cluster</span><br><span class="line">ceph-deploy new lab1</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">[ceph-admin@lab1 ceph-cluster]$ ls -l</span><br><span class="line">total 24</span><br><span class="line">-rw-rw-r-- 1 ceph-admin ceph-admin   196 Aug 17 14:31 ceph.conf</span><br><span class="line">-rw-rw-r-- 1 ceph-admin ceph-admin 12759 Aug 17 14:31 ceph-deploy-ceph.log</span><br><span class="line">-rw------- 1 ceph-admin ceph-admin    73 Aug 17 14:31 ceph.mon.keyring</span><br><span class="line">[ceph-admin@lab1 ceph-cluster]$ more ceph.mon.keyring </span><br><span class="line">[mon.]</span><br><span class="line">key = AQC2a3ZbAAAAABAAor15nkYQCXuC681B/Q53og==</span><br><span class="line">caps mon = allow *</span><br><span class="line">[ceph-admin@lab1 ceph-cluster]$ more ceph.conf </span><br><span class="line">[global]</span><br><span class="line">fsid = fb212173-233c-4c5e-a98e-35be9359f8e2</span><br><span class="line">mon_initial_members = lab1</span><br><span class="line">mon_host = 192.168.105.92</span><br><span class="line">auth_cluster_required = cephx</span><br><span class="line">auth_service_required = cephx</span><br><span class="line">auth_client_required = cephx</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>把 Ceph 配置文件里的默认副本数从 3 改成 2 ，这样只有两个 OSD 也可以达到 active + clean 状态。把下面这行加入 [global] 段：</p>
<p><code>osd pool default size = 2</code></p>
<p>再把 <code>public network</code> 写入 Ceph 配置文件的<code>[global]</code>段下</p>
<p><code>public network = 192.168.105.0/24</code></p>
<p>安装 Ceph</p>
<p><code>ceph-deploy install lab1 lab4 lab5 --no-adjust-repos</code></p>
<p>配置monitor(s)并收集所有密钥：</p>
<p><code>ceph-deploy mon create-initial</code></p>
<p>完成上述操作后，当前目录里应该会出现这些密钥环：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line">[ceph-admin@lab1 ceph-cluster]$ ceph-deploy mon create-initial</span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /home/ceph-admin/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO ] Invoked (2.0.1): /bin/ceph-deploy mon create-initial</span><br><span class="line">[ceph_deploy.cli][INFO ] ceph-deploy options:</span><br><span class="line">[ceph_deploy.cli][INFO ]  username                      : None</span><br><span class="line">[ceph_deploy.cli][INFO ]  verbose                       : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  overwrite_conf                : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  subcommand                    : create-initial</span><br><span class="line">[ceph_deploy.cli][INFO ]  quiet                         : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x1160f38&gt;</span><br><span class="line">[ceph_deploy.cli][INFO ]  cluster                       : ceph</span><br><span class="line">[ceph_deploy.cli][INFO ]  func                          : &lt;function mon at 0x115c2a8&gt;</span><br><span class="line">[ceph_deploy.cli][INFO ]  ceph_conf                     : None</span><br><span class="line">[ceph_deploy.cli][INFO ]  default_release               : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  keyrings                      : None</span><br><span class="line">[ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts lab1</span><br><span class="line">[ceph_deploy.mon][DEBUG ] detecting platform for host lab1 ...</span><br><span class="line">[lab1][DEBUG ] connection detected need for sudo</span><br><span class="line">[lab1][DEBUG ] connected to host: lab1 </span><br><span class="line">[lab1][DEBUG ] detect platform information from remote host</span><br><span class="line">[lab1][DEBUG ] detect machine type</span><br><span class="line">[lab1][DEBUG ] find the location of an executable</span><br><span class="line">[ceph_deploy.mon][INFO ] distro info: CentOS Linux 7.5.1804 Core</span><br><span class="line">[lab1][DEBUG ] determining if provided host has same hostname in remote</span><br><span class="line">[lab1][DEBUG ] get remote short hostname</span><br><span class="line">[lab1][DEBUG ] deploying mon to lab1</span><br><span class="line">[lab1][DEBUG ] get remote short hostname</span><br><span class="line">[lab1][DEBUG ] remote hostname: lab1</span><br><span class="line">[lab1][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</span><br><span class="line">[lab1][DEBUG ] create the mon path if it does not exist</span><br><span class="line">[lab1][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-lab1/done</span><br><span class="line">[lab1][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-lab1/done</span><br><span class="line">[lab1][INFO ] creating keyring file: /var/lib/ceph/tmp/ceph-lab1.mon.keyring</span><br><span class="line">[lab1][DEBUG ] create the monitor keyring file</span><br><span class="line">[lab1][INFO ] Running command: sudo ceph-mon --cluster ceph --mkfs -i lab1 --keyring /var/lib/ceph/tmp/ceph-lab1.mon.keyring --setuser 167 --setgroup 167</span><br><span class="line">[lab1][INFO ] unlinking keyring file /var/lib/ceph/tmp/ceph-lab1.mon.keyring</span><br><span class="line">[lab1][DEBUG ] create a done file to avoid re-doing the mon deployment</span><br><span class="line">[lab1][DEBUG ] create the init path if it does not exist</span><br><span class="line">[lab1][INFO ] Running command: sudo systemctl enable ceph.target</span><br><span class="line">[lab1][INFO ] Running command: sudo systemctl enable ceph-mon@lab1</span><br><span class="line">[lab1][INFO ] Running command: sudo systemctl start ceph-mon@lab1</span><br><span class="line">[lab1][INFO ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.lab1.asok mon_status</span><br><span class="line">[lab1][DEBUG ] ********************************************************************************</span><br><span class="line">[lab1][DEBUG ] status for monitor: mon.lab1</span><br><span class="line">[lab1][DEBUG ] &#123;</span><br><span class="line">[lab1][DEBUG ]   &quot;election_epoch&quot;: 3, </span><br><span class="line">[lab1][DEBUG ]   &quot;extra_probe_peers&quot;: [], </span><br><span class="line">[lab1][DEBUG ]   &quot;feature_map&quot;: &#123;</span><br><span class="line">[lab1][DEBUG ]     &quot;mon&quot;: &#123;</span><br><span class="line">[lab1][DEBUG ]       &quot;group&quot;: &#123;</span><br><span class="line">[lab1][DEBUG ]         &quot;features&quot;: &quot;0x3ffddff8eea4fffb&quot;, </span><br><span class="line">[lab1][DEBUG ]         &quot;num&quot;: 1, </span><br><span class="line">[lab1][DEBUG ]         &quot;release&quot;: &quot;luminous&quot;</span><br><span class="line">[lab1][DEBUG ]       &#125;</span><br><span class="line">[lab1][DEBUG ]     &#125;</span><br><span class="line">[lab1][DEBUG ]   &#125;, </span><br><span class="line">[lab1][DEBUG ]   &quot;features&quot;: &#123;</span><br><span class="line">[lab1][DEBUG ]     &quot;quorum_con&quot;: &quot;4611087853745930235&quot;, </span><br><span class="line">[lab1][DEBUG ]     &quot;quorum_mon&quot;: [</span><br><span class="line">[lab1][DEBUG ]       &quot;kraken&quot;, </span><br><span class="line">[lab1][DEBUG ]       &quot;luminous&quot;</span><br><span class="line">[lab1][DEBUG ]     ], </span><br><span class="line">[lab1][DEBUG ]     &quot;required_con&quot;: &quot;153140804152475648&quot;, </span><br><span class="line">[lab1][DEBUG ]     &quot;required_mon&quot;: [</span><br><span class="line">[lab1][DEBUG ]       &quot;kraken&quot;, </span><br><span class="line">[lab1][DEBUG ]       &quot;luminous&quot;</span><br><span class="line">[lab1][DEBUG ]     ]</span><br><span class="line">[lab1][DEBUG ]   &#125;, </span><br><span class="line">[lab1][DEBUG ]   &quot;monmap&quot;: &#123;</span><br><span class="line">[lab1][DEBUG ]     &quot;created&quot;: &quot;2018-08-17 14:46:18.770540&quot;, </span><br><span class="line">[lab1][DEBUG ]     &quot;epoch&quot;: 1, </span><br><span class="line">[lab1][DEBUG ]     &quot;features&quot;: &#123;</span><br><span class="line">[lab1][DEBUG ]       &quot;optional&quot;: [], </span><br><span class="line">[lab1][DEBUG ]       &quot;persistent&quot;: [</span><br><span class="line">[lab1][DEBUG ]         &quot;kraken&quot;, </span><br><span class="line">[lab1][DEBUG ]         &quot;luminous&quot;</span><br><span class="line">[lab1][DEBUG ]       ]</span><br><span class="line">[lab1][DEBUG ]     &#125;, </span><br><span class="line">[lab1][DEBUG ]     &quot;fsid&quot;: &quot;fb212173-233c-4c5e-a98e-35be9359f8e2&quot;, </span><br><span class="line">[lab1][DEBUG ]     &quot;modified&quot;: &quot;2018-08-17 14:46:18.770540&quot;, </span><br><span class="line">[lab1][DEBUG ]     &quot;mons&quot;: [</span><br><span class="line">[lab1][DEBUG ]       &#123;</span><br><span class="line">[lab1][DEBUG ]         &quot;addr&quot;: &quot;192.168.105.92:6789/0&quot;, </span><br><span class="line">[lab1][DEBUG ]         &quot;name&quot;: &quot;lab1&quot;, </span><br><span class="line">[lab1][DEBUG ]         &quot;public_addr&quot;: &quot;192.168.105.92:6789/0&quot;, </span><br><span class="line">[lab1][DEBUG ]         &quot;rank&quot;: 0</span><br><span class="line">[lab1][DEBUG ]       &#125;</span><br><span class="line">[lab1][DEBUG ]     ]</span><br><span class="line">[lab1][DEBUG ]   &#125;, </span><br><span class="line">[lab1][DEBUG ]   &quot;name&quot;: &quot;lab1&quot;, </span><br><span class="line">[lab1][DEBUG ]   &quot;outside_quorum&quot;: [], </span><br><span class="line">[lab1][DEBUG ]   &quot;quorum&quot;: [</span><br><span class="line">[lab1][DEBUG ]     0</span><br><span class="line">[lab1][DEBUG ]   ], </span><br><span class="line">[lab1][DEBUG ]   &quot;rank&quot;: 0, </span><br><span class="line">[lab1][DEBUG ]   &quot;state&quot;: &quot;leader&quot;, </span><br><span class="line">[lab1][DEBUG ]   &quot;sync_provider&quot;: []</span><br><span class="line">[lab1][DEBUG ] &#125;</span><br><span class="line">[lab1][DEBUG ] ********************************************************************************</span><br><span class="line">[lab1][INFO ] monitor: mon.lab1 is running</span><br><span class="line">[lab1][INFO ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.lab1.asok mon_status</span><br><span class="line">[ceph_deploy.mon][INFO ] processing monitor mon.lab1</span><br><span class="line">[lab1][DEBUG ] connection detected need for sudo</span><br><span class="line">[lab1][DEBUG ] connected to host: lab1 </span><br><span class="line">[lab1][DEBUG ] detect platform information from remote host</span><br><span class="line">[lab1][DEBUG ] detect machine type</span><br><span class="line">[lab1][DEBUG ] find the location of an executable</span><br><span class="line">[lab1][INFO ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.lab1.asok mon_status</span><br><span class="line">[ceph_deploy.mon][INFO ] mon.lab1 monitor has reached quorum!</span><br><span class="line">[ceph_deploy.mon][INFO ] all initial monitors are running and have formed quorum</span><br><span class="line">[ceph_deploy.mon][INFO ] Running gatherkeys...</span><br><span class="line">[ceph_deploy.gatherkeys][INFO ] Storing keys in temp directory /tmp/tmpfUkCWD</span><br><span class="line">[lab1][DEBUG ] connection detected need for sudo</span><br><span class="line">[lab1][DEBUG ] connected to host: lab1 </span><br><span class="line">[lab1][DEBUG ] detect platform information from remote host</span><br><span class="line">[lab1][DEBUG ] detect machine type</span><br><span class="line">[lab1][DEBUG ] get remote short hostname</span><br><span class="line">[lab1][DEBUG ] fetch remote file</span><br><span class="line">[lab1][INFO ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.lab1.asok mon_status</span><br><span class="line">[lab1][INFO ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-lab1/keyring auth get client.admin</span><br><span class="line">[lab1][INFO ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-lab1/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *</span><br><span class="line">[lab1][INFO ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-lab1/keyring auth get client.bootstrap-mds</span><br><span class="line">[lab1][INFO ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-lab1/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds</span><br><span class="line">[lab1][INFO ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-lab1/keyring auth get client.bootstrap-mgr</span><br><span class="line">[lab1][INFO ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-lab1/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr</span><br><span class="line">[lab1][INFO ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-lab1/keyring auth get client.bootstrap-osd</span><br><span class="line">[lab1][INFO ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-lab1/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd</span><br><span class="line">[lab1][INFO ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-lab1/keyring auth get client.bootstrap-rgw</span><br><span class="line">[lab1][INFO ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-lab1/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw</span><br><span class="line">[ceph_deploy.gatherkeys][INFO ] Storing ceph.client.admin.keyring</span><br><span class="line">[ceph_deploy.gatherkeys][INFO ] Storing ceph.bootstrap-mds.keyring</span><br><span class="line">[ceph_deploy.gatherkeys][INFO ] Storing ceph.bootstrap-mgr.keyring</span><br><span class="line">[ceph_deploy.gatherkeys][INFO ] keyring &#x27;ceph.mon.keyring&#x27; already exists</span><br><span class="line">[ceph_deploy.gatherkeys][INFO ] Storing ceph.bootstrap-osd.keyring</span><br><span class="line">[ceph_deploy.gatherkeys][INFO ] Storing ceph.bootstrap-rgw.keyring</span><br><span class="line">[ceph_deploy.gatherkeys][INFO ] Destroy temp directory /tmp/tmpfUkCWD</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>用 ceph-deploy 把配置文件和 admin 密钥拷贝到管理节点和 Ceph 节点，这样你每次执行 Ceph 命令行时就无需指定 <code>monitor</code> 地址和<code>ceph.client.admin.keyring</code>了。</p>
<p><code>ceph-deploy admin lab1 lab4 lab5</code></p>
<blockquote>
<p>注意<br>ceph-deploy 和本地管理主机（ admin-node ）通信时，必须通过主机名可达。必要时可修改 &#x2F;etc&#x2F;hosts ，加入管理主机的名字。</p>
</blockquote>
<p>确保你对 ceph.client.admin.keyring 有正确的操作权限。</p>
<p><code>sudo chmod +r /etc/ceph/ceph.client.admin.keyring</code></p>
<p>安装mrg</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy mgr create lab1</span><br><span class="line">ceph-deploy mgr create lab2</span><br><span class="line">ceph-deploy mgr create lab3</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>检查集群的健康状况。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[ceph-admin@lab1 ceph-cluster]$ ceph health</span><br><span class="line">HEALTH_OK</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<h5 id="增加OSD"><a href="#增加OSD" class="headerlink" title="增加OSD"></a>增加OSD</h5><p>列举磁盘并擦净</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">[ceph-admin@lab1 ceph-cluster]$ ceph-deploy disk list lab4</span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /home/ceph-admin/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO ] Invoked (2.0.1): /bin/ceph-deploy disk list lab4</span><br><span class="line">[ceph_deploy.cli][INFO ] ceph-deploy options:</span><br><span class="line">[ceph_deploy.cli][INFO ]  username                      : None</span><br><span class="line">[ceph_deploy.cli][INFO ]  verbose                       : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  debug                         : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  overwrite_conf                : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  subcommand                    : list</span><br><span class="line">[ceph_deploy.cli][INFO ]  quiet                         : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x20d97e8&gt;</span><br><span class="line">[ceph_deploy.cli][INFO ]  cluster                       : ceph</span><br><span class="line">[ceph_deploy.cli][INFO ]  host                          : [&#x27;lab4&#x27;]</span><br><span class="line">[ceph_deploy.cli][INFO ]  func                          : &lt;function disk at 0x20ca7d0&gt;</span><br><span class="line">[ceph_deploy.cli][INFO ]  ceph_conf                     : None</span><br><span class="line">[ceph_deploy.cli][INFO ]  default_release               : False</span><br><span class="line">[lab4][DEBUG ] connection detected need for sudo</span><br><span class="line">[lab4][DEBUG ] connected to host: lab4 </span><br><span class="line">[lab4][DEBUG ] detect platform information from remote host</span><br><span class="line">[lab4][DEBUG ] detect machine type</span><br><span class="line">[lab4][DEBUG ] find the location of an executable</span><br><span class="line">[lab4][INFO ] Running command: sudo fdisk -l</span><br><span class="line">[lab4][INFO ] Disk /dev/sda: 107.4 GB, 107374182400 bytes, 209715200 sectors</span><br><span class="line">[lab4][INFO ] Disk /dev/sdb: 214.7 GB, 214748364800 bytes, 419430400 sectors</span><br><span class="line">[lab4][INFO ] Disk /dev/mapper/cl-root: 97.8 GB, 97840529408 bytes, 191094784 sectors</span><br><span class="line">[lab4][INFO ] Disk /dev/mapper/cl-swap: 8455 MB, 8455716864 bytes, 16515072 sectors</span><br><span class="line">[lab4][INFO ] Disk /dev/mapper/vg_a66945efa6324ffeb209d165cac8ede9-tp_1f4ce4f4bfb224aa385f35516236af43_tmeta: 12 MB, 12582912 bytes, 24576 sectors</span><br><span class="line">[lab4][INFO ] Disk /dev/mapper/vg_a66945efa6324ffeb209d165cac8ede9-tp_1f4ce4f4bfb224aa385f35516236af43_tdata: 2147 MB, 2147483648 bytes, 4194304 sectors</span><br><span class="line">[lab4][INFO ] Disk /dev/mapper/vg_a66945efa6324ffeb209d165cac8ede9-tp_1f4ce4f4bfb224aa385f35516236af43-tpool: 2147 MB, 2147483648 bytes, 4194304 sectors</span><br><span class="line">[lab4][INFO ] Disk /dev/mapper/vg_a66945efa6324ffeb209d165cac8ede9-tp_1f4ce4f4bfb224aa385f35516236af43: 2147 MB, 2147483648 bytes, 4194304 sectors</span><br><span class="line">[lab4][INFO ] Disk /dev/mapper/vg_a66945efa6324ffeb209d165cac8ede9-brick_1f4ce4f4bfb224aa385f35516236af43: 2147 MB, 2147483648 bytes, 4194304 sectors</span><br><span class="line">[ceph-admin@lab1 ceph-cluster]$ ceph-deploy disk zap lab4 /dev/sdb</span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /home/ceph-admin/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO ] Invoked (2.0.1): /bin/ceph-deploy disk zap lab4 /dev/sdb</span><br><span class="line">[ceph_deploy.cli][INFO ] ceph-deploy options:</span><br><span class="line">[ceph_deploy.cli][INFO ]  username                      : None</span><br><span class="line">[ceph_deploy.cli][INFO ]  verbose                       : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  debug                         : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  overwrite_conf                : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  subcommand                    : zap</span><br><span class="line">[ceph_deploy.cli][INFO ]  quiet                         : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0xd447e8&gt;</span><br><span class="line">[ceph_deploy.cli][INFO ]  cluster                       : ceph</span><br><span class="line">[ceph_deploy.cli][INFO ]  host                          : lab4</span><br><span class="line">[ceph_deploy.cli][INFO ]  func                          : &lt;function disk at 0xd357d0&gt;</span><br><span class="line">[ceph_deploy.cli][INFO ]  ceph_conf                     : None</span><br><span class="line">[ceph_deploy.cli][INFO ]  default_release               : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  disk                          : [&#x27;/dev/sdb&#x27;]</span><br><span class="line">[ceph_deploy.osd][DEBUG ] zapping /dev/sdb on lab4</span><br><span class="line">[lab4][DEBUG ] connection detected need for sudo</span><br><span class="line">[lab4][DEBUG ] connected to host: lab4 </span><br><span class="line">[lab4][DEBUG ] detect platform information from remote host</span><br><span class="line">[lab4][DEBUG ] detect machine type</span><br><span class="line">[lab4][DEBUG ] find the location of an executable</span><br><span class="line">[ceph_deploy.osd][INFO ] Distro info: CentOS Linux 7.5.1804 Core</span><br><span class="line">[lab4][DEBUG ] zeroing last few blocks of device</span><br><span class="line">[lab4][DEBUG ] find the location of an executable</span><br><span class="line">[lab4][INFO ] Running command: sudo /usr/sbin/ceph-volume lvm zap /dev/sdb</span><br><span class="line">[lab4][DEBUG ] --&gt; Zapping: /dev/sdb</span><br><span class="line">[lab4][DEBUG ] Running command: /usr/sbin/cryptsetup status /dev/mapper/</span><br><span class="line">[lab4][DEBUG ]  stdout: /dev/mapper/ is inactive.</span><br><span class="line">[lab4][DEBUG ] Running command: wipefs --all /dev/sdb</span><br><span class="line">[lab4][DEBUG ] Running command: dd if=/dev/zero of=/dev/sdb bs=1M count=10</span><br><span class="line">[lab4][DEBUG ] --&gt; Zapping successful for: /dev/sdb</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p>同理，lab5的sdb也一样。</p>
<p>创建pv、vg、lv，略。</p>
<p>创建 OSD</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line">[ceph-admin@lab1 ceph-cluster]$ ceph-deploy osd create lab4 --fs-type btrfs --data vg1/lvol0     </span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /home/ceph-admin/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO ] Invoked (2.0.1): /bin/ceph-deploy osd create lab4 --fs-type btrfs --data vg1/lvol0</span><br><span class="line">[ceph_deploy.cli][INFO ] ceph-deploy options:</span><br><span class="line">[ceph_deploy.cli][INFO ]  verbose                       : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  bluestore                     : None</span><br><span class="line">[ceph_deploy.cli][INFO ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x26d4908&gt;</span><br><span class="line">[ceph_deploy.cli][INFO ]  cluster                       : ceph</span><br><span class="line">[ceph_deploy.cli][INFO ]  fs_type                       : btrfs</span><br><span class="line">[ceph_deploy.cli][INFO ]  block_wal                     : None</span><br><span class="line">[ceph_deploy.cli][INFO ]  default_release               : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  username                      : None</span><br><span class="line">[ceph_deploy.cli][INFO ]  journal                       : None</span><br><span class="line">[ceph_deploy.cli][INFO ]  subcommand                    : create</span><br><span class="line">[ceph_deploy.cli][INFO ]  host                          : lab4</span><br><span class="line">[ceph_deploy.cli][INFO ]  filestore                     : None</span><br><span class="line">[ceph_deploy.cli][INFO ]  func                          : &lt;function osd at 0x26c4758&gt;</span><br><span class="line">[ceph_deploy.cli][INFO ]  ceph_conf                     : None</span><br><span class="line">[ceph_deploy.cli][INFO ]  zap_disk                      : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  data                          : vg1/lvol0</span><br><span class="line">[ceph_deploy.cli][INFO ]  block_db                      : None</span><br><span class="line">[ceph_deploy.cli][INFO ]  dmcrypt                       : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  overwrite_conf                : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys</span><br><span class="line">[ceph_deploy.cli][INFO ]  quiet                         : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  debug                         : False</span><br><span class="line">[ceph_deploy.osd][DEBUG ] Creating OSD on cluster ceph with data device vg1/lvol0</span><br><span class="line">[lab4][DEBUG ] connection detected need for sudo</span><br><span class="line">[lab4][DEBUG ] connected to host: lab4 </span><br><span class="line">[lab4][DEBUG ] detect platform information from remote host</span><br><span class="line">[lab4][DEBUG ] detect machine type</span><br><span class="line">[lab4][DEBUG ] find the location of an executable</span><br><span class="line">[ceph_deploy.osd][INFO ] Distro info: CentOS Linux 7.5.1804 Core</span><br><span class="line">[ceph_deploy.osd][DEBUG ] Deploying osd to lab4</span><br><span class="line">[lab4][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</span><br><span class="line">[lab4][DEBUG ] find the location of an executable</span><br><span class="line">[lab4][INFO ] Running command: sudo /usr/sbin/ceph-volume --cluster ceph lvm create --bluestore --data vg1/lvol0</span><br><span class="line">[lab4][DEBUG ] Running command: /bin/ceph-authtool --gen-print-key</span><br><span class="line">[lab4][DEBUG ] Running command: /bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring -i - osd new 7bb2b8f4-9e9d-4cd2-a2da-802a953a4d62</span><br><span class="line">[lab4][DEBUG ] Running command: /bin/ceph-authtool --gen-print-key</span><br><span class="line">[lab4][DEBUG ] Running command: mount -t tmpfs tmpfs /var/lib/ceph/osd/ceph-1</span><br><span class="line">[lab4][DEBUG ] Running command: chown -h ceph:ceph /dev/vg1/lvol0</span><br><span class="line">[lab4][DEBUG ] Running command: chown -R ceph:ceph /dev/dm-2</span><br><span class="line">[lab4][DEBUG ] Running command: ln -s /dev/vg1/lvol0 /var/lib/ceph/osd/ceph-1/block</span><br><span class="line">[lab4][DEBUG ] Running command: ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /var/lib/ceph/osd/ceph-1/activate.monmap</span><br><span class="line">[lab4][DEBUG ]  stderr: got monmap epoch 1</span><br><span class="line">[lab4][DEBUG ] Running command: ceph-authtool /var/lib/ceph/osd/ceph-1/keyring --create-keyring --name osd.1 --add-key AQAmjHZbiiGUChAAVCWdPZqHms99mLgSZ7M+fQ==</span><br><span class="line">[lab4][DEBUG ]  stdout: creating /var/lib/ceph/osd/ceph-1/keyring</span><br><span class="line">[lab4][DEBUG ] added entity osd.1 auth auth(auid = 18446744073709551615 key=AQAmjHZbiiGUChAAVCWdPZqHms99mLgSZ7M+fQ== with 0 caps)</span><br><span class="line">[lab4][DEBUG ] Running command: chown -R ceph:ceph /var/lib/ceph/osd/ceph-1/keyring</span><br><span class="line">[lab4][DEBUG ] Running command: chown -R ceph:ceph /var/lib/ceph/osd/ceph-1/</span><br><span class="line">[lab4][DEBUG ] Running command: /bin/ceph-osd --cluster ceph --osd-objectstore bluestore --mkfs -i 1 --monmap /var/lib/ceph/osd/ceph-1/activate.monmap --keyfile - --osd-data /var/lib/ceph/osd/ceph-1/ --osd-uuid 7bb2b8f4-9e9d-4cd2-a2da-802a953a4d62 --setuser ceph --setgroup ceph</span><br><span class="line">[lab4][DEBUG ] --&gt; ceph-volume lvm prepare successful for: vg1/lvol0</span><br><span class="line">[lab4][DEBUG ] Running command: ceph-bluestore-tool --cluster=ceph prime-osd-dir --dev /dev/vg1/lvol0 --path /var/lib/ceph/osd/ceph-1</span><br><span class="line">[lab4][DEBUG ] Running command: ln -snf /dev/vg1/lvol0 /var/lib/ceph/osd/ceph-1/block</span><br><span class="line">[lab4][DEBUG ] Running command: chown -h ceph:ceph /var/lib/ceph/osd/ceph-1/block</span><br><span class="line">[lab4][DEBUG ] Running command: chown -R ceph:ceph /dev/dm-2</span><br><span class="line">[lab4][DEBUG ] Running command: chown -R ceph:ceph /var/lib/ceph/osd/ceph-1</span><br><span class="line">[lab4][DEBUG ] Running command: systemctl enable ceph-volume@lvm-1-7bb2b8f4-9e9d-4cd2-a2da-802a953a4d62</span><br><span class="line">[lab4][DEBUG ]  stderr: Created symlink from /etc/systemd/system/multi-user.target.wants/ceph-volume@lvm-1-7bb2b8f4-9e9d-4cd2-a2da-802a953a4d62.service to /usr/lib/systemd/system/ceph-volume@.service.</span><br><span class="line">[lab4][DEBUG ] Running command: systemctl start ceph-osd@1</span><br><span class="line">[lab4][DEBUG ] --&gt; ceph-volume lvm activate successful for osd ID: 1</span><br><span class="line">[lab4][DEBUG ] --&gt; ceph-volume lvm create successful for: vg1/lvol0</span><br><span class="line">[lab4][INFO ] checking OSD status...</span><br><span class="line">[lab4][DEBUG ] find the location of an executable</span><br><span class="line">[lab4][INFO ] Running command: sudo /bin/ceph --cluster=ceph osd stat --format=json</span><br><span class="line">[lab4][WARNIN] there is 1 OSD down</span><br><span class="line">[lab4][WARNIN] there is 1 OSD out</span><br><span class="line">[ceph_deploy.osd][DEBUG ] Host lab4 is now ready for osd use.</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><code>ceph-deploy osd create lab5 --fs-type btrfs --data vg1/lvol0</code></p>
<h4 id="扩展集群"><a href="#扩展集群" class="headerlink" title="扩展集群"></a>扩展集群</h4><p>一个基本的集群启动并开始运行后，下一步就是扩展集群。在 lab6、lab7 各上添加一个 OSD 守护进程和一个元数据服务器。然后分别在 lab2 和 lab3 上添加 <code>Ceph Monitor </code>，以形成 Monitors 的法定人数。</p>
<p>添加 MONITORS</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy mon add lab2</span><br><span class="line">ceph-deploy mon add lab3 </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>过程：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br></pre></td><td class="code"><pre><span class="line">[ceph-admin@lab1 ceph-cluster]$ ceph-deploy mon add lab3</span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /home/ceph-admin/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO ] Invoked (2.0.1): /bin/ceph-deploy mon add lab3</span><br><span class="line">[ceph_deploy.cli][INFO ] ceph-deploy options:</span><br><span class="line">[ceph_deploy.cli][INFO ]  username                      : None</span><br><span class="line">[ceph_deploy.cli][INFO ]  verbose                       : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  overwrite_conf                : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  subcommand                    : add</span><br><span class="line">[ceph_deploy.cli][INFO ]  quiet                         : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x29f0f38&gt;</span><br><span class="line">[ceph_deploy.cli][INFO ]  cluster                       : ceph</span><br><span class="line">[ceph_deploy.cli][INFO ]  mon                           : [&#x27;lab3&#x27;]</span><br><span class="line">[ceph_deploy.cli][INFO ]  func                          : &lt;function mon at 0x29ec2a8&gt;</span><br><span class="line">[ceph_deploy.cli][INFO ]  address                       : None</span><br><span class="line">[ceph_deploy.cli][INFO ]  ceph_conf                     : None</span><br><span class="line">[ceph_deploy.cli][INFO ]  default_release               : False</span><br><span class="line">[ceph_deploy.mon][INFO ] ensuring configuration of new mon host: lab3</span><br><span class="line">[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to lab3</span><br><span class="line">[lab3][DEBUG ] connection detected need for sudo</span><br><span class="line">[lab3][DEBUG ] connected to host: lab3 </span><br><span class="line">[lab3][DEBUG ] detect platform information from remote host</span><br><span class="line">[lab3][DEBUG ] detect machine type</span><br><span class="line">[lab3][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</span><br><span class="line">[ceph_deploy.mon][DEBUG ] Adding mon to cluster ceph, host lab3</span><br><span class="line">[ceph_deploy.mon][DEBUG ] using mon address by resolving host: 192.168.105.94</span><br><span class="line">[ceph_deploy.mon][DEBUG ] detecting platform for host lab3 ...</span><br><span class="line">[lab3][DEBUG ] connection detected need for sudo</span><br><span class="line">[lab3][DEBUG ] connected to host: lab3 </span><br><span class="line">[lab3][DEBUG ] detect platform information from remote host</span><br><span class="line">[lab3][DEBUG ] detect machine type</span><br><span class="line">[lab3][DEBUG ] find the location of an executable</span><br><span class="line">[ceph_deploy.mon][INFO ] distro info: CentOS Linux 7.5.1804 Core</span><br><span class="line">[lab3][DEBUG ] determining if provided host has same hostname in remote</span><br><span class="line">[lab3][DEBUG ] get remote short hostname</span><br><span class="line">[lab3][DEBUG ] adding mon to lab3</span><br><span class="line">[lab3][DEBUG ] get remote short hostname</span><br><span class="line">[lab3][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</span><br><span class="line">[lab3][DEBUG ] create the mon path if it does not exist</span><br><span class="line">[lab3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-lab3/done</span><br><span class="line">[lab3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-lab3/done</span><br><span class="line">[lab3][INFO ] creating keyring file: /var/lib/ceph/tmp/ceph-lab3.mon.keyring</span><br><span class="line">[lab3][DEBUG ] create the monitor keyring file</span><br><span class="line">[lab3][INFO ] Running command: sudo ceph --cluster ceph mon getmap -o /var/lib/ceph/tmp/ceph.lab3.monmap</span><br><span class="line">[lab3][WARNIN] got monmap epoch 2</span><br><span class="line">[lab3][INFO ] Running command: sudo ceph-mon --cluster ceph --mkfs -i lab3 --monmap /var/lib/ceph/tmp/ceph.lab3.monmap --keyring /var/lib/ceph/tmp/ceph-lab3.mon.keyring --setuser 167 --setgroup 167</span><br><span class="line">[lab3][INFO ] unlinking keyring file /var/lib/ceph/tmp/ceph-lab3.mon.keyring</span><br><span class="line">[lab3][DEBUG ] create a done file to avoid re-doing the mon deployment</span><br><span class="line">[lab3][DEBUG ] create the init path if it does not exist</span><br><span class="line">[lab3][INFO ] Running command: sudo systemctl enable ceph.target</span><br><span class="line">[lab3][INFO ] Running command: sudo systemctl enable ceph-mon@lab3</span><br><span class="line">[lab3][WARNIN] Created symlink from /etc/systemd/system/ceph-mon.target.wants/ceph-mon@lab3.service to /usr/lib/systemd/system/ceph-mon@.service.</span><br><span class="line">[lab3][INFO ] Running command: sudo systemctl start ceph-mon@lab3</span><br><span class="line">[lab3][INFO ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.lab3.asok mon_status</span><br><span class="line">[lab3][WARNIN] lab3 is not defined in `mon initial members`</span><br><span class="line">[lab3][WARNIN] monitor lab3 does not exist in monmap</span><br><span class="line">[lab3][INFO ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.lab3.asok mon_status</span><br><span class="line">[lab3][DEBUG ] ********************************************************************************</span><br><span class="line">[lab3][DEBUG ] status for monitor: mon.lab3</span><br><span class="line">[lab3][DEBUG ] &#123;</span><br><span class="line">[lab3][DEBUG ]   &quot;election_epoch&quot;: 0, </span><br><span class="line">[lab3][DEBUG ]   &quot;extra_probe_peers&quot;: [</span><br><span class="line">[lab3][DEBUG ]     &quot;192.168.105.93:6789/0&quot;</span><br><span class="line">[lab3][DEBUG ]   ], </span><br><span class="line">[lab3][DEBUG ]   &quot;feature_map&quot;: &#123;</span><br><span class="line">[lab3][DEBUG ]     &quot;mon&quot;: &#123;</span><br><span class="line">[lab3][DEBUG ]       &quot;group&quot;: &#123;</span><br><span class="line">[lab3][DEBUG ]         &quot;features&quot;: &quot;0x3ffddff8eea4fffb&quot;, </span><br><span class="line">[lab3][DEBUG ]         &quot;num&quot;: 1, </span><br><span class="line">[lab3][DEBUG ]         &quot;release&quot;: &quot;luminous&quot;</span><br><span class="line">[lab3][DEBUG ]       &#125;</span><br><span class="line">[lab3][DEBUG ]     &#125;</span><br><span class="line">[lab3][DEBUG ]   &#125;, </span><br><span class="line">[lab3][DEBUG ]   &quot;features&quot;: &#123;</span><br><span class="line">[lab3][DEBUG ]     &quot;quorum_con&quot;: &quot;0&quot;, </span><br><span class="line">[lab3][DEBUG ]     &quot;quorum_mon&quot;: [], </span><br><span class="line">[lab3][DEBUG ]     &quot;required_con&quot;: &quot;144115188077969408&quot;, </span><br><span class="line">[lab3][DEBUG ]     &quot;required_mon&quot;: [</span><br><span class="line">[lab3][DEBUG ]       &quot;kraken&quot;, </span><br><span class="line">[lab3][DEBUG ]       &quot;luminous&quot;</span><br><span class="line">[lab3][DEBUG ]     ]</span><br><span class="line">[lab3][DEBUG ]   &#125;, </span><br><span class="line">[lab3][DEBUG ]   &quot;monmap&quot;: &#123;</span><br><span class="line">[lab3][DEBUG ]     &quot;created&quot;: &quot;2018-08-17 16:38:21.075805&quot;, </span><br><span class="line">[lab3][DEBUG ]     &quot;epoch&quot;: 3, </span><br><span class="line">[lab3][DEBUG ]     &quot;features&quot;: &#123;</span><br><span class="line">[lab3][DEBUG ]       &quot;optional&quot;: [], </span><br><span class="line">[lab3][DEBUG ]       &quot;persistent&quot;: [</span><br><span class="line">[lab3][DEBUG ]         &quot;kraken&quot;, </span><br><span class="line">[lab3][DEBUG ]         &quot;luminous&quot;</span><br><span class="line">[lab3][DEBUG ]       ]</span><br><span class="line">[lab3][DEBUG ]     &#125;, </span><br><span class="line">[lab3][DEBUG ]     &quot;fsid&quot;: &quot;4395328d-17fc-4039-96d0-1d3241a4cafa&quot;, </span><br><span class="line">[lab3][DEBUG ]     &quot;modified&quot;: &quot;2018-08-17 17:58:23.179585&quot;, </span><br><span class="line">[lab3][DEBUG ]     &quot;mons&quot;: [</span><br><span class="line">[lab3][DEBUG ]       &#123;</span><br><span class="line">[lab3][DEBUG ]         &quot;addr&quot;: &quot;192.168.105.92:6789/0&quot;, </span><br><span class="line">[lab3][DEBUG ]         &quot;name&quot;: &quot;lab1&quot;, </span><br><span class="line">[lab3][DEBUG ]         &quot;public_addr&quot;: &quot;192.168.105.92:6789/0&quot;, </span><br><span class="line">[lab3][DEBUG ]         &quot;rank&quot;: 0</span><br><span class="line">[lab3][DEBUG ]       &#125;, </span><br><span class="line">[lab3][DEBUG ]       &#123;</span><br><span class="line">[lab3][DEBUG ]         &quot;addr&quot;: &quot;192.168.105.93:6789/0&quot;, </span><br><span class="line">[lab3][DEBUG ]         &quot;name&quot;: &quot;lab2&quot;, </span><br><span class="line">[lab3][DEBUG ]         &quot;public_addr&quot;: &quot;192.168.105.93:6789/0&quot;, </span><br><span class="line">[lab3][DEBUG ]         &quot;rank&quot;: 1</span><br><span class="line">[lab3][DEBUG ]       &#125;, </span><br><span class="line">[lab3][DEBUG ]       &#123;</span><br><span class="line">[lab3][DEBUG ]         &quot;addr&quot;: &quot;192.168.105.94:6789/0&quot;, </span><br><span class="line">[lab3][DEBUG ]         &quot;name&quot;: &quot;lab3&quot;, </span><br><span class="line">[lab3][DEBUG ]         &quot;public_addr&quot;: &quot;192.168.105.94:6789/0&quot;, </span><br><span class="line">[lab3][DEBUG ]         &quot;rank&quot;: 2</span><br><span class="line">[lab3][DEBUG ]       &#125;</span><br><span class="line">[lab3][DEBUG ]     ]</span><br><span class="line">[lab3][DEBUG ]   &#125;, </span><br><span class="line">[lab3][DEBUG ]   &quot;name&quot;: &quot;lab3&quot;, </span><br><span class="line">[lab3][DEBUG ]   &quot;outside_quorum&quot;: [</span><br><span class="line">[lab3][DEBUG ]     &quot;lab3&quot;</span><br><span class="line">[lab3][DEBUG ]   ], </span><br><span class="line">[lab3][DEBUG ]   &quot;quorum&quot;: [], </span><br><span class="line">[lab3][DEBUG ]   &quot;rank&quot;: 2, </span><br><span class="line">[lab3][DEBUG ]   &quot;state&quot;: &quot;probing&quot;, </span><br><span class="line">[lab3][DEBUG ]   &quot;sync_provider&quot;: []</span><br><span class="line">[lab3][DEBUG ] &#125;</span><br><span class="line">[lab3][DEBUG ] ********************************************************************************</span><br><span class="line">[lab3][INFO ] monitor: mon.lab3 is running</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">[ceph-admin@lab1 ceph-cluster]$ ceph quorum_status --format json-pretty</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    &quot;election_epoch&quot;: 12,</span><br><span class="line">    &quot;quorum&quot;: [</span><br><span class="line">        0,</span><br><span class="line">        1,</span><br><span class="line">        2</span><br><span class="line">    ],</span><br><span class="line">    &quot;quorum_names&quot;: [</span><br><span class="line">        &quot;lab1&quot;,</span><br><span class="line">        &quot;lab2&quot;,</span><br><span class="line">        &quot;lab3&quot;</span><br><span class="line">    ],</span><br><span class="line">    &quot;quorum_leader_name&quot;: &quot;lab1&quot;,</span><br><span class="line">    &quot;monmap&quot;: &#123;</span><br><span class="line">        &quot;epoch&quot;: 3,</span><br><span class="line">        &quot;fsid&quot;: &quot;4395328d-17fc-4039-96d0-1d3241a4cafa&quot;,</span><br><span class="line">        &quot;modified&quot;: &quot;2018-08-17 17:58:23.179585&quot;,</span><br><span class="line">        &quot;created&quot;: &quot;2018-08-17 16:38:21.075805&quot;,</span><br><span class="line">        &quot;features&quot;: &#123;</span><br><span class="line">            &quot;persistent&quot;: [</span><br><span class="line">                &quot;kraken&quot;,</span><br><span class="line">                &quot;luminous&quot;</span><br><span class="line">            ],</span><br><span class="line">            &quot;optional&quot;: []</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;mons&quot;: [</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;rank&quot;: 0,</span><br><span class="line">                &quot;name&quot;: &quot;lab1&quot;,</span><br><span class="line">                &quot;addr&quot;: &quot;192.168.105.92:6789/0&quot;,</span><br><span class="line">                &quot;public_addr&quot;: &quot;192.168.105.92:6789/0&quot;</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;rank&quot;: 1,</span><br><span class="line">                &quot;name&quot;: &quot;lab2&quot;,</span><br><span class="line">                &quot;addr&quot;: &quot;192.168.105.93:6789/0&quot;,</span><br><span class="line">                &quot;public_addr&quot;: &quot;192.168.105.93:6789/0&quot;</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;rank&quot;: 2,</span><br><span class="line">                &quot;name&quot;: &quot;lab3&quot;,</span><br><span class="line">                &quot;addr&quot;: &quot;192.168.105.94:6789/0&quot;,</span><br><span class="line">                &quot;public_addr&quot;: &quot;192.168.105.94:6789/0&quot;</span><br><span class="line">            &#125;</span><br><span class="line">        ]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><em>添加元数据服务器</em></p>
<p>至少需要一个元数据服务器才能使用 CephFS ，执行下列命令创建元数据服务器：</p>
<p><code>ceph-deploy mds create lab4</code></p>
<p>到此，可以创建RBD和cephFS的ceph集群搭建完成。</p>
<h4 id="Ceph使用技巧"><a href="#Ceph使用技巧" class="headerlink" title="Ceph使用技巧"></a>Ceph使用技巧</h4><p><em>推送配置文件</em></p>
<p>只推送配置文件</p>
<p><code>ceph-deploy --overwrite-conf config push lab1 lab2</code></p>
<p>推送配置文件和client.admin key</p>
<p><code>ceph-deploy admin lab1 lab2</code></p>
<p><em>查看状态的常用命令</em></p>
<p>集群状态</p>
<p><code>ceph -s</code></p>
<p>查看正在操作的动作</p>
<p><code>ceph -w</code></p>
<p>查看已经创建的磁盘</p>
<p><code>rbd ls -l</code></p>
<p>查看ceph集群 </p>
<p><code>ceph osd tree</code></p>
<p>查看ceph授权信息</p>
<p><code>ceph auth get client.admin</code></p>
<p>移除monitor节点</p>
<p><code>ceph-deploy mon destroy lab1</code></p>
<p>详细列出集群每块磁盘的使用情况</p>
<p><code>ceph osd df</code></p>
<p>检查 MDS 状态:</p>
<p><code>ceph mds stat</code></p>
<p><em>开启Dashbord管理界面</em></p>
<p>创建管理域密钥<br><code>ceph auth get-or-create mgr.lab1 mon &#39;allow profile mgr&#39; osd &#39;allow *&#39; mds &#39;allow *&#39;</code><br>或：<br><code>ceph auth get-key client.admin | base64</code></p>
<p><em>开启 ceph-mgr 管理域</em></p>
<p><code>ceph-mgr -i master</code></p>
<p>开启dashboard</p>
<p><code>ceph mgr module enable dashboard</code></p>
<p>绑定开启 dashboard 模块的 ceph-mgr 节点的 ip 地址</p>
<p><code>ceph config-key set mgr/dashboard/master/server_addr 192.168.105.92</code></p>
<p>dashboard 默认运行在7000端口</p>
<p><em>RBD常用命令</em></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">创建pool</span><br><span class="line">若少于5个OSD， 设置pg_num为128。</span><br><span class="line">5~10个OSD，设置pg_num为512。</span><br><span class="line">10~50个OSD，设置pg_num为4096。</span><br><span class="line"></span><br><span class="line">超过50个OSD，可以参考pgcalc计算。</span><br><span class="line">ceph osd pool create rbd 128 128 </span><br><span class="line">rbd pool init rbd</span><br><span class="line"></span><br><span class="line">删除pool</span><br><span class="line">ceph osd pool rm rbd rbd –yes-i-really-really-mean-it </span><br><span class="line">## ceph.conf 添加 </span><br><span class="line">## mon_allow_pool_delete = true</span><br><span class="line"></span><br><span class="line">手动创建一个rbd磁盘</span><br><span class="line">rbd create --image-feature layering [rbd-name] -s 10240</span><br><span class="line"></span><br></pre></td></tr></table></figure>



<p><em>OSD常用命令</em></p>
<p>清除磁盘上的逻辑卷</p>
<p><code>ceph-volume lvm zap --destroy /dev/vdc</code>   # 本机操作<br><code>ceph-deploy disk zap lab4 /dev/sdb</code>           # 远程操作</p>
<p>创建osd</p>
<p><code>ceph-deploy osd create lab4 --fs-type btrfs --data vg1/lvol0</code></p>
<p>删除osd节点的node4</p>
<p>查看节点node4上的所有osd，比如osd.9 osd.10：</p>
<p><code>ceph osd tree</code>                                                        #查看目前cluster状态</p>
<p>把node4上的所欲osd踢出集群：（node1节点上执行）</p>
<p><code>ceph osd out osd.9</code><br><code>ceph osd out osd.10</code></p>
<p>让node4上的所有osd停止工作：（node4上执行）</p>
<p><code>service ceph stop osd.9</code><br><code>service ceph stop osd.10</code></p>
<p>查看node4上osd的状态是否为down，权重为0</p>
<p><code>ceph osd tree</code></p>
<p>移除node4上的所有osd：</p>
<p><code>ceph osd crush remove osd.9</code><br><code>ceph osd crush remove osd.10</code></p>
<p>删除节点node4：</p>
<p><code>ceph osd crush remove ceph-node4</code></p>
<p>替换一个失效的磁盘驱动</p>
<p>首先<code>ceph osd tree</code> 查看down掉的osd，将因盘问题down掉的osd及相关key删除</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">ceph osd out osd.0                         # 都在node1节点下执行</span><br><span class="line">ceph osd crush rm osd.0</span><br><span class="line">ceph auth del osd.0</span><br><span class="line">ceph osd rm osd.0</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>zap新磁盘 清理新磁盘：<br><code>ceph-deploy disk zap node1 /dev/sdb</code><br>在磁盘上新建一个osd，ceph会把它添加为osd:0：<br><code>ceph-deploy --overwrite-conf osd create node1 /dev/sdb</code></p>
<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p>[1] <a target="_blank" rel="noopener" href="http://docs.ceph.com/docs/master/start/">http://docs.ceph.com/docs/master/start/</a><br>[2] <a target="_blank" rel="noopener" href="http://docs.ceph.org.cn/start/">http://docs.ceph.org.cn/start/</a><br>[3] <a target="_blank" rel="noopener" href="http://docs.ceph.org.cn/install/manual-deployment/">http://docs.ceph.org.cn/install/manual-deployment/</a><br>[4] <a target="_blank" rel="noopener" href="http://www.cnblogs.com/freedom314/p/9247602.html">http://www.cnblogs.com/freedom314/p/9247602.html</a><br>[5] <a target="_blank" rel="noopener" href="http://docs.ceph.org.cn/rados/operations/monitoring/">http://docs.ceph.org.cn/rados/operations/monitoring/</a></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/%E5%AD%A6%E4%B9%A0%E6%9D%82%E8%AE%B0/" rel="tag"># 学习杂记</a>
              <a href="/tags/Ceph/" rel="tag"># Ceph</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/passages/%E5%AD%A6%E4%B9%A0%E6%9D%82%E8%AE%B0%E4%B9%8BCeph%E5%8D%95%E8%8A%82%E7%82%B9%E9%83%A8%E7%BD%B2/" rel="prev" title="学习杂记之Ceph单节点部署">
      <i class="fa fa-chevron-left"></i> 学习杂记之Ceph单节点部署
    </a></div>
      <div class="post-nav-item">
    <a href="/passages/React%E7%9B%B8%E5%85%B3/" rel="next" title="React相关">
      React相关 <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    <div class="comments" id="valine-comments"></div>

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#CentOS7%E4%B8%8B%E5%AE%89%E8%A3%85Ceph%E4%BE%9BKubernetes%E4%BD%BF%E7%94%A8"><span class="nav-number">1.</span> <span class="nav-text">CentOS7下安装Ceph供Kubernetes使用</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%8E%AF%E5%A2%83%E8%AF%B4%E6%98%8E"><span class="nav-number">1.1.</span> <span class="nav-text">环境说明</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Ceph%E9%83%A8%E7%BD%B2%E5%87%86%E5%A4%87"><span class="nav-number">1.2.</span> <span class="nav-text">Ceph部署准备</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%8A%82%E7%82%B9%E8%A7%84%E5%88%92"><span class="nav-number">1.2.1.</span> <span class="nav-text">节点规划</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%B7%BB%E5%8A%A0yum%E6%BA%90"><span class="nav-number">1.2.2.</span> <span class="nav-text">添加yum源</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AE%89%E8%A3%85Ceph%E9%83%A8%E7%BD%B2%E5%B7%A5%E5%85%B7"><span class="nav-number">1.2.3.</span> <span class="nav-text">安装Ceph部署工具</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AE%89%E8%A3%85%E6%97%B6%E9%97%B4%E5%90%8C%E6%AD%A5%E5%B7%A5%E5%85%B7chrony"><span class="nav-number">1.2.4.</span> <span class="nav-text">安装时间同步工具chrony</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%AE%89%E8%A3%85SSH%E6%9C%8D%E5%8A%A1"><span class="nav-number">1.2.5.</span> <span class="nav-text">安装SSH服务</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E9%83%A8%E7%BD%B2-CEPH-%E7%9A%84%E7%94%A8%E6%88%B7"><span class="nav-number">1.2.6.</span> <span class="nav-text">创建部署 CEPH 的用户</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%85%81%E8%AE%B8%E6%97%A0%E5%AF%86%E7%A0%81-SSH-%E7%99%BB%E5%BD%95"><span class="nav-number">1.2.7.</span> <span class="nav-text">允许无密码 SSH 登录</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%BC%80%E6%94%BE%E6%89%80%E9%9C%80%E7%AB%AF%E5%8F%A3"><span class="nav-number">1.2.8.</span> <span class="nav-text">开放所需端口</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BB%88%E7%AB%AF%EF%BC%88-TTY-%EF%BC%89"><span class="nav-number">1.2.9.</span> <span class="nav-text">终端（ TTY ）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#SELINUX"><span class="nav-number">1.2.10.</span> <span class="nav-text">SELINUX</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%95%B4%E7%90%86%E4%BB%A5%E4%B8%8A%E6%89%80%E6%9C%89ceph%E8%8A%82%E7%82%B9%E6%93%8D%E4%BD%9C"><span class="nav-number">1.2.11.</span> <span class="nav-text">整理以上所有ceph节点操作</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%AD%98%E5%82%A8%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2"><span class="nav-number">1.3.</span> <span class="nav-text">存储集群部署</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%88%9B%E5%BB%BA%E9%9B%86%E7%BE%A4%E5%B9%B6%E5%87%86%E5%A4%87%E9%85%8D%E7%BD%AE"><span class="nav-number">1.3.1.</span> <span class="nav-text">创建集群并准备配置</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%A2%9E%E5%8A%A0OSD"><span class="nav-number">1.3.2.</span> <span class="nav-text">增加OSD</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%89%A9%E5%B1%95%E9%9B%86%E7%BE%A4"><span class="nav-number">1.4.</span> <span class="nav-text">扩展集群</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Ceph%E4%BD%BF%E7%94%A8%E6%8A%80%E5%B7%A7"><span class="nav-number">1.5.</span> <span class="nav-text">Ceph使用技巧</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E8%B5%84%E6%96%99"><span class="nav-number">1.6.</span> <span class="nav-text">参考资料</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Sheldon Lu</p>
  <div class="site-description" itemprop="description">静下心来写点东西</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">62</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">11</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">34</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/sheldon-lu" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;sheldon-lu" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 2019 – 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Sheldon Lu</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  


<script>
NexT.utils.loadComments(document.querySelector('#valine-comments'), () => {
  NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
    var GUEST = ['nick', 'mail', 'link'];
    var guest = 'nick,mail,link';
    guest = guest.split(',').filter(item => {
      return GUEST.includes(item);
    });
    new Valine({
      el         : '#valine-comments',
      verify     : false,
      notify     : false,
      appId      : 'wcykO4AM4RdvFXlzAJAOVgGz-gzGzoHsz',
      appKey     : '1okhim5n5Pq6KwHuFKmTRC9t',
      placeholder: "Just go go",
      avatar     : 'mm',
      meta       : guest,
      pageSize   : '10' || 10,
      visitor    : false,
      lang       : '' || 'zh-cn',
      path       : location.pathname,
      recordIP   : ,
      serverURLs : ''
    });
  }, window.Valine);
});
</script>

</body>
</html>
