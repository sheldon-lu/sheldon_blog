<!--崩溃欺骗-->
<script type="text/javascript" src="/js/src/crash_cheat.js"></script>
<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
















  <meta name="baidu-site-verification" content="pM3LIEUO3X">









<link rel="stylesheet" href="/sheldon_blog/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/sheldon_blog/css/main.css?v=7.0.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/sheldon_blog/images/apple-touch-icon-next.png?v=7.0.1">


  <link rel="icon" type="image/png" sizes="32x32" href="/sheldon_blog/images/favicon-32x32-next.png?v=7.0.1">


  <link rel="icon" type="image/png" sizes="16x16" href="/sheldon_blog/images/favicon-16x16-next.png?v=7.0.1">


  <link rel="mask-icon" href="/sheldon_blog/images/logo.svg?v=7.0.1" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/sheldon_blog/',
    scheme: 'Mist',
    version: '7.0.1',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="Ceph块设备块是一个字节序列（例如，一个512字节的数据块）。基于块的存储接口是最常见的存储数据的方法，它通常基于旋转介质，像硬盘、CD、软盘，甚至传统的9道磁带。   基本的块设备命令rbd命令可以让你创建、罗列、审查和删除块设备image。你也可以用它来克隆image、创建快照、回滚快照、查看快照等。关于rbd命令使用细节，可查看RBD - Manage RADOS Block Device">
<meta name="keywords" content="Ceph">
<meta property="og:type" content="article">
<meta property="og:title" content="ceph-block">
<meta property="og:url" content="https://sheldon-lu.github.io/passages/block/index.html">
<meta property="og:site_name" content="Sheldon_Lu">
<meta property="og:description" content="Ceph块设备块是一个字节序列（例如，一个512字节的数据块）。基于块的存储接口是最常见的存储数据的方法，它通常基于旋转介质，像硬盘、CD、软盘，甚至传统的9道磁带。   基本的块设备命令rbd命令可以让你创建、罗列、审查和删除块设备image。你也可以用它来克隆image、创建快照、回滚快照、查看快照等。关于rbd命令使用细节，可查看RBD - Manage RADOS Block Device">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://sheldon-lu.github.io/passages/images/snapshot_clone.png">
<meta property="og:image" content="https://sheldon-lu.github.io/passages/images/clone_a_snapshot.png">
<meta property="og:image" content="https://sheldon-lu.github.io/passages/images/Live_migration.png">
<meta property="og:image" content="https://sheldon-lu.github.io/passages/images/RBD_Persistent_Cache_00.png">
<meta property="og:image" content="https://sheldon-lu.github.io/passages/images/qemu.png">
<meta property="og:image" content="https://sheldon-lu.github.io/passages/images/Ceph_libvirt.png">
<meta property="og:image" content="https://sheldon-lu.github.io/passages/images/Ceph_OpenStack.png">
<meta property="og:image" content="https://sheldon-lu.github.io/passages/images/Ceph_iSCSI_Gateway.png">
<meta property="og:updated_time" content="2019-09-03T14:46:01.001Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="ceph-block">
<meta name="twitter:description" content="Ceph块设备块是一个字节序列（例如，一个512字节的数据块）。基于块的存储接口是最常见的存储数据的方法，它通常基于旋转介质，像硬盘、CD、软盘，甚至传统的9道磁带。   基本的块设备命令rbd命令可以让你创建、罗列、审查和删除块设备image。你也可以用它来克隆image、创建快照、回滚快照、查看快照等。关于rbd命令使用细节，可查看RBD - Manage RADOS Block Device">
<meta name="twitter:image" content="https://sheldon-lu.github.io/passages/images/snapshot_clone.png">





  
  
  <link rel="canonical" href="https://sheldon-lu.github.io/passages/block/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>ceph-block | Sheldon_Lu</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/sheldon_blog/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Sheldon_Lu</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/sheldon_blog/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    

    <a href="/sheldon_blog/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/sheldon_blog/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/sheldon_blog/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/sheldon_blog/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>Search</a>
        </li>
      
    </ul>
  

  
    

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://sheldon-lu.github.io/sheldon_blog/passages/block/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Sheldon Lu">
      <meta itemprop="description" content="静下心来写点东西">
      <meta itemprop="image" content="/sheldon_blog/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sheldon_Lu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">ceph-block

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-09-02 21:34:50" itemprop="dateCreated datePublished" datetime="2019-09-02T21:34:50+08:00">2019-09-02</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-09-03 22:46:01" itemprop="dateModified" datetime="2019-09-03T22:46:01+08:00">2019-09-03</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/sheldon_blog/categories/Ceph/" itemprop="url" rel="index"><span itemprop="name">Ceph</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">Comments: </span>
                <a href="/sheldon_blog/passages/block/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/sheldon_blog/passages/block/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/sheldon_blog/passages/block/" class="leancloud_visitors" data-flag-title="ceph-block">
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">Views: </span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">51k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">47 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h1 id="Ceph块设备"><a href="#Ceph块设备" class="headerlink" title="Ceph块设备"></a>Ceph块设备</h1><p>块是一个字节序列（例如，一个512字节的数据块）。基于块的存储接口是最常见的存储数据的方法，它通常基于旋转介质，像硬盘、CD、软盘，甚至传统的9道磁带。  </p>
<h1 id="基本的块设备命令"><a href="#基本的块设备命令" class="headerlink" title="基本的块设备命令"></a>基本的块设备命令</h1><p>rbd命令可以让你创建、罗列、审查和删除块设备image。你也可以用它来克隆image、创建快照、回滚快照、查看快照等。关于rbd命令使用细节，可查看<a href="https://docs.ceph.com/docs/master/man/8/rbd/" target="_blank" rel="noopener"><font color="red">RBD - Manage RADOS Block Device(RBD) Images</font></a>来了解详情。  </p>
<h2 id="创建块设备资源池"><a href="#创建块设备资源池" class="headerlink" title="创建块设备资源池"></a>创建块设备资源池</h2><ol>
<li>在管理节点，使用ceph工具<font color="red">创建一个资源池</font>。</li>
<li>在管理节点，使用rbd工具RBD来初始化资源池：</li>
</ol>
<blockquote>
<p>rbd pool init \<pool-name></pool-name></p>
</blockquote>
<h2 id="创建块设备用户"><a href="#创建块设备用户" class="headerlink" title="创建块设备用户"></a>创建块设备用户</h2><p>除非另有说明，否则rbd命令将使用管理员ID访问Ceph集群。此ID允许对集群进行完全的管理访问。建议尽可能使用限制更严格的用户。  </p>
<p><font color="red">创建Ceph用户</font>，可以使用Ceph命令<code>auth get-or-create</code>，需要提供用户名称、监视器和OSD：</p>
<blockquote>
<p>ceph auth get-or-create client.{ID} mon ‘profile rbd’ osd ‘profile {profile name} [pool={pool-name}] [, profile …]’  </p>
</blockquote>
<p>例如，创建一个命名称为qemu用户ID，拥有读写权限访问vms资源池，只读images资源池，执行下列命令：  </p>
<blockquote>
<p>ceph auth get-or-create client.qemu mon ‘profile rbd’ osd ‘profile rbd pool=vms,profile rbd-read-only pool=images’</p>
</blockquote>
<p>ceph auth get-or-create 命令的输出是新增用户的密钥环，可以写入到<em>/etc/ceph/cepn.client.{ID}.keyring</em>文件中。  </p>
<h2 id="创建块设备image"><a href="#创建块设备image" class="headerlink" title="创建块设备image"></a>创建块设备image</h2><p>在你将块设备挂载到节点之前，你必须先在Ceph存储集群中创建一个image。可以使用以下命令创建块设备image：  </p>
<blockquote>
<p>rbd create –size {megabytes} {pool-name}/{image-name}</p>
</blockquote>
<p>例如，在名称为<code>swimmingpool</code>的资源池中创建一个名称为<code>bar</code>的1Gimage来存储消息，执行以下命令：  </p>
<blockquote>
<p>rbd create –size 1024 swimmingpool/bar</p>
</blockquote>
<p>如果在创建image时你不指明资源池，image将被存储在默认的资源池<code>rbd</code>中。例如，在默认的资源池<code>rbd</code>中创建一个名称为<code>foo</code>的1G大小的image：  </p>
<blockquote>
<p>rbd create –size 1024 foo</p>
</blockquote>
<h2 id="罗列块设备image"><a href="#罗列块设备image" class="headerlink" title="罗列块设备image"></a>罗列块设备image</h2><p>罗列<code>rbd</code>资源池中的块设备image，可以执行以下命令：  </p>
<blockquote>
<p>rbd ls</p>
</blockquote>
<p>罗列指定资源池中的块设备image，可以执行以下命令，但需要将<code>{poolname}</code>替换为指定的资源池名称：  </p>
<blockquote>
<p>rbd ls {poolname}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd ls swimmingpool  </p>
</blockquote>
<p>罗列<code>rbd</code>资源池回收站中的待删除的块设备，执行以下命令：  </p>
<blockquote>
<p>rbd trash ls</p>
</blockquote>
<p>罗列指定资源池回收站中待删除的块设备，可以执行以下命令，但需要将<code>{poolname}</code>替换为指定的资源池名称：  </p>
<blockquote>
<p>rbd trash ls {poolname}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd trash ls swimming</p>
</blockquote>
<h2 id="检索image信息"><a href="#检索image信息" class="headerlink" title="检索image信息"></a>检索image信息</h2><p>检索指定image的信息，可以执行以下命令，但需要将<code>{image-name}</code>替换为指定的image名称：</p>
<blockquote>
<p>rbd info {image-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd info foo  </p>
</blockquote>
<p>检索指定资源池中image的信息，可以执行以下命令，但需要将<code>{image-name}</code>替换成image名称，<code>{pool-name}</code>替换成资源池名称：  </p>
<blockquote>
<p>rbd info {pool-name}/{image-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd info swimmingpool/bar</p>
</blockquote>
<h2 id="重置块设备image大小"><a href="#重置块设备image大小" class="headerlink" title="重置块设备image大小"></a>重置块设备image大小</h2><p><font color="red">Ceph块设备</font>是精简配置的。在你开始在其上保存数据之前它们不会占用任何物理存储。但是它们确实具有你使用<code>--size</code>操作指定的最大容量。如果你想增加（或者减小）Ceph块设备的最大尺寸，可以执行以下命令：  </p>
<blockquote>
<p>rbd resize –size 2048 foo (to increase)<br>rbd resize –size 2048 foo –allow-shrink (to decrease)</p>
</blockquote>
<h2 id="删除块设备image"><a href="#删除块设备image" class="headerlink" title="删除块设备image"></a>删除块设备image</h2><p>删除块设备image，可以使用以下命令，但需要将<code>{image-name}</code>替换为你想要删除的image名称：  </p>
<blockquote>
<p>rbd rm {image-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd rm foo</p>
</blockquote>
<p>删除指定资源池中的块设备，可以执行以下命令，但需要将<code>{image-name}</code>替换成你想要删除的image名称，<code>{pool-name}</code>替换成资源池名称：  </p>
<blockquote>
<p>rbd rm {pool-name}/{image-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd rm swimming/bar</p>
</blockquote>
<p>要从资源池回收站中移除块设备，可以执行以下命令，但需要将<code>{image-name}</code>替换成你想要删除的image名称，<code>{pool-name}</code>替换成资源池名称：  </p>
<blockquote>
<p>rbd trash mv {pool-name}/{image-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd trash mv swimming/bar</p>
</blockquote>
<p>要从资源池回收站中删除块设备，可以执行以下命令，但需要将<code>{image-id}</code>替换成你想要删除的image的id，<code>{pool-name}</code>替换成资源池名称：  </p>
<blockquote>
<p>rbd trash rm {pool-name}/{image-id}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd trash rm swimming/2bf4474bodc51</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Note:</span><br><span class="line">    · 你可以将image移动到回收站中，即使它拥有快照或其副本正在使用中，但是不能将它从回收站中删除。</span><br><span class="line">    · 你可以使用 --expires-at 来设置滞留时间（默认为当前时间），如果尚未到达滞留时间，除非你使用 --force，否则它不会被删除</span><br></pre></td></tr></table></figure>
<h2 id="恢复块设备image"><a href="#恢复块设备image" class="headerlink" title="恢复块设备image"></a>恢复块设备image</h2><p>恢复<code>rbd</code>资源池中待删除的块设备，可以使用以下命令，但需要将<code>{image-id}</code>替换为imageid：  </p>
<blockquote>
<p>rbd trash restore {image-id}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd trash restore 2bf4474b0dc51</p>
</blockquote>
<p>恢复指定资源池中待删除的块设备，可以使用以下命令，但需要将<code>{image-id}</code>替换为imageid，<code>{pool-name}</code>替换为资源池名称：  </p>
<blockquote>
<p>rbd trash restore {pool-name}/{image-id}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd trash restore swimming/2bf4474b0dc51  </p>
</blockquote>
<p>你也可以恢复image时使用 –image 来重命名称它：  </p>
<p>例如：  </p>
<blockquote>
<p>rbd trash restore swimming/2bf4474b0dc51 –image new-name  </p>
</blockquote>
<h1 id="块设备操作"><a href="#块设备操作" class="headerlink" title="块设备操作"></a>块设备操作</h1><h2 id="快照"><a href="#快照" class="headerlink" title="快照"></a>快照</h2><p>快照是指定时间点image状态的一种只读拷贝。Ceph块设备的一种高级操作是你可以闯将一个image的快照来保存image状态的历史记录。Ceph还支持快照分层，允许你快速轻松地克隆image（例如，VMimage）。Ceph使用rbd命令和更高级的接口，包括<font color="red">QEMU</font>，<font color="red">libvirt</font>，<font color="red">OpenStack</font>和<font color="red">CloudStack</font>，来支持快照。  </p>
<h3 id="CEPHX-笔记"><a href="#CEPHX-笔记" class="headerlink" title="CEPHX 笔记"></a>CEPHX 笔记</h3><p>当cephx启用（默认启用）时，则你必须指定一个用户名称或ID以及包含有用户对应的密钥的密钥环的路径。详情参见<a href="https://docs.ceph.com/docs/master/rados/operations/user-management/#user-management" target="_blank" rel="noopener"><font color="red">用户管理</font></a>。你也可以将CEPH_ARGS添加到环境变量中来避免重复输入以下参数：  </p>
<blockquote>
<p>rbd –id {user-ID} –keyring=/path/to/secret [commands]<br>rbd –name {username} –keyring=/path/to/secret [commands]  </p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd –id admin –keyring=/etc/ceph/ceph.keyring [commands]<br>rbd –name client.admin –keyring=/etc/ceph/ceph.keyring [commands]</p>
</blockquote>
<h3 id="快照基本操作"><a href="#快照基本操作" class="headerlink" title="快照基本操作"></a>快照基本操作</h3><p>接下来介绍如何在命令行下使用rbd命令来创建、罗列和删除快照。  </p>
<h4 id="创建快照"><a href="#创建快照" class="headerlink" title="创建快照"></a>创建快照</h4><p>使用rbd创建快照，snap create额外选项包括资源池名称和image名称。  </p>
<blockquote>
<p>rbd snap create {pool-name}/{image-name}@{snap-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd snap create rbd/foo@snamname</p>
</blockquote>
<h4 id="罗列快照"><a href="#罗列快照" class="headerlink" title="罗列快照"></a>罗列快照</h4><p>要罗列image的快照，需要额外指定资源池名称和image名称：  </p>
<blockquote>
<p>rbd snap ls {pool-name}/{image-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd snap ls rbd/foo</p>
</blockquote>
<h4 id="回滚快照"><a href="#回滚快照" class="headerlink" title="回滚快照"></a>回滚快照</h4><p>使用rbd回滚快照，snap rollback操作需要额外指明资源池名称、image名称和快照名称：  </p>
<blockquote>
<p>rbd snap rollback {pool-name}/{image-name}@{snap-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd snap rollback rbd/foo@snapname</p>
</blockquote>
<h4 id="删除快照"><a href="#删除快照" class="headerlink" title="删除快照"></a>删除快照</h4><p>使用rbd删除快照，snap rm操作需要额外指明资源池名称、image名称和快照名称：  </p>
<blockquote>
<p>rbd snap rm {pool-name}/{image-name}@{snap-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd snap rm rbd/foo@snapname</p>
</blockquote>
<h4 id="清空快照"><a href="#清空快照" class="headerlink" title="清空快照"></a>清空快照</h4><p>使用rbd删除image的所有快照，snap purge操作需要额外指明资源池名称、image名称：  </p>
<blockquote>
<p>rbd snap purge {pool-name}/{image-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd snap purge rbd/foo</p>
</blockquote>
<h3 id="分层"><a href="#分层" class="headerlink" title="分层"></a>分层</h3><p>Ceph支持创建多个块设备快照的写时复制克隆。快照分层可以使Ceph块设备客户端快速地创建image。例如，你可以创建一个写有Linux VM的块设备image；然后，快照image，保护快照，创建任意数量的写时复制克隆体。因为快照是只读的，克隆快照简化了语义，使得快速创建克隆体成为可能。  </p>
<p><img src="../images/snapshot_clone.png" alt="snapshot clone">  </p>
<p>每个克隆的image（子）都存储着父image的引用，这使得克隆的image可以打开父快照并读取它。  </p>
<p>快照的写时复制克隆行为跟其它的Ceph块设备image一样。你可以读、写、克隆和重设克隆的image的大小。克隆的image没有特殊的限制。然而，快照的写时复制克隆指的是快照，所以在你克隆快照之前你必须保护它。下面的示意图描述的就是这个过程。  </p>
<h4 id="分层入门"><a href="#分层入门" class="headerlink" title="分层入门"></a>分层入门</h4><p>Ceph块设备分层是个简单的操作。你必须有一个image，必须创建这个image的快照，必须保护这个快照。一旦已完成了这些步骤，你就可以开始克隆快照了。  </p>
<p><img src="../images/clone_a_snapshot.png" alt="clone a snapshot"></p>
<p>克隆的image保留了父快照的引用，并包含了资源池ID、imageID和快照ID。包含资源池ID意味着可以将快照从一个资源池克隆到另一个资源池中的image。  </p>
<ol>
<li><strong>Image Template</strong>：块设备分层的一个常见的使用场景是创建一个主image和一个快照，作为克隆的模板。例如，用户可能创建一个Linux发行版（例如，Ubuntu 12.04）的image，并为它创建快照。用户可能会周期性地更新image并创建新的快照（例如，<code>sudo apt-get update</code>，<code>sudo apt-get upgrade</code>，<code>sudo apt-get dist-upgrade</code>之后使用<code>rbd snap create</code>创建新的快照）。随着image的完善，用户可以克隆快照中的任何一个。</li>
</ol>
<ol start="2">
<li><strong>Extended Template</strong>：更高级的使用场景是包含扩展模板image，提供比基础image更多的信息。例如，用户可能克隆一个image（例如，VM模板）并且安装其它的软件（例如，数据库，内容管理系统，分析系统），然后快照扩展后的image，它本身可以像基础image一样被更新。  </li>
</ol>
<ol start="3">
<li><strong>Template Pool</strong>：使用块设备分层的一个方法是创建一个资源池，其中包括作为模板的主image以及那些模板的快照。然后你可以将制度权限扩展到其他用户，这样他们就可以克隆快照，但不能在资源池中写入和执行。  </li>
</ol>
<ol start="4">
<li><strong>Image Migration/Recovery</strong>：使用块设备分层的一个方法从一个资源池迁移或恢复到另一个资源池。  </li>
</ol>
<h4 id="保护快照"><a href="#保护快照" class="headerlink" title="保护快照"></a>保护快照</h4><p>克隆体可以访问父快照。如果用户不小心删除了父快照，那么所有的克隆体都将损坏。为了防止数据丢失，在你克隆快照前你<strong>必须</strong>保护它。  </p>
<blockquote>
<p>rbd snap protect {pool-name}/{image-name}@{snapshot-name}  </p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd snap protect rbd/my-image@my-snapshot  </p>
</blockquote>
<h4 id="克隆快照"><a href="#克隆快照" class="headerlink" title="克隆快照"></a>克隆快照</h4><p>要克隆快照，你需要额外说明的信息有父资源池、image和快照，以及子资源池和image名称称。在你克隆快照之前你<strong>必须</strong>先保护它。</p>
<blockquote>
<p>rbd clone {pool-name}/{parent-image}@{snap-name} {pool-name}/{child-image-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd clone rbd/my-image@my-snapshot rbd/new-image</p>
</blockquote>
<h4 id="解保快照"><a href="#解保快照" class="headerlink" title="解保快照"></a>解保快照</h4><p>在你删除快照之前，你必选先接触保护。另外，你<strong>不能</strong>删除具有克隆体引用的快照。在你删除快照之前，你<strong>必须</strong>平整该快照的所有克隆。</p>
<blockquote>
<p>rbd snap unprotect {pool-name}/{image-name}@{snapshot-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd snap unprotect rbd/my-image@my-snapshot</p>
</blockquote>
<h4 id="罗列快照的子快照"><a href="#罗列快照的子快照" class="headerlink" title="罗列快照的子快照"></a>罗列快照的子快照</h4><p>要罗列一个快照的子快照，可以执行以下命令：  </p>
<blockquote>
<p>rbd children {pool-name}/{image-name}@{snapshot-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd childre rbd/my-image@my-snapshot  </p>
</blockquote>
<h4 id="平整克隆的image"><a href="#平整克隆的image" class="headerlink" title="平整克隆的image"></a>平整克隆的image</h4><p>克隆的image保留了到父快照的引用。当你移除子克隆体到父快照的引用时，通过从快照拷贝信息到克隆体，你可以高效地“平整”image。平整image花费的时间随着快照体积的增大而增大。要想删除快照，你必须先平整它的子image。  </p>
<blockquote>
<p>rbd flatten {pool-name}/{image-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd flatten rbd/new-image  </p>
</blockquote>
<h2 id="RBD-镜像"><a href="#RBD-镜像" class="headerlink" title="RBD 镜像"></a>RBD 镜像</h2><p>RBDimage可以在两个Ceph集群间异步备份。该能力利用了RBD日志image特性来保证集群间的crash-consistent复制。镜像功能需要在同伴集群中的每一个对应的pool上进行配置，可设定自动备份某个存储池内的所有images或仅备份images的一个特定子集。用rbd命令来配置镜像功能。rbd-mirror守护进程负责从远端集群拉取image的更新，并写入本地集群的对应image中。  </p>
<p>根据复制的需要，RBD镜像可以配置为单向或者双向复制：  </p>
<ul>
<li><strong>单向复制</strong>：当数据仅从主集群镜像到从集群时，rbd-mirror守护进程只运行在从集群。</li>
<li><strong>双向复制</strong>：当数据从一个集群上的主映像镜像到另一个集群上的非主映像(反之亦然)时，rd -mirror守护进程在两个集群上运行。  </li>
</ul>
<h3 id="资源池配置"><a href="#资源池配置" class="headerlink" title="资源池配置"></a>资源池配置</h3><p>下面的程序说明如何使用rbd命令执行基本的管理任务来配置镜像功能。镜像功能需要在同伴集群中的每一个对应的pool上进行配置。  </p>
<p>资源池的配置操作应在所有的同伴集群上执行。为了清晰起见，这些过程假设可以从单个主机访问两个集群，分别称为“local”和“remote”。  </p>
<p>有关如何连接到不同Ceph集群的详细信息，请参阅<a href="https://docs.ceph.com/docs/master/man/8/rbd" target="_blank" rel="noopener"><font color="red">rbd</font></a>手册页。  </p>
<h4 id="启用镜像功能"><a href="#启用镜像功能" class="headerlink" title="启用镜像功能"></a>启用镜像功能</h4><p>使用rbd启用镜像，需要<code>mirror pool enable</code>命令，指明资源池名称和镜像模式：  </p>
<blockquote>
<p>rbd mirror pool enable {pool-name} {mode}</p>
</blockquote>
<p>镜像模式可以是pool或image：  </p>
<ul>
<li><strong>pool</strong>：当配置为pool模式时，带有日志特性的资源池中的所有image都将被镜像。</li>
<li><strong>image</strong>：当配置为image模式时，每个image的镜像功能都需要被<a href="https://docs.ceph.com/docs/master/rbd/rbd-mirroring/#enable-image-mirroring" target="_blank" rel="noopener"><font color="red">显示启用</font></a>。</li>
</ul>
<p>例如：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ rbd --cluster <span class="built_in">local</span> mirror pool <span class="built_in">enable</span> image-pool pool</span><br><span class="line">$ rbd --cluster remote mirror pool enabel image-pool pool</span><br></pre></td></tr></table></figure>
<h4 id="禁用镜像"><a href="#禁用镜像" class="headerlink" title="禁用镜像"></a>禁用镜像</h4><p>使用rbd禁用镜像，需要额外的<code>mirror pool disable</code>命令和资源池名称：  </p>
<blockquote>
<p>rbd mirror pool disable {pool-name}</p>
</blockquote>
<p>以这种方式在池上禁用镜像时，对于已明确启用镜像的任何映像（池内），也将禁用镜像。  </p>
<p>例如：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ rbd --cluster <span class="built_in">local</span> mirror pool <span class="built_in">disable</span> image-pool</span><br><span class="line">$ rbd --cluster remote mirror pool <span class="built_in">disable</span> image-pool</span><br></pre></td></tr></table></figure>
<h4 id="新增集群伙伴"><a href="#新增集群伙伴" class="headerlink" title="新增集群伙伴"></a>新增集群伙伴</h4><p>为了让<code>rbd-mirror</code>守护进程发现它的伙伴集群，伙伴集群需要被注册到资源池中。使用rbd新增镜像伙伴Ceph集群，需要额外的<code>mirror pool peer</code>添加命令、资源池名称和集群说明：  </p>
<blockquote>
<p>rbd mirror pool peer add {pool-name} {client-name}@{cluster-name}</p>
</blockquote>
<p>例如：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ rbd --cluster <span class="built_in">local</span> mirror pool peer add image-pool client.remote@remote</span><br><span class="line">$ rbd --cluster remote mirror pool peer add image-pool client.local@<span class="built_in">local</span></span><br></pre></td></tr></table></figure>
<p>默认情况下，<code>rbd-mirror</code>守护进程需要有访问位于<em>/etc/ceph/{cluster-name}.conf</em>的Ceph配置的权限，它提供了伙伴集群的监视器地址；此外还有位于默认或者配置的密钥环检索路径（例如，/etc/ceph/{cluster-name}.{client-name}.keyring）下的密钥环的访问权限。  </p>
<p>另外，伙伴集群的监视器或客户端密钥可以安全地存储在本地Ceph监视器的confi-key存储中。要在添加伙伴镜像时指定伙伴集群的连接属性，请使用<code>--remote-mon-host</code>和<code>--remote-key-file</code>选项。例如：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ rbd --cluster <span class="built_in">local</span> mirror pool peer add image-pool client.remote@remote --remote-mon-host 192.168.1.1,192.168.1.2 --remote-key-file &lt; (<span class="built_in">echo</span> <span class="string">'AQAeuZdbMMoBChAAcj++/XUxNOLFaWdtTREEsw=='</span>)</span><br><span class="line">$ rbd --cluster <span class="built_in">local</span> mirror pool info image-pool --all</span><br><span class="line">Mode: pool</span><br><span class="line">Peers:</span><br><span class="line">  UUID                                 NAME   CLIENT        MON_HOST                KEY</span><br><span class="line">  587b08db-3d33-4f32-8af8-421e77abb081 remote client.remote 192.168.1.1,192.168.1.2 AQAeuZdbMMoBChAAcj++/XUxNOLFaWdtTREEsw==</span><br></pre></td></tr></table></figure>
<h4 id="移除伙伴集群"><a href="#移除伙伴集群" class="headerlink" title="移除伙伴集群"></a>移除伙伴集群</h4><p>要使用rbd移除镜像伙伴Ceph集群，需要额外的<code>mirror pool peer remove</code>命令、资源池名称和伙伴的UUID（可从<code>rbd mirror pool info</code>命令获取）：  </p>
<blockquote>
<p>rbd mirror pool peer remove {pool-name} {peer-uuid}</p>
</blockquote>
<p>例如：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ rbd --cluster <span class="built_in">local</span> mirror pool peer remove image-pool 55672766-c02b-4729-8567-f13a66893445</span><br><span class="line">$ rbd --cluster remote mirror pool peer remove image-pool 60c0e299-b38f-4234-91f6-eed0a367be08</span><br></pre></td></tr></table></figure>
<h4 id="数据池"><a href="#数据池" class="headerlink" title="数据池"></a>数据池</h4><p>在目标集群上创建images时，<code>rbd-mirror</code>收集如下数据池：  </p>
<ol>
<li>如果目标集群有配置好的默认数据池（<code>rbd_default_data_pool</code>配置选项），那么这个数据池会被使用。</li>
<li>否则，如果源image使用单独的数据池，且目标集群上存在同名称的数据池，则使用该池。</li>
<li>如果以上都不成立，则不会设置数据池  </li>
</ol>
<h3 id="Image-配置"><a href="#Image-配置" class="headerlink" title="Image 配置"></a>Image 配置</h3><p>与资源池配置不同，image配置只需针对单个镜像的伙伴集群进行操作。  </p>
<p>镜像RBD image被指定为主image或非主image。这是image的属性而不是池的属性。被指定为非主image的image不可修改。  </p>
<p>在image上第一次启用镜像时，image会自动提升为主image（如果镜像模式为池模式，且image启用了日志image特性，则会隐式地将image提升为主image，或者通过rbd命令<a href="https://docs.ceph.com/docs/master/rbd/rbd-mirroring/#enable-image-mirroring" target="_blank" rel="noopener">显式启用</a>镜像）。  </p>
<h4 id="启用image日志支持"><a href="#启用image日志支持" class="headerlink" title="启用image日志支持"></a>启用image日志支持</h4><p>RBD镜像使用RBD日志特性来确保复制的image总是保持crash-consistent。在image可以被镜像到伙伴集群之前，日志特性必须启用。改特性可以在image创建时通过使用rbd命令，提供<code>--image-feature exclusive-lock,journaling</code>选项来启用。  </p>
<p>或者，日志特性可以在预先存在的RBD images上启用。要使用rbd启用日志，需要额外的<code>feature enable</code>命令、资源池名称、image名称和特性名称：  </p>
<blockquote>
<p>rbd feature enable {pool-name}/{image-name} {feature-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>$ rbd –cluster local feature enable image-pool/image-1 journaling</p>
</blockquote>
<h4 id="启用image镜像"><a href="#启用image镜像" class="headerlink" title="启用image镜像"></a>启用image镜像</h4><p>如果镜像是以image池的image模式配置的，则需要显式地为池中的每个image启用镜像。要使用rbd为特定的image启用镜像，需要额外的<code>mirror image enable</code>命令、资源池名称和image名称：  </p>
<blockquote>
<p>rbd mirror image enable {pool-name}/{image-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>$ rbd –cluster local mirror image enable image-pool/image-1</p>
</blockquote>
<h4 id="禁用image镜像"><a href="#禁用image镜像" class="headerlink" title="禁用image镜像"></a>禁用image镜像</h4><p>要使用rbd禁用特定的image镜像，需要额外的<code>mirror image disable</code>命令、资源池名称和image名称：  </p>
<blockquote>
<p>rbd mirror image disable {pool-name}/{image-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>$ rbd –cluster local mirror image disable image-pool/image-1</p>
</blockquote>
<h4 id="image-晋级和降级"><a href="#image-晋级和降级" class="headerlink" title="image 晋级和降级"></a>image 晋级和降级</h4><p>在故障转移场景中，需要将主节点转移到伙伴集群的image中，应该停止对主image的访问（例如，关闭VM或从VM中移除关联的设备），降级当前的主image，提升新的主image，且恢复对备用集群上image的访问。  </p>
<p>要使用rbd将指定的image降级为non-primary，需要额外的<code>mirror image demote</code>命令、资源池名称和image名称：  </p>
<blockquote>
<p>rbd mirror image demote {pool-name}/{image-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>$ rbd –cluster local mirror image demote image-pool/image-1</p>
</blockquote>
<p>要是用rbd将池中的所有主image降级为non-primary，需要额外的<code>mirror pool demote</code>命令和资源池名称：  </p>
<blockquote>
<p>rbd mirror pool demote {pool-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>$ rbd –cluster local mirror pool demote image-pool  </p>
</blockquote>
<p>要使用rbd将指定的image升级为primary，需要额外的<code>mirror image promote</code>命令、资源池名称和image名称：  </p>
<blockquote>
<p>rbd mirror image promote [–force] {pool-name}/{image-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>$ rbd –cluster remote mirror image promote image-pool/image-1  </p>
</blockquote>
<p>要是用rbd将池中的所有的non-primary image升级为primary，需要额外的<code>mirror pool promote</code>命令和资源池名称：  </p>
<blockquote>
<p>rbd mirror pool promote [–force] {pool-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>$ rbd –cluster local mirror pool promote image-pool  </p>
</blockquote>
<h4 id="强制image同步"><a href="#强制image同步" class="headerlink" title="强制image同步"></a>强制image同步</h4><p>如果rbd-mirror守护进程检测到有裂脑事件，则在更正之前它不会镜像受到影响的image。要恢复image镜像，首先要降级已过期的image，然后请求同步到主image。要使用rbd请求同步image，需要额外的<code>mirror image resync</code>命令，池名称和image名称：  </p>
<blockquote>
<p>rbd mirror image resync {pool-name}/{image-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>$ rbd mirror image resync image-pool/image-1</p>
</blockquote>
<h3 id="镜像状态"><a href="#镜像状态" class="headerlink" title="镜像状态"></a>镜像状态</h3><p>伙伴集群的副本状态存储在每个主镜像中。该状态可被<code>mirror image status</code>和<code>mirror pool status</code>命令检索到。  </p>
<p>要使用rbd命令请求镜像状态，需要额外的<code>mirror image status</code>命令，池名称和image名称：  </p>
<blockquote>
<p>rbd mirror image status {pool-name}/{image-nmae}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>$ rbd mirror image status image-pool/image-1</p>
</blockquote>
<p>要使用rbd命令请求镜像池的总体状态，需要额外的<code>mirror pool status</code>命令和池名称：  </p>
<blockquote>
<p>rbd mirror pool status {pool-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>$ rbd mirror pool status image-pool</p>
</blockquote>
<h3 id="RBD-mirror-守护进程"><a href="#RBD-mirror-守护进程" class="headerlink" title="RBD-mirror 守护进程"></a>RBD-mirror 守护进程</h3><p>两个rbd-mirror守护进程负责监听远端image日志、伙伴集群和重放本地集群的日志事件。RBD image日志特性安装出现的顺序记录下所有对image的修改。这确保了远程镜像的crash-consistent在本地可以。  </p>
<p>每个rbd-mirror守护进程应该使用唯一的Ceph用户ID。要使用ceph创建一个Ceph用户，需要额外的<code>auth get-or-create</code>命令，用户名、监视器上限和OSD上限：  </p>
<blockquote>
<p>ceph auth get-or-create client.rbd-mirror.{unique id} mon ‘profile rbd-mirror’ osd ‘profile rbd’</p>
</blockquote>
<p>通过指定用户ID作为守护进程实例，systemd可以管理rbd-mirror守护进程：  </p>
<blockquote>
<p>systemctl enable ceph-rbd-mirror@rbd-mirror.{unique id}</p>
</blockquote>
<p>rbd-mirror也可以在前台通过rbd-mirror命令启动：  </p>
<blockquote>
<p>rbd-mirror -f –log-file={log_path}  </p>
</blockquote>
<h2 id="image实时迁移"><a href="#image实时迁移" class="headerlink" title="image实时迁移"></a>image实时迁移</h2><p>RBD images可以在同一集群的不同资源池或不同image格式和布局之间实时迁移。启动时，源image会被深拷贝到目标image上，提取所有快照历史记录，并可选择保留到源image的父image的任何链接，以帮助保持稀疏性。  </p>
<p>新的目标image在使用的同时赋值操作可以安全地在后台运行。在准备迁移之前，需要暂时停止使用源image，这有助于确保该image的客户端被更新为指向新的目标image。  </p>
<p><img src="../images/Live_migration.png" alt="image live migration">  </p>
<p>实时迁移过程由三步组成：  </p>
<ol>
<li><strong>准备迁移</strong>：初始步骤创建新的目标image并交叉链接源image和目标image。与<font color="red">分层映像</font>相似，尝试读取目标image中未初始化的区域将在内部重定向到源image，而写入目标image中未初始化区域将在内部将重叠的源image块深拷贝到目标image。</li>
<li><strong>执行迁移</strong>：将所有初始化的块从源image深拷贝到目标，该操作在后台执行。此步骤可以在客户端活跃访问新目标image时执行。</li>
<li><strong>结束迁移</strong>：一旦后台迁移操作完成，就可以提交或者中断迁移。提交迁移将删除源与目标image之间的交叉链接并移除源image。中断迁移将移除交叉链接并移除目标image。  </li>
</ol>
<h3 id="准备迁移"><a href="#准备迁移" class="headerlink" title="准备迁移"></a>准备迁移</h3><p>执行<code>rbd migration prepare</code>命令初始化实时迁移操作，需要体统源与目标images：  </p>
<blockquote>
<p>$ rbd migration prepare migration_source [migration_target]  </p>
</blockquote>
<p><code>rbd migration prepare</code>命令接收与<code>rbd create</code>命令相同的布局选项,它允许更改磁盘上不可更改的image布局。如果只是更改磁盘上的布局，<em>migration_target</em>可省略，沿用原始image名称。  </p>
<p>准备进行实时迁移前，所有使用源image的客户端都必须停止使用。如果发现有任何客户端以读/写模式打开image，准备步骤将会失败。一旦准备步骤完成，客户端可以使用心得目标image名称重启。尝试使用源image重启的客户端将重启失败。  </p>
<p><code>rbd status</code>命令可以展示实时迁移的当前状态：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ rbd status migration_target</span><br><span class="line">Watchers: none</span><br><span class="line">Migration:</span><br><span class="line">            <span class="built_in">source</span>: rbd/migration_source (5e2cba2f62e)</span><br><span class="line">            destination: rbd/migration_target (5e2ed95ed806)</span><br><span class="line">            state: prepared</span><br></pre></td></tr></table></figure>
<p>注意，源image将被移动到RBD回收站中，以防迁移过程中的误操作：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ rbd info migration_source</span><br><span class="line">rbd: error opening image migration_source: (2) No such file or directory</span><br><span class="line">$ rbd trash ls --all</span><br><span class="line">5e2cba2f62e migration_source</span><br></pre></td></tr></table></figure>
<h3 id="执行迁移"><a href="#执行迁移" class="headerlink" title="执行迁移"></a>执行迁移</h3><p>准备完实时迁移之后，image块必须从源image中复制到目标image中。执行<code>rbd migration execute</code>命令可以实现上述操作：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ rbd migration execute migration_target</span><br><span class="line">Image migration: 100% complete...done.</span><br></pre></td></tr></table></figure>
<p><code>rbd status</code>命令也可提供深拷贝过程中迁移块操作的反馈：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ rbd status migration_target</span><br><span class="line">Watchers:</span><br><span class="line">    watcher=1.2.3.4:0/3695551461 client.123 cookie=123</span><br><span class="line">Migration:</span><br><span class="line">            <span class="built_in">source</span>: rbd/migration_source (5e2cba2f62e)</span><br><span class="line">            destination: rbd/migration_target (5e2ed95ed806)</span><br><span class="line">            state: executing (32% complete)</span><br></pre></td></tr></table></figure>
<h3 id="提交迁移"><a href="#提交迁移" class="headerlink" title="提交迁移"></a>提交迁移</h3><p>一旦实时迁移完成从源image到目标image所有数据块的深拷贝操作，迁移操作就可以提交：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ rbd status migration_target</span><br><span class="line">Watchers: none</span><br><span class="line">Migration:</span><br><span class="line">            <span class="built_in">source</span>: rbd/migration_source (5e2cba2f62e)</span><br><span class="line">            destination: rbd/migration_target (5e2ed95ed806)</span><br><span class="line">            state: executed</span><br><span class="line">$ rbd migration commit migration_target</span><br><span class="line">Commit image migration: 100% complete...done.</span><br></pre></td></tr></table></figure>
<p>如果<em>migration_source</em>image是一个或者多个克隆体的父节点，在确保所有的克隆体都停用后需要额外提供 <em>-force</em>选项。  </p>
<p>提交迁移将删除源与目标image之间的交叉链接并移除源image：  </p>
<blockquote>
<p>$ rbd trash list –all</p>
</blockquote>
<h3 id="中断迁移"><a href="#中断迁移" class="headerlink" title="中断迁移"></a>中断迁移</h3><p>如果你想回退准备或执行步骤，执行<code>rbd migration abort</code>命令可以回退迁移过程：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ rbd migration abort migration_target</span><br><span class="line">Abort image migration: 100% complete...done.</span><br></pre></td></tr></table></figure>
<p>中断迁移将删除目标image，并访问要恢复的原始源image：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ rbd ls</span><br><span class="line">migration_source</span><br></pre></td></tr></table></figure>
<h2 id="RBD-持久缓存"><a href="#RBD-持久缓存" class="headerlink" title="RBD 持久缓存"></a>RBD 持久缓存</h2><h3 id="共享的、只读父image缓存"><a href="#共享的、只读父image缓存" class="headerlink" title="共享的、只读父image缓存"></a>共享的、只读父image缓存</h3><p>从父image<font color="red">克隆的RBD images</font>通常只修改了image中的一小部分。例如，在VDI工作模式下，VMs是从同一基础image克隆的，仅主机名称和IP地址不同。在启动阶段，从集群里重新读取的父image其实很大部分是重复的。如果本地有父image的缓存，不仅可以加快读取速度、降低等待事件，还能减少客户机到集群的网络流量。RBD共享的只读父image缓存需要在<em>ceph.conf</em>中显式启用。ceph-immmutable-object-cache守护进程负责将父内容缓存到本地磁盘上，将来对该数据的读取将从本地缓存提供服务。  </p>
<p><img src="../images/RBD_Persistent_Cache_00.png" alt="RBD Persistent Cache">  </p>
<h4 id="启用RBD共享只读父image缓存"><a href="#启用RBD共享只读父image缓存" class="headerlink" title="启用RBD共享只读父image缓存"></a>启用RBD共享只读父image缓存</h4><p>要启用RBD共享只读父image缓存，需要在你的ceph.conf文件中的[client]部分添加如下设置：  </p>
<blockquote>
<p>rbd parent cache enabled = true  </p>
</blockquote>
<h3 id="不可变对象缓存守护进程"><a href="#不可变对象缓存守护进程" class="headerlink" title="不可变对象缓存守护进程"></a>不可变对象缓存守护进程</h3><p>ceph-immmutable-object-cache守护进程负责将父内容缓存到本地缓存目录上。为了获得更好的性能，建议使用SSD作为底层存储介质。  </p>
<p>守护进程的关键组件有：  </p>
<p>$\qquad$1. <strong>Domain socket based IPC</strong>：守护进程将在启动时监听本地域套接字，并等待来自librbd客户端的连接。<br>$\qquad$2. <strong>LRU based promotion/demotion policy</strong>：守护进程将维护每个缓存文件上缓存命中的内存统计信息。如果容量达到配置的阈值，它将降级冷缓存。<br>$\qquad$3. <strong>File-based caching store</strong>：守护进程将维护一个基于缓存存储的简单文件。在升级是，将从RADOS集群获取RADOS对象并存储到本地缓存目录中。  </p>
<p>在打开每个克隆的rbd image时，librbd会尝试通过它的域套接字来连接缓存守护进程。如果成功连接，librbd将在后续读取时自动与守护进程进行检查。如果有未缓存的读操作，守护进程将把RADOS对象提升到本地缓存目录，所以下次对该对象的读操作将从本地文件提供服务。守护进程还会维护简答的LRU统计数据，因此如果没有足够的容量，它将删除一些冷缓存文件。  </p>
<p>以下是一些与下列设置相对应的重要缓存选项:  </p>
<ul>
<li>immutable_object_cache_path 不可变对象缓存数据目录</li>
<li>immutable_object_cache_max_size 不可变缓存最大值</li>
<li>immutable_object_cache_watermark 缓存水印。如果容量达到这个水印，守护进程将会根据LRU统计数据删除冷缓存。  </li>
</ul>
<p>ceph-immutable-object-cache守护进程可在可选的ceph-immutable-object-cache分发包中使用。  </p>
<p>ceph-immutable-object-cache守护进程应该使用唯一的Ceph用户ID。要<font color="red">要创建Ceph用户</font>，使用ceph auth get-or-create命令，需要提供用户名，监视器权限，OSD权限：  </p>
<blockquote>
<p>ceph auth get-or-create client.ceph-immutable-object-cache.{unique id} mon ‘allow r’ osd ‘profile rbd-read-only’  </p>
</blockquote>
<p>作为守护进程实例，ceph-immutable-object-cache守护进程可以通过systemd使用指定的用户ID进行管理：  </p>
<blockquote>
<p>systemctl enable ceph-immutable-object-cache@immutable-object-cache.{unique id}</p>
</blockquote>
<p>ceph-immutable-object-cache守护进程也可在前台通过ceph-immutable-object-cache命令执行：  </p>
<blockquote>
<p>ceph-immutable-object-cache -f –log-file={log_path}  </p>
</blockquote>
<h2 id="librbd-配置设置"><a href="#librbd-配置设置" class="headerlink" title="librbd 配置设置"></a>librbd 配置设置</h2><p>详情参见<a href="https://docs.ceph.com/docs/master/rbd" target="_blank" rel="noopener"><font color="red">块设备</font></a>。  </p>
<h3 id="缓存设置"><a href="#缓存设置" class="headerlink" title="缓存设置"></a>缓存设置</h3><p>Ceph块设备的用户空间实现（即librbd）无法使用Linux页面缓存，因此她包含了自己的内存缓存，名为“RBD缓存”。RBD缓存行为类似于性能良好的硬盘缓存。当系统发送中断或刷新请求时，所有的数据都会被写入OSDs。这意味着回写式缓存与使用性能良好的物理硬盘一样安全，其中的VM可以正确地发送刷新（Linux内核&gt;=2.6.32）。缓存使用Least Recently Used(LRU)算法，并且在回写模式下，为了更高的吞吐量它可以合并相邻的请求。  </p>
<blockquote>
<p>内核缓存： Ceph块设备的内核驱动可以使用Linux页面缓存提升性能。  </p>
</blockquote>
<p>librbd缓存默认启用并三种不同的缓存策略：write-around，write-back和write-through。在write-around和write-back策略下写操作立即返回，除非有超过RBD缓存最大未写入字节写到存储集群中。write-around策略不同于write-back在于它不尝试为从缓存中读取请求提供服务，因此在写入工作负载下拥有更高的性能。在write-through策略下，只有在所有副本中数据都落盘时才会返回，但读操作可能来自缓存。  </p>
<p>在收到刷新请求之前，缓存的行为类似write-through缓存，以确保旧操作系统不发送刷新请求来保证crash-consistent。  </p>
<p>如果librbd缓存禁用，读写操作将直接作用于存储集群，写操作只有在所有副本中数据都落盘时才会返回。  </p>
<p>RBD的设置应被设置在你的ceph.conf配置文件中[client]部分，设置包括：  </p>
<p>rbd cache：  </p>
<p>$\qquad$ <strong>描述</strong>：启用RADOS块设备(RBD)缓存<br>$\qquad$ <strong>类型</strong>：Boolean<br>$\qquad$ <strong>必要</strong>：否<br>$\qquad$ <strong>默认值</strong>：true  </p>
<p>rbd cache policy：  </p>
<p>$\qquad$ <strong>描述</strong>：为librbd选择缓存策略<br>$\qquad$ <strong>类型</strong>：Enum<br>$\qquad$ <strong>必要</strong>：否<br>$\qquad$ <strong>默认值</strong>：writearound<br>$\qquad$ <strong>可选值</strong>：writearound，writeback，writethrough</p>
<p>rbd cache writethrough until flush：  </p>
<p>$\qquad$ <strong>描述</strong>：从write-through模式开始，在收到第一个刷新请求后切换到write-back。启用这个选项是一个保守但安全的设置，以防运行在rbd上的vm太老而无法发送刷新，就像2.6.32之前Linux中的virtio驱动程序一样。<br>$\qquad$ <strong>类型</strong>：Boolean<br>$\qquad$ <strong>必要</strong>：否<br>$\qquad$ <strong>默认值</strong>：true   </p>
<p>rbd cache size：  </p>
<p>$\qquad$ <strong>描述</strong>：RBD缓存字节数<br>$\qquad$ <strong>类型</strong>：64-bit Integer<br>$\qquad$ <strong>必要</strong>：否<br>$\qquad$ <strong>默认值</strong>：32 MiB<br>$\qquad$ <strong>策略</strong>：write-back和write-through  </p>
<p>rbd cache max dirty：  </p>
<p>$\qquad$ <strong>描述</strong>：在缓存中触发write-back的dirty字节限制，如果是0，使用write-through缓存<br>$\qquad$ <strong>类型</strong>：64-bit Integer<br>$\qquad$ <strong>必要</strong>：否<br>$\qquad$ <strong>限制</strong>：必须小于rbd cache size<br>$\qquad$ <strong>默认值</strong>：24 MiB<br>$\qquad$ <strong>策略</strong>：write-through和write-back  </p>
<p>rbd cache target dirty：  </p>
<p>$\qquad$ <strong>描述</strong>：在缓存开始向数据存储写入数据之前的target dirty。不阻塞对缓存的写入<br>$\qquad$ <strong>类型</strong>：64-bit Integer<br>$\qquad$ <strong>必要</strong>：否<br>$\qquad$ <strong>限制</strong>：必须小于rbd cache max dirty<br>$\qquad$ <strong>默认值</strong>：16 MiB<br>$\qquad$ <strong>策略</strong>：write-back  </p>
<p>rbd cache max dirty age：  </p>
<p>$\qquad$ <strong>描述</strong>：writeback开始前dirty数据在缓存中的保留秒数<br>$\qquad$ <strong>类型</strong>：Float<br>$\qquad$ <strong>必要</strong>：否<br>$\qquad$ <strong>默认值</strong>：1.0<br>$\qquad$ <strong>策略</strong>：write-back    </p>
<h3 id="预读设置"><a href="#预读设置" class="headerlink" title="预读设置"></a>预读设置</h3><p>librbd支持预读/预处理以优化小、有序的读取。在VM的情况下，这通常应该由客户机系统来处理，但引导加载器可能不会发出有效的读取操作。如果禁用缓存或缓存策略是write-around，预读操作将自动禁用。  </p>
<p>rbd readahead trigger requests  </p>
<p>$\qquad$ <strong>描述</strong>：触发预读操作的有序读请求的数量<br>$\qquad$ <strong>类型</strong>：Integer<br>$\qquad$ <strong>必要</strong>：否<br>$\qquad$ <strong>默认值</strong>：10  </p>
<p>rbd readahead max bytes  </p>
<p>$\qquad$ <strong>描述</strong>：最大预读请求数量。如果为0，禁用预读<br>$\qquad$ <strong>类型</strong>：64-bit Integer<br>$\qquad$ <strong>必要</strong>：否<br>$\qquad$ <strong>默认值</strong>：512 KiB  </p>
<p>rbd readahead disable after bytes  </p>
<p>$\qquad$ <strong>描述</strong>：当从RBD image读取到如此多字节后，该image的预读操作将被禁用直至它关闭。这允许客户机系统在启动后接管预读。如果是0，预读一直启用。<br>$\qquad$ <strong>类型</strong>：64-bit Integer<br>$\qquad$ <strong>必要</strong>：否<br>$\qquad$ <strong>默认值</strong>：50 MiB  </p>
<h3 id="image-特性"><a href="#image-特性" class="headerlink" title="image 特性"></a>image 特性</h3><p>RBD支持高级特性，可以在创建image时通过命令行指定，也可以通过Ceph配置文件通过‘rbd_default_features = \<sum of feature numeric values>’ or ‘rbd_default_features = \<comma-delimited list of cli values>’来指定。  </comma-delimited></sum></p>
<p>Layering  </p>
<p>$\qquad$ <strong>描述</strong>：分层使你可以使用克隆<br>$\qquad$ <strong>内部值</strong>：1<br>$\qquad$ <strong>CLI值</strong>：layering<br>$\qquad$ <strong>引入</strong>：v0.70(Emperor)<br>$\qquad$ <strong>KRBD支持</strong>：从v3.10<br>$\qquad$ <strong>默认</strong>：是  </p>
<p>Striping v2  </p>
<p>$\qquad$ <strong>描述</strong>：条带化将数据传播到多个对象。条带化有助于提高顺序读写工作负载的并行性。<br>$\qquad$ <strong>内部值</strong>：2<br>$\qquad$ <strong>CLI值</strong>：striping<br>$\qquad$ <strong>引入</strong>：v0.70(Emperor)<br>$\qquad$ <strong>KRBD支持</strong>：从v3.10<br>$\qquad$ <strong>默认</strong>：是  </p>
<p>Exclusive locking  </p>
<p>$\qquad$ <strong>描述</strong>：如果启用，它要求客户端在对象上执行写入前获取所。仅当单个客户端同时访问image时，才应启用独占锁。<br>$\qquad$ <strong>内部值</strong>：4<br>$\qquad$ <strong>CLI值</strong>：exclusive-lock<br>$\qquad$ <strong>引入</strong>：v0.92(Hammer)<br>$\qquad$ <strong>KRBD支持</strong>：从v4.9<br>$\qquad$ <strong>默认</strong>：是   </p>
<p>Object map  </p>
<p>$\qquad$ <strong>描述</strong>：对象映射的支持基于独占锁的支持。块设备是精简配置的，意味着，它们只存储实际存在的数据。对象映射支持帮助追踪哪些对象实际存在（已将数据存储在驱动上）。启用对象映射支持可加速用于克隆的I/O操作、导入和导出稀疏映像、删除。<br>$\qquad$ <strong>内部值</strong>：8<br>$\qquad$ <strong>CLI值</strong>：object-map<br>$\qquad$ <strong>引入</strong>：v0.92(Hammer)<br>$\qquad$ <strong>KRBD支持</strong>：从v5.3<br>$\qquad$ <strong>默认</strong>：是   </p>
<p>Fast-diff  </p>
<p>$\qquad$ <strong>描述</strong>：Fast-diff的支持依赖于对象映射的支持和独占锁的支持。它想对象映射添加了另一个属性，这使得生成image快照之间的差异和快照实际数据使用之间的差异要快很多。<br>$\qquad$ <strong>内部值</strong>：16<br>$\qquad$ <strong>CLI值</strong>：fast-diff<br>$\qquad$ <strong>引入</strong>：v9.0.1(Infernalis)<br>$\qquad$ <strong>KRBD支持</strong>：从v5.3<br>$\qquad$ <strong>默认</strong>：是   </p>
<p>Deep-flatten  </p>
<p>$\qquad$ <strong>描述</strong>：Deep-flatten使rbd flatten工作于image本身之外的所有快照。没有它，image快照仍然依赖于父级，因此父级在快照被删除之前是不可删除的。Deep-flatten使父类独立于它的克隆，即使它们有快照。<br>$\qquad$ <strong>内部值</strong>：32<br>$\qquad$ <strong>CLI值</strong>：deep-flatten<br>$\qquad$ <strong>引入</strong>：v9.0.2(Infernalis)<br>$\qquad$ <strong>KRBD支持</strong>：从v5.1<br>$\qquad$ <strong>默认</strong>：是   </p>
<p>Journaling  </p>
<p>$\qquad$ <strong>描述</strong>：日志的支持依赖于独占锁的支持。日志按照出现的顺序记录下所有对image的修改。RBD镜像利用日志将crash-consistent image复制到远程集群。<br>$\qquad$ <strong>内部值</strong>：64<br>$\qquad$ <strong>CLI值</strong>：journaling<br>$\qquad$ <strong>引入</strong>：v10.0.1(Jewel)<br>$\qquad$ <strong>KRBD支持</strong>：否<br>$\qquad$ <strong>默认</strong>：否  </p>
<p>Data Pool  </p>
<p>$\qquad$ <strong>描述</strong>：在erasure-coded池中，需要将image数据块对象存储在于image元数据分开的池中。<br>$\qquad$ <strong>内部值</strong>：128<br>$\qquad$ <strong>引入</strong>：v11.1.1(Kraken)<br>$\qquad$ <strong>KRBD支持</strong>：从v4.11<br>$\qquad$ <strong>默认</strong>：否  </p>
<p>Operations  </p>
<p>$\qquad$ <strong>描述</strong>：用于限制旧客户端对image执行某些维护操作（例如克隆、创建快照）。<br>$\qquad$ <strong>内部值</strong>：256<br>$\qquad$ <strong>引入</strong>：v13.0.2(Mimic)<br>$\qquad$ <strong>KRBD支持</strong>：从v4.16  </p>
<p>Migrating  </p>
<p>$\qquad$ <strong>描述</strong>：用于限制旧客户端在image处于迁移状态是打开image。<br>$\qquad$ <strong>内部值</strong>512<br>$\qquad$ <strong>引入</strong>：v14.0.1(Nautilus)<br>$\qquad$ <strong>KRBD支持</strong>：否  </p>
<h3 id="QOS设置"><a href="#QOS设置" class="headerlink" title="QOS设置"></a>QOS设置</h3><p>librbd支持限制每个image IO，由以下设置控制。  </p>
<p>rbd qos iops limit  </p>
<p>$\qquad$ <strong>描述</strong>：每秒IO操作的极限<br>$\qquad$ <strong>类型</strong>：Unsigned Integer<br>$\qquad$ <strong>必须</strong>：否<br>$\qquad$ <strong>默认</strong>：0   </p>
<p>rbd qos bps limit  </p>
<p>$\qquad$ <strong>描述</strong>：每秒IO字节的极限<br>$\qquad$ <strong>类型</strong>：Unsigned Integer<br>$\qquad$ <strong>必须</strong>：否<br>$\qquad$ <strong>默认</strong>：0   </p>
<p>rbd qos read iops limit  </p>
<p>$\qquad$ <strong>描述</strong>：每秒读操作的极限<br>$\qquad$ <strong>类型</strong>：Unsigned Integer<br>$\qquad$ <strong>必须</strong>：否<br>$\qquad$ <strong>默认</strong>：0   </p>
<p>rbd qos write iops limit  </p>
<p>$\qquad$ <strong>描述</strong>：每秒写操作的极限<br>$\qquad$ <strong>类型</strong>：Unsigned Integer<br>$\qquad$ <strong>必须</strong>：否<br>$\qquad$ <strong>默认</strong>：0   </p>
<p>rbd qos read bps limit  </p>
<p>$\qquad$ <strong>描述</strong>：每秒读取字节的极限<br>$\qquad$ <strong>类型</strong>：Unsigned Integer<br>$\qquad$ <strong>必须</strong>：否<br>$\qquad$ <strong>默认</strong>：0   </p>
<p>rbd qos write bps limit  </p>
<p>$\qquad$ <strong>描述</strong>：每秒写入字节的极限<br>$\qquad$ <strong>类型</strong>：Unsigned Integer<br>$\qquad$ <strong>必须</strong>：否<br>$\qquad$ <strong>默认</strong>：0   </p>
<p>rbd qos iops burst  </p>
<p>$\qquad$ <strong>描述</strong>：IO操作的爆炸极限<br>$\qquad$ <strong>类型</strong>：Unsigned Integer<br>$\qquad$ <strong>必须</strong>：否<br>$\qquad$ <strong>默认</strong>：0   </p>
<p>rbd qos bps burst  </p>
<p>$\qquad$ <strong>描述</strong>：IO字节的爆炸极限<br>$\qquad$ <strong>类型</strong>：Unsigned Integer<br>$\qquad$ <strong>必须</strong>：否<br>$\qquad$ <strong>默认</strong>：0   </p>
<p>rbd qos read iops burst  </p>
<p>$\qquad$ <strong>描述</strong>：读操作的爆炸极限<br>$\qquad$ <strong>类型</strong>：Unsigned Integer<br>$\qquad$ <strong>必须</strong>：否<br>$\qquad$ <strong>默认</strong>：0  </p>
<p>rbd qos write iops burst  </p>
<p>$\qquad$ <strong>描述</strong>：写操作的爆炸极限<br>$\qquad$ <strong>类型</strong>：Unsigned Integer<br>$\qquad$ <strong>必须</strong>：否<br>$\qquad$ <strong>默认</strong>：0   </p>
<p>rbd qos read bps burst  </p>
<p>$\qquad$ <strong>描述</strong>：读字节的爆炸极限<br>$\qquad$ <strong>类型</strong>：Unsigned Integer<br>$\qquad$ <strong>必须</strong>：否<br>$\qquad$ <strong>默认</strong>：0   </p>
<p>rbd qos write bps burst  </p>
<p>$\qquad$ <strong>描述</strong>：写字节的爆炸极限<br>$\qquad$ <strong>类型</strong>：Unsigned Integer<br>$\qquad$ <strong>必须</strong>：否<br>$\qquad$ <strong>默认</strong>：0   </p>
<p>rbd qos schedule tick min  </p>
<p>$\qquad$ <strong>描述</strong>：QOS的最小时间调度（以毫秒为单位）<br>$\qquad$ <strong>类型</strong>：Unsigned Integer<br>$\qquad$ <strong>必须</strong>：否<br>$\qquad$ <strong>默认</strong>：50  </p>
<h2 id="RBD重播"><a href="#RBD重播" class="headerlink" title="RBD重播"></a>RBD重播</h2><p>RBD replay是一系列捕获和重播RBD工作负载的工具。要捕获RBD工作负载，客户端必须安装lttng-tools，且librbd必须是v0.87(Giant)或之后的版本。要重播RBD工作负载，客户端上的librbd必须是v0.87(Giant)或之后的版本。  </p>
<p>捕获和重播需要三步：  </p>
<ol>
<li>捕获记录。确保捕获pthread_id上下文：  </li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p traces</span><br><span class="line">lttng create -o traces librbd</span><br><span class="line">lttng <span class="built_in">enable</span>-event -u <span class="string">'librbd:*'</span></span><br><span class="line">lttng add-context -u -t pthread_id</span><br><span class="line">lttng start</span><br><span class="line"><span class="comment"># run RBD workload here</span></span><br><span class="line">lttng stop</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>使用<font color="red">rbd-replay-prep</font>处理记录：  </li>
</ol>
<blockquote>
<p>rbd-replay-prep traces/ust/uid/<em>/</em> replay.bin  </p>
</blockquote>
<ol start="3">
<li>使用<font color="red">rbd-replay</font>重播记录。使用只读，直到你知道它在做你想在的：  </li>
</ol>
<blockquote>
<p>rbd-replay –read-only replay.bin  </p>
</blockquote>
<p>重新播放的工作负载不必针对相同的RBD映像，甚至不必针对与捕获的工作负载相同的集群。为了说明差异，您可能需要使用rbd-replay的-pool和-map-image选项。  </p>
<h1 id="Ceph块设备第三方集成"><a href="#Ceph块设备第三方集成" class="headerlink" title="Ceph块设备第三方集成"></a>Ceph块设备第三方集成</h1><h2 id="内核模块操作"><a href="#内核模块操作" class="headerlink" title="内核模块操作"></a>内核模块操作</h2><h3 id="获取image列表"><a href="#获取image列表" class="headerlink" title="获取image列表"></a>获取image列表</h3><p>要挂载块设备image，首先需要获取image列表：  </p>
<blockquote>
<p>rbd list</p>
</blockquote>
<h3 id="映射块设备"><a href="#映射块设备" class="headerlink" title="映射块设备"></a>映射块设备</h3><p>使用rbd映射image名称到内核模块。你必须指定image名称，资源池名称和用户名。如果尚未加载rbd内核模块，rbd将为你加载它：  </p>
<blockquote>
<p>sudo rbd device map {pool-name}/{image-name} –id {user-name}  </p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>sudo rbd device map rbd/myimage –id admin  </p>
</blockquote>
<p>如果你使用<font color="red">cephx</font>认证，你必须额外指明密码。它可能来自密钥环或包含密码的文件：  </p>
<blockquote>
<p>sudo rbd device map rbd/myimage –id admin –keyring /path/to/keyring<br>sudo rbd device map rbd/myimage –id admin –keyfile /path/to/file  </p>
</blockquote>
<h3 id="展示已映射的块设备"><a href="#展示已映射的块设备" class="headerlink" title="展示已映射的块设备"></a>展示已映射的块设备</h3><p>要使用rbd展示映射到内核模块的块设备image，指定参数<code>device list</code>：  </p>
<blockquote>
<p>rbd device list  </p>
</blockquote>
<h3 id="取消映射"><a href="#取消映射" class="headerlink" title="取消映射"></a>取消映射</h3><p>要使用rbd命令取消块设备image映射，需要指定<code>device unmap</code>参数和设备名称（例如，按照惯例，有块设备image同名）：  </p>
<blockquote>
<p>sudo rbd device unmap /dev/rbd/{poolname}/{imagename}  </p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>sudo rbd device unmap /dev/rbd/rbd/foo</p>
</blockquote>
<h2 id="QEMU-与块设备"><a href="#QEMU-与块设备" class="headerlink" title="QEMU 与块设备"></a>QEMU 与块设备</h2><p>最常见的Ceph块设备使用场景是为虚拟机提供块设备images。例如，用户可以在理想的配置中使用OS和相关软件创建一个“黄金”image。然后用户创建一个这个image的快照。最后，用户克隆这个快照（通常是很多次）。详见<a href="https://docs.ceph.com/docs/master/rbd/rbd-snapshot/" target="_blank" rel="noopener"><font color="red">Snapshots</font></a>。对快照进行写时复制的能力意味着Ceph可以快速地将块设备image提供给虚拟机，因为客户端不必在每次启用一个新的虚拟机时下载整个image。  </p>
<p><img src="../images/qemu.png" alt="QEMU and Ceph Block">  </p>
<p>Ceph块设备可以跟QEMU虚拟机融为一体。关于QEMU详情，请查看<a href="http://wiki.qemu.org/Main_Page" target="_blank" rel="noopener"><font color="red">QEMU Open Source Processor Emulator</font></a>。QEMU文件参见<a href="http://wiki.qemu.org/Manual" target="_blank" rel="noopener"><font color="red">文档</font></a>，安装细节，参见<a href="https://docs.ceph.com/docs/master/install" target="_blank" rel="noopener"><font color="red">安装</font></a>。  </p>
<h3 id="使用指南"><a href="#使用指南" class="headerlink" title="使用指南"></a>使用指南</h3><p>QEMU命令行期望你指定资源池名称和image名称。你也可以指明快照名称。  </p>
<p>QEMU假设Ceph配置文件存放在默认的路径(即/etc/ceph/$cluster.conf)，且你可以使用默认的client.admin用户执行命令，除非你指明其它的Ceph配置文件路径或用户。当指定用户是，QEMU只使用ID而不是全部的TYPE:ID。详见<a href="https://docs.ceph.com/docs/master/rados/operations/user-management#user" target="_blank" rel="noopener"><font color="red">用户管理-用户</font></a>。在用户ID之前不用使用client类型的前缀（例如，client.），否则你会收到认证错误。使用<code>:id={user}</code>选项时你应该持有管理员用户或其他用户的密钥，密钥文件应该存储在默认路径（即，/etc/ceph）或具有合适的文件所有权和权限的本地路径。用法格式如下：  </p>
<blockquote>
<p>qemu-img {command} [options] rbd:{pool-name}/{image-name}[@snapshot-name][:option1=value1][:option2=value2…]  </p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>qemu-img {command} [options] rbd:glance-pool/maipo:id=glance:conf=/etc/ceph/ceph.conf</p>
</blockquote>
<h3 id="使用QEMU创建images"><a href="#使用QEMU创建images" class="headerlink" title="使用QEMU创建images"></a>使用QEMU创建images</h3><p>你可以从QEMU中创建块设备image。你必须指明rbd，资源池名称以及你想要创建的image名称，同时你还必须指明image大小。  </p>
<blockquote>
<p>qemu-img create -f raw rbd:{pool-name}/{image-name} {size}  </p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>qemu-img create -f raw rbd:data/foo 10G</p>
</blockquote>
<h3 id="使用QEMU重置image大小"><a href="#使用QEMU重置image大小" class="headerlink" title="使用QEMU重置image大小"></a>使用QEMU重置image大小</h3><p>你可以从QEMU中重置块设备image的大小。你必须指明rbd，资源池名称以及你想重置大小的image的名称。你还必须指明image的大小。  </p>
<blockquote>
<p>qemu-img resize rbd:{pool-name}/{image-name} {size}  </p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>qemu-img resize rbd:data/foo 10G  </p>
</blockquote>
<h3 id="使用QEMU检索image信息"><a href="#使用QEMU检索image信息" class="headerlink" title="使用QEMU检索image信息"></a>使用QEMU检索image信息</h3><p>你可以从QEMU中检索块设备image的信息。你必须指明rbd，资源池名称以及image名称。  </p>
<blockquote>
<p>qemu-img info rbd:{pool-name}/{image-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>qemu-img info rbd:data/foo  </p>
</blockquote>
<h3 id="使用rbd运行QEMU"><a href="#使用rbd运行QEMU" class="headerlink" title="使用rbd运行QEMU"></a>使用rbd运行QEMU</h3><p>QEMU可以将块设备从主机上传递给客户机，但从QEMU 0.15起，不再需要将image映射为主机上的块设备了。取而代之的是，QEMU可以通过librbd直接访问作为虚拟块设备的image。这样做更高效，因为这样避免了上下文切换的额外开销，同时也能利用<font color="red">RBD缓存</font>。  </p>
<p>你可使用<code>qemu-img</code>将已存在的虚拟机image转换成Ceph的块设备image。例如，加入你有一个qcow2的镜像，你可以执行：  </p>
<blockquote>
<p>qemu-img convert -f qcow2 -O raw debian_squeeze.qcow2 rbd:data/squeeze</p>
</blockquote>
<p>要从这个镜像启动虚拟机，你可以执行：  </p>
<blockquote>
<p>qemu -m 1024 -drive format=raw,file=rbd:data/squeeze  </p>
</blockquote>
<p><font color="red">RBD缓存</font>可以显著地提升性能。从QEMU 1.2起，QEMU的缓存选项可以控制librbd缓存：  </p>
<blockquote>
<p>qemu -m 1024 -drive format=rbd,file=rbd:data/squeeze,cache=writeback  </p>
</blockquote>
<p>如果你使用的是旧版的QEMU，你可以将librbd缓存配置（跟任何Ceph配置选项一样）设置为“文件”参数的一部分：  </p>
<blockquote>
<p>qemu -m 1024 -drive format=raw,file=rbd:data/squeeze:rbd_cache=true,cache=writeback  </p>
</blockquote>
<h3 id="启用-Discard-Trim"><a href="#启用-Discard-Trim" class="headerlink" title="启用 Discard/Trim"></a>启用 Discard/Trim</h3><p>从Ceph 0.46版本和QEMU 1.1版本开始，Ceph块设备支持丢弃操作。这意味着客户机可以发送裁剪（Trim）请求来让Ceph块设备回收未使用的空间。在客户机使用discard选项挂载ext4或XFS可以启用该功能。  </p>
<p>为了在客户机上可用，块设备必须显式启用该功能。要做到这些，你必须指明<code>discard_granularity</code>与设备关联：  </p>
<blockquote>
<p>qemu -m 1024 -drive format=raw,file=rbd:data/squeeze,id=drive1,if=none -device driver=ide-hd,drive=drive1,discard_granularity=512  </p>
</blockquote>
<p>注意，这里使用的是IDE设备。virtio设备不支持丢弃。  </p>
<p>如果使用libvirt，使用virsh编辑你的libvirt守护进程的配置文件，添加<code>xmlns:qemu</code>。然后，添加<code>qemu:commandline</code>块作为守护进程的子进程。下面的示例说明如何将emu id=的两个设备设置为不同的discard_particle值。  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;domain type=&apos;kvm&apos; xmlns:qemu=&apos;http://libvirt.org/schemas/domain/qemu/1.0&apos;&gt;</span><br><span class="line">        &lt;qemu:commandline&gt;</span><br><span class="line">                &lt;qemu:arg value=&apos;-set&apos;/&gt;</span><br><span class="line">                &lt;qemu:arg value=&apos;block.scsi0-0-0.discard_granularity=4096&apos;/&gt;</span><br><span class="line">                &lt;qemu:arg value=&apos;-set&apos;/&gt;</span><br><span class="line">                &lt;qemu:arg value=&apos;block.scsi0-0-1.discard_granularity=65536&apos;/&gt;</span><br><span class="line">        &lt;/qemu:commandline&gt;</span><br><span class="line">&lt;/domain&gt;</span><br></pre></td></tr></table></figure>
<h3 id="QEMU缓存选项"><a href="#QEMU缓存选项" class="headerlink" title="QEMU缓存选项"></a>QEMU缓存选项</h3><p>QEMU的缓存选项与下面的Ceph<font color="red">RBD缓存</font>设置相符合。  </p>
<p>Writeback：  </p>
<blockquote>
<p>rbd_cache = true</p>
</blockquote>
<p>Writethrough：  </p>
<blockquote>
<p>rbd_cache = true<br>rbd_cache_max_dirty = 0  </p>
</blockquote>
<p>None:  </p>
<blockquote>
<p>rbd_cache = false  </p>
</blockquote>
<p>QEMU的缓存设置会覆盖Ceph的缓存设置（包括在Ceph配置文件中明确设置的配置）。    </p>
<h2 id="libvirt与Ceph-RBD"><a href="#libvirt与Ceph-RBD" class="headerlink" title="libvirt与Ceph RBD"></a>libvirt与Ceph RBD</h2><p>libvirt库在管理程序接口和使用它们的软件应用程序之间创建了一个虚拟机抽象层。使用libvirt，在许多不同的管理程序间，开发者和系统管理员可以关注于通用管理框架、通用API和通用shell接口（即，virsh），包括：  </p>
<ul>
<li>QEMU/KVM</li>
<li>XEN</li>
<li>LXC</li>
<li>VirtualBox</li>
<li>等等  </li>
</ul>
<p>Ceph块设备支持QEMU/KVM。你可以通过使用libvirt接口的软件来使用Ceph块设备。下图描述了libvirt和QEMU如何通过librbd来使用Ceph块设备。  </p>
<p><img src="../images/Ceph_libvirt.png" alt="Ceph with libvirt">  </p>
<p>最常见的需要libvirt提供Ceph块设备的使用场景是云解决方案，例如OpenStack和CloudStack。云解决方案使用libvirt与QEMU/KVM进行交互，QEMU/KVM通过librbd与Ceph块设备进行交互。详见<a href="https://docs.ceph.com/docs/master/rbd/rbd-openstack" target="_blank" rel="noopener"><font color="red">块设备与OpenStack</font></a>和<a href="https://docs.ceph.com/docs/master/rbd/rbd-cloudstack" target="_blank" rel="noopener"><font color="red">块设备与CloudStack</font></a>。安装详见<a href="https://docs.ceph.com/docs/master/install" target="_blank" rel="noopener"><font color="red">安装</font></a>。  </p>
<p>你也可以通过Libvirt、virsh和libvirt API来使用Ceph块设备。详见<a href="http://www.libvirt.org/" target="_blank" rel="noopener"><font color="red">libvirt虚拟化API</font></a>。  </p>
<p>要使用Ceph块设备创建VMs，可使用以下章节中的操作。在接下来的示例中，我们使用libvirt-pool作为资源池名称，client.libvirt作为用户名，new-libvirt-image作为image名称。当然你可以使用你喜欢的名称，但请确保在随后的操作中执行命令时替换这些名称。  </p>
<h3 id="配置Ceph"><a href="#配置Ceph" class="headerlink" title="配置Ceph"></a>配置Ceph</h3><p>要配置Ceph以便libvirt使用，执行下列步骤：  </p>
<ol>
<li><a href="https://docs.ceph.com/docs/master/rados/operations/pools#create-a-pool" target="_blank" rel="noopener"><font color="red">创建资源池</font></a>。下面的示例是创建一个拥有128个归置组的名为libvirt-pool的资源池：  </li>
</ol>
<blockquote>
<p>ceph osd pool create libvirt-pool 128 128</p>
</blockquote>
<p>确认资源池已存在：  </p>
<blockquote>
<p>ceph osd lspools</p>
</blockquote>
<ol start="2">
<li>使用rbd工具RBD初始化资源池：  </li>
</ol>
<blockquote>
<p>rbd pool init \<pool-name></pool-name></p>
</blockquote>
<ol start="3">
<li><a href="https://docs.ceph.com/docs/master/rados/operations/user-management#add-a-user" target="_blank" rel="noopener"><font color="red">创建Ceph用户</font></a>。下面的示例是使用名为client.libvirt的用户和引用libvirt-pool：  </li>
</ol>
<blockquote>
<p>ceph auth get-or-create client.libvirt mon ‘profile rbd’ osd ‘profile rbd pool=libvirt-pool’</p>
</blockquote>
<p>确认用户已存在：  </p>
<blockquote>
<p>ceph auth ls</p>
</blockquote>
<ol start="4">
<li>使用QEMU在你的RBD池中<a href="https://docs.ceph.com/docs/master/rbd/qemu-rbd#creating-images-with-qemu" target="_blank" rel="noopener"><font color="red">创建image</font></a>。下面吗的示例是引用libvirt-pool创建名为new-libvirt-image的image：  </li>
</ol>
<blockquote>
<p>qemu-img create -f rbd rbd:libvirt-pool/new-libvirt-image 2G</p>
</blockquote>
<p>确认image以存在：  </p>
<blockquote>
<p>rbd -p libvirt-pool ls</p>
</blockquote>
<blockquote>
<p>你也可以使用[<font color="red">rbd create</font>]来创建image，但我们建议确保QEMU可以正常工作。  </p>
</blockquote>
<h3 id="准备VM管理员"><a href="#准备VM管理员" class="headerlink" title="准备VM管理员"></a>准备VM管理员</h3><p>你可能在没有VM管理员的情况下使用libvirt，但你会发现使用virt-manager来创建你的第一域更简单。  </p>
<ol>
<li>安装虚拟机管理员。详见[<font red="red">KVM/VirtManager</font>]。  </li>
</ol>
<blockquote>
<p>sudo apt-get install virt-manager</p>
</blockquote>
<ol start="2">
<li>如果需要，下载系统镜像。</li>
<li>启用虚拟机管理：  </li>
</ol>
<blockquote>
<p>sudo virt-manager</p>
</blockquote>
<h3 id="创建一个VM"><a href="#创建一个VM" class="headerlink" title="创建一个VM"></a>创建一个VM</h3><p>要使用virt-manager创建一个VM，执行以下步骤：  </p>
<ol>
<li>按下<strong>Create New Virtual Machine</strong>按钮</li>
<li>命名新的虚拟机域。在接下来的示例中，我们使用libvirt-virtual-machine名称。你也可以使用你想要的名称，但要确保在接下的命令行和配置示例中替换你选则的名称。  </li>
</ol>
<blockquote>
<p>libvirt-virtual-machine</p>
</blockquote>
<ol start="3">
<li>导入镜像</li>
</ol>
<blockquote>
<p>/path/to/image/recent-linux.img</p>
</blockquote>
<blockquote>
<p>注意，导入最近的镜像。某些老的镜像可能无法重新扫描为正确的虚拟设备。</p>
</blockquote>
<ol start="4">
<li>配置从启动VM</li>
<li>你可以使用<code>virsh list</code>来确认VM域已存在</li>
</ol>
<blockquote>
<p>sudo virsh list</p>
</blockquote>
<ol start="6">
<li>登录VM(root/root)</li>
<li>配置VM使用Ceph前需要先关闭</li>
</ol>
<h3 id="配置VM"><a href="#配置VM" class="headerlink" title="配置VM"></a>配置VM</h3><p>在为Ceph配置VM时，在适当的地方使用virsh是很重要的。此外，virsh命令通常需要root权限（例如，sudo）且不会返回适当的结果，也不会通知你需要root权限。关于virsh命令的引用，详见<a href="http://www.libvirt.org/virshcmdref.html" target="_blank" rel="noopener"><font color="red">Virsh Command Reference</font></a>。  </p>
<ol>
<li>使用virsh edit打开配置文件。  </li>
</ol>
<blockquote>
<p>sudo virsh edit {vm-domain-name}</p>
</blockquote>
<p>\<devices>下应该有\<disk>条目。  </disk></devices></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;devices&gt;</span><br><span class="line">        &lt;emulator&gt;/usr/bin/kvm&lt;/emulator&gt;</span><br><span class="line">        &lt;disk type=&apos;file&apos; device=&apos;disk&apos;&gt;</span><br><span class="line">                &lt;driver name=&apos;qemu&apos; type=&apos;raw&apos;/&gt;</span><br><span class="line">                &lt;source file=&apos;/path/to/image/recent-linux.img&apos;/&gt;</span><br><span class="line">                &lt;target dev=&apos;vda&apos; bus=&apos;virtio&apos;/&gt;</span><br><span class="line">                &lt;address type=&apos;drive&apos; controller=&apos;0&apos; bus=&apos;0&apos; unit=&apos;0&apos;/&gt;</span><br><span class="line">        &lt;/disk&gt;</span><br></pre></td></tr></table></figure>
<p>使用OS镜像的路径替换<code>/path/to/image/recent-linux.img</code>。使用更快的virtio的最小内核版本为2.6.25。详见<a href="http://www.linux-kvm.org/page/Virtio" target="_blank" rel="noopener"><font color="red">Virtio</font></a>。  </p>
<blockquote>
<p><strong>重要</strong>：使用<code>sudo virsh edit</code>而不是文本编辑器。如果你使用文本编辑器编辑<code>/etc/libvirt/qemu</code>目录下的配置文件，libvirt可能不会感知到改变。如果<code>/etc/libvirt/qemu</code>目录下的XML文件内容与<code>sudo virsh dumpxml {vm-domain-name}</code>的返回结果有差异，你的VM可能不会正常工作。  </p>
</blockquote>
<ol start="2">
<li>将你创建的Ceph RBD image作为\<disk>条目添加  </disk></li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;disk type=&apos;network&apos; device=&apos;disk&apos;&gt;</span><br><span class="line">        &lt;source protocol=&apos;rbd&apos; name=&apos;libvirt-pool/new-libvirt-image&apos;&gt;</span><br><span class="line">                &lt;host name=&apos;&#123;monitor-host&#125;&apos; port=&apos;6789&apos;/&gt;</span><br><span class="line">        &lt;/source&gt;</span><br><span class="line">        &lt;target dev=&apos;vda&apos; bus=&apos;virtio&apos;/&gt;</span><br><span class="line">&lt;/disk&gt;</span><br></pre></td></tr></table></figure>
<p>使用你的主机名替换{monitor-host}，替换资源池或image名称也是必要。你也可以为你的Ceph监视器添加多个\<host>条目。逻辑设备的属性可在你的VM的<code>/dev</code>目录下找到。bus属性选项指示要模拟的硬盘设备的类型。驱动的有效设置是明确的（即，“ide”，“scsi”，“virtio”，“xen”，“usb”或“sata”）。  </host></p>
<p>\<disk>元素及其子元素和属性，详见<a href="http://www.libvirt.org/formatdomain.html#elementsDisks" target="_blank" rel="noopener"><font color="red">Disks</font></a>。  </disk></p>
<ol start="3">
<li>保存文件</li>
<li>如果你的Ceph存储集群已启用<a href="https://docs.ceph.com/docs/master/rados/configuration/auth-config-ref" target="_blank" rel="noopener"><font color="red">Ceph 认证</font></a>（默认启用），你必须生成密码  </li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; secret.xml &lt;&lt;EOF</span><br><span class="line">&lt;secret ephemeral=<span class="string">'no'</span> private=<span class="string">'no'</span>&gt;</span><br><span class="line">        &lt;usage <span class="built_in">type</span>=<span class="string">'ceph'</span>&gt;</span><br><span class="line">                &lt;name&gt;client.libvirt secret&lt;/name&gt;</span><br><span class="line">        &lt;/usage&gt;</span><br><span class="line">&lt;/secret&gt;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<ol start="5">
<li>定义密码  </li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo virsh secret-define --file secret.xml</span><br><span class="line">&lt;uuid of secret is output here&gt;</span><br></pre></td></tr></table></figure>
<ol start="6">
<li>获取并保存client.libvirt密钥到文件中  </li>
</ol>
<blockquote>
<p>ceph auth get-key client.libvirt | sudo tee client.libvirt.key</p>
</blockquote>
<ol start="7">
<li>设置密码的UUID  </li>
</ol>
<blockquote>
<p>sudo virsh secret-set-value –secret {uuid of secret} –base64 $(cat client.libvirt.key) &amp;&amp; rm client.libvirt.key secret.xml</p>
</blockquote>
<p>你必须通过手动添加下面的\<auth>条目到你之前创建的\<disk>元素的方式来设置密码（UUID的值用从上述命令得到的结果替换）。  </disk></auth></p>
<blockquote>
<p>sudo virsh edit {vm-domain-name}  </p>
</blockquote>
<p>然后添加\<auth>\</auth>元素到域配置文件中：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">&lt;/source&gt;</span><br><span class="line">&lt;auth username=&apos;libvirt&apos;&gt;</span><br><span class="line">        &lt;secret type=&apos;ceph&apos; uuid=&apos;9ec59067-fdbc-a6c0-03ff-df165c0587b8&apos;/&gt;</span><br><span class="line">&lt;/auth&gt;</span><br><span class="line">&lt;target ...</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>注意</strong>：示例ID是libvirt，而不是第二部<font color="red">配置Ceph</font>生成的Ceph名称 client.libvirt。确保你使用的Ceph名称的ID组件是你生成的。如果由于某些原因你需要重新生成密码。在你再次执行<code>sudo virsh secret-set-value</code>命令之前，你必须要先执行<code>sudo virsh secret-undefine {uuid}</code>命令。  </p>
</blockquote>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>在你配置你的VM使用Ceph之前，你可以先启动VM。建议先确认VM和Ceph状态，你可以使用下面的操作。  </p>
<ol>
<li>检查Ceph是否运行：  </li>
</ol>
<blockquote>
<p>ceph health  </p>
</blockquote>
<ol start="2">
<li>检查VM是否运行：  </li>
</ol>
<blockquote>
<p>sudo virsh list</p>
</blockquote>
<ol start="3">
<li>检查VM是否可以访问Ceph。使用的VM域替换{vm-domain-name}：  </li>
</ol>
<blockquote>
<p>sudo virsh qemu-monitor-command –hmp {vm-domain-name} ‘info block’  </p>
</blockquote>
<ol start="4">
<li>检查<code>&lt;target dev=&#39;hdb&#39; bus=&#39;ide&#39;/&gt;</code>下的驱动是否存在/dev或/proc/partitions下：  </li>
</ol>
<blockquote>
<p>ls /dev<br>cat /proc/patitions</p>
</blockquote>
<p>如果每一步都成功，那么你可以在你的VM中使用Ceph块设备了。</p>
<h2 id="块设备与OpenStack"><a href="#块设备与OpenStack" class="headerlink" title="块设备与OpenStack"></a>块设备与OpenStack</h2><p>你可以通过libvirt在OpenStack中使用Ceph块设备images，将QEMU接口配置为librbd。Ceph在集群中将块设备images条带化为对象，这意味着大型的Ceph块设备images拥有比单独的服务器更好的性能！  </p>
<p>要在OpenStack中使用Ceph块设备，首先你必须安装QEMU，libvirt和OpenStack。我们建议你使用单独的物理设备安装OpenStack。OpenStack建议最小8GB RAM和四核处理器。下图描绘了OpenStack/Ceph的技术栈。  </p>
<p><img src="../images/Ceph_OpenStack.png" alt="Ceph with OpenStack">  </p>
<p>OpenStack的三个部分与Ceph块设备集成：  </p>
<ul>
<li><strong>Images</strong>：OpenStack Glance管理着VMs的镜像。镜像是不可变的。OpenStack将镜像视为二进制块并对应的下载它们。</li>
<li><strong>Volumes</strong>：Volumes是块设备。OpenStack使用volume引导VMs，或挂载volumes来运行VMs。OpenStack使用Cinder服务管理volumes。</li>
<li><strong>Guest Disks</strong>：客户硬盘时客户操作系统的硬盘。默认情况下，当你启动虚拟机时，它的硬盘作为文件出现在虚拟机监控程序的文件系统上（通常在/var/lib/nova/instances/\<uuid>/下）。在OpenStack Havana之前，在Ceph中启动VM的唯一方法是使用Cinder的boot-from-volume功能。然后，现在不使用Cinder，直接在Ceph中启动每个虚拟机成为可能，这是非常有利的，应为它允许你使用实时迁移过程轻松地执行维护操作。此外，如果你的管理程序失效，还可以方便地触发nova疏散并在其它地方几乎无缝地运行虚拟机。  </uuid></li>
</ul>
<p>你也可以在Ceph块设备中使用OpenStack Glance来存储镜像，并且你可以通过写时复制克隆一个image来使用Cinder引导VM。  </p>
<p>接下来的详细介绍Glance，Cinder和Nova，尽管它们不必一起使用。你可以在本地磁盘运行VM时将image存储到Ceph块设备中，反之亦可。  </p>
<h3 id="创建资源池"><a href="#创建资源池" class="headerlink" title="创建资源池"></a>创建资源池</h3><p>默认情况下，Ceph块设备使用rbd资源池。你也可以使用其它可用的资源池。我们建议你问Cinder创建一个资源池，也为Glance创建一个资源池。确保你的Ceph集群在运行中，然后创建资源池：  </p>
<blockquote>
<p>ceph osd pool create volumes 128<br>ceph osd pool create images 128<br>ceph osd pool create backups 128<br>ceph osd pool create vms 128  </p>
</blockquote>
<p>资源池中归置组数量的说明详见<a href="https://docs.ceph.com/docs/master/rados/operations/pools#createpool" target="_blank" rel="noopener"><font color="red">Create a Pool</font></a>，资源池中你应该设置的归置组数量详见<a href="https://docs.ceph.com/docs/master/rados/operations/placement-groups" target="_blank" rel="noopener"><font color="red">Placement Groups</font></a>。  </p>
<p>新创建的资源池在使用之前必须初始化。使用rbd工具初始化这些资源池：  </p>
<blockquote>
<p>rbd pool init volumes<br>rbd pool init images<br>rbd pool init backups<br>rbd pool init vms</p>
</blockquote>
<h3 id="配置OpenStack-Ceph客户端"><a href="#配置OpenStack-Ceph客户端" class="headerlink" title="配置OpenStack Ceph客户端"></a>配置OpenStack Ceph客户端</h3><p>运行glance-api、cinder-volume、nova-compute和cinder-backup的节点作为Ceph客户端。每个节点都需要ceph.conf文件：  </p>
<blockquote>
<p>ssh {your-openstack-server} sudo tee /etc/ceph/ceph.conf &lt;/etc/ceph/ceph.conf  </p>
</blockquote>
<h4 id="安装Ceph客户端包"><a href="#安装Ceph客户端包" class="headerlink" title="安装Ceph客户端包"></a>安装Ceph客户端包</h4><p>在glance-api节点，你需要librbd的Python包：  </p>
<blockquote>
<p>sudo apt-get install python-rbd<br>sudo yum install python-rbd</p>
</blockquote>
<p>在nove-compute、cinder-backup和cinder-volume节点，需要同时安装Python包和客户端命令行工具：  </p>
<blockquote>
<p>sudo apt-get install ceph-common<br>sudo yum install ceph-common</p>
</blockquote>
<h4 id="安装Ceph-客户端认证"><a href="#安装Ceph-客户端认证" class="headerlink" title="安装Ceph 客户端认证"></a>安装Ceph 客户端认证</h4><p>如果你已经启用了<a href="https://docs.ceph.com/docs/master/rados/configuration/auth-config-ref/#enabling-disabling-cephx" target="_blank" rel="noopener"><font color="red">cephx authentication</font></a>,需要为Nova/Cinder和Glance创建用户。执行以下命令：  </p>
<blockquote>
<p>ceph auth get-or-create client.glance mon ‘profile rbd’ osd ‘profile rbd pool=images’<br>ceph auth get-or-create client.cinder mon ‘profile rbd’ osd ‘profile rbd pool=volumes, profile rbd pool=vms, profile rbd-read-only pool=images’<br>ceph auth get-or-create client.cinder-backup mon ‘profile rbd’ osd ‘profile rbd pool=backups’</p>
</blockquote>
<p>将client.cinder、client.glance、client.cinder-backup的密钥环添加到合适的节点并改变它们的从属关系：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ceph auth get-or-create client.glance | ssh &#123;your-glance-api-server&#125; sudo tee /etc/ceph/ceph.client.glance.keyring</span><br><span class="line">ssh &#123;your-glance-api-server&#125; sudo chown glance:glance /etc/ceph/ceph.client.glance.keyring</span><br><span class="line">ceph auth get-or-create client.cinder | ssh &#123;your-volume-server&#125; sudo tee /etc/ceph/ceph.client.cinder.keyring</span><br><span class="line">ssh &#123;your-cinder-volume-server&#125; sudo chown cinder:cinder /etc/ceph/ceph.client.cinder.keyring</span><br><span class="line">ceph auth get-or-create client.cinder-backup | ssh &#123;your-cinder-backup-server&#125; sudo tee /etc/ceph/ceph.client.cinder-backup.keyring</span><br><span class="line">ssh &#123;your-cinder-backup-server&#125; sudo chown cinder:cinder /etc/ceph/ceph.client.cinder-backup.keyring</span><br></pre></td></tr></table></figure>
<p>运行nova-compute的节点需要nova-compute进程的密钥环文件：  </p>
<blockquote>
<p>ceph auth get-or-create client.cinder | ssh {your-nova-compute-server} sudo tee /etc/ceph/ceph.client.cinder.keyring</p>
</blockquote>
<p>在libvirt中，也需要存储client.cinder用户的密码。在从Cinder附加一个块设备时，libvirt需要它来访问集群。  </p>
<p>在运行nova-compute的节点创建临时密码副本：  </p>
<blockquote>
<p>ceph auth get-key client.cinder | ssh {your-compute-node} tee client.cinder.key</p>
</blockquote>
<p>然后，在计算节点上，添加密码到libvirt中并移除临时密钥副本：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">uuidgen</span><br><span class="line">457eb676-33da-42ec-9a8c-9293d545c337</span><br><span class="line"></span><br><span class="line">cat &gt; secret.xml &lt;&lt;EOF</span><br><span class="line">&lt;secret ephemeral=<span class="string">'no'</span> private=<span class="string">'no'</span>&gt;</span><br><span class="line">  &lt;uuid&gt;457eb676-33da-42ec-9a8c-9293d545c337&lt;/uuid&gt;</span><br><span class="line">  &lt;usage <span class="built_in">type</span>=<span class="string">'ceph'</span>&gt;</span><br><span class="line">    &lt;name&gt;client.cinder secret&lt;/name&gt;</span><br><span class="line">  &lt;/usage&gt;</span><br><span class="line">&lt;/secret&gt;</span><br><span class="line">EOF</span><br><span class="line">sudo virsh secret-define --file secret.xml</span><br><span class="line">Secret 457eb676-33da-42ec-9a8c-9293d545c337 created</span><br><span class="line">sudo virsh secret-set-value --secret 457eb676-33da-42ec-9a8c-9293d545c337 --base64 $(cat client.cinder.key) &amp;&amp; rm client.cinder.key secret.xml</span><br></pre></td></tr></table></figure>
<p>保存密码的UUID以便后续配置nova-compute。  </p>
<h3 id="配置OpenStack使用Ceph"><a href="#配置OpenStack使用Ceph" class="headerlink" title="配置OpenStack使用Ceph"></a>配置OpenStack使用Ceph</h3><h4 id="配置Glance"><a href="#配置Glance" class="headerlink" title="配置Glance"></a>配置Glance</h4><p>Glance可以使用多个后端来存储镜像。要默认使用Ceph块设备，如下配置Glance。  </p>
<h5 id="KILO-and-After"><a href="#KILO-and-After" class="headerlink" title="KILO and After"></a>KILO and After</h5><p>编辑<code>/etc/glance/glance-api.conf</code>并新增如下[glance_store]章节：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[glance_store]</span><br><span class="line">stores = rbd</span><br><span class="line">default_store = rbd</span><br><span class="line">rbd_store_pool = images</span><br><span class="line">rbd_store_user = glance</span><br><span class="line">rbd_store_ceph_conf = /etc/ceph/ceph.conf</span><br><span class="line">rbd_store_chunk_size = 8</span><br></pre></td></tr></table></figure>
<p>更多有关Glance中可用的配置选项，请查看OpenStack配置应用：<a href="http://docs.openstack.org/" target="_blank" rel="noopener"><font color="red">http://docs.openstack.org/</font></a>。  </p>
<h5 id="启用镜像的写时复制"><a href="#启用镜像的写时复制" class="headerlink" title="启用镜像的写时复制"></a>启用镜像的写时复制</h5><p>注意这会通过Glance的API公开后端位置，所以启用此选项的终端不应公开访问。  </p>
<p>除了Mitaka外的所有OpenStack发行版  </p>
<p>如果你想启用镜像的写时复制克隆，在[DEFAULT]章节下新增：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_image_direct_url = True</span><br></pre></td></tr></table></figure>
<h5 id="禁用缓存管理（所有OpenStack发行版）"><a href="#禁用缓存管理（所有OpenStack发行版）" class="headerlink" title="禁用缓存管理（所有OpenStack发行版）"></a>禁用缓存管理（所有OpenStack发行版）</h5><p>假设你的配置文件中有<code>flavor = keystone+cachemanagement</code>，禁用Glance的缓存管理以免在<code>/var/lib/glance/image-cache/</code>下缓存镜像：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[paste_deploy]</span><br><span class="line">flavor = keystone</span><br></pre></td></tr></table></figure>
<h5 id="镜像属性"><a href="#镜像属性" class="headerlink" title="镜像属性"></a>镜像属性</h5><p>我们建议为你的镜像使用如下属性：  </p>
<ul>
<li>hw_scsi_model=virtio-scsi：添加virtio-scsi控制，获得更好的性能，支持丢弃选项</li>
<li>hw_disk_bus=scsi：连接每个Cinder块设备到控制器</li>
<li>hw_qemu_guest_agent=yes：启用QEMU客户代理</li>
<li>os_require_quiesce=yes：通过QEMU客户代理发送fs-freeze/thaw请求  </li>
</ul>
<h4 id="配置Cinder"><a href="#配置Cinder" class="headerlink" title="配置Cinder"></a>配置Cinder</h4><p>OpenStack需要一个驱动与Ceph块设备进行交互。你必须为块设备指明资源池名称。在你的OpenStack节点上，编辑<code>/etc/cinder/cinder.conf</code>，添加：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[DEFAULT]</span><br><span class="line">...</span><br><span class="line">enabled_backends = ceph</span><br><span class="line">glance_api_version = 2</span><br><span class="line">...</span><br><span class="line">[ceph]</span><br><span class="line">volume_driver = cinder.volume.drivers.rbd.RBDDriver</span><br><span class="line">volume_backend_name = ceph</span><br><span class="line">rbd_pool = volumes</span><br><span class="line">rbd_ceph_conf = /etc/ceph/ceph.conf</span><br><span class="line">rbd_flatten_volume_from_snapshot = false</span><br><span class="line">rbd_max_clone_depth = 5</span><br><span class="line">rbd_store_chunk_size = 4</span><br><span class="line">rados_connect_timeout = -1</span><br></pre></td></tr></table></figure>
<p>如果你已经启用了<a href="https://docs.ceph.com/docs/master/rados/configuration/auth-config-ref/#enabling-disabling-cephx" target="_blank" rel="noopener"><font color="red">cephx authentication</font></a>，如之前所述，你还要配置你添加的密码的用户和UUID：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[ceph]</span><br><span class="line">...</span><br><span class="line">rbd_user = cinder</span><br><span class="line">rbd_secret_uuid = 457eb676-33da-42ec-9a8c-9293d545c337</span><br></pre></td></tr></table></figure>
<p>注意，如果你配置了多个Cinder后端，在[DEFAULT]章节下必须设置glance_api_version = 2。  </p>
<h4 id="配置Cinder备份"><a href="#配置Cinder备份" class="headerlink" title="配置Cinder备份"></a>配置Cinder备份</h4><p>OpenStack Cinder Backup要求一个特殊的守护进程，所以别忘了安装它。在你的Cinder Backup节点，编辑<code>/etc/cinder/cinder.conf</code>并新增：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">backup_driver = cinder.backup.drivers.ceph</span><br><span class="line">backup_ceph_conf = /etc/ceph/ceph.conf</span><br><span class="line">backup_ceph_user = cinder-backup</span><br><span class="line">backup_ceph_chunk_size = 134217728</span><br><span class="line">backup_ceph_pool = backups</span><br><span class="line">backup_ceph_stripe_unit = 0</span><br><span class="line">backup_ceph_stripe_count = 0</span><br><span class="line">restore_discard_excess_bytes = true</span><br></pre></td></tr></table></figure>
<h4 id="为附加Ceph-RBD块设备配置Nova"><a href="#为附加Ceph-RBD块设备配置Nova" class="headerlink" title="为附加Ceph RBD块设备配置Nova"></a>为附加Ceph RBD块设备配置Nova</h4><p>为了附件Cinder设备（无论是普通的块设备还是从volume中引导），你必须告诉Nova（和libvirt）在附加设备时引用的是哪个用户和UUID。在连接和认证Ceph集群时，libvirt将使用这个用户。  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[libvirt]</span><br><span class="line">...</span><br><span class="line">rbd_user = cinder</span><br><span class="line">rbd_secret_uuid = 457eb676-33da-42ec-9a8c-9293d545c337</span><br></pre></td></tr></table></figure>
<p>这两个标志会被Nova临时后端使用。  </p>
<h4 id="配置Nova"><a href="#配置Nova" class="headerlink" title="配置Nova"></a>配置Nova</h4><p>为了直接从Ceph中引导所有的虚拟机，你必须为Nova配置临时后端。  </p>
<p>建议在你的Ceph配置文件中（从Giant开始默认启用）启用RBD缓存。例外，启用管理员套接字在定位问题时将带来很多好处。使用Ceph块设备时为每一个虚拟机设置一个套接字将有助于调查性能和错误行为。  </p>
<p>像下面这样反问套接字：  </p>
<blockquote>
<p>ceph daemon /var/run/ceph/ceph-client.cinder.19195.32310016.asok help</p>
</blockquote>
<p>现在，在每个计算节点上编辑你的Ceph配置文件：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[client]</span><br><span class="line">    rbd cache = true</span><br><span class="line">    rbd cache writethrough until flush = true</span><br><span class="line">    admin socket = /var/run/ceph/guests/$cluster-$type.$id.$pid.$cctid.asok</span><br><span class="line">    log file = /var/log/qemu/qemu-guest-$pid.log</span><br><span class="line">    rbd concurrent management ops = 20</span><br></pre></td></tr></table></figure>
<p>配置这些路径的权限：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /var/run/ceph/guests/ /var/<span class="built_in">log</span>/qemu/</span><br><span class="line">chown qemu:libvirtd /var/run/ceph/guests /var/<span class="built_in">log</span>/qemu/</span><br></pre></td></tr></table></figure>
<p>注意，QEMU用户和libvirt组可能因系统而异。提供的例子是基于RedHat系统工作的。  </p>
<h4 id="重启OpenStack"><a href="#重启OpenStack" class="headerlink" title="重启OpenStack"></a>重启OpenStack</h4><p>为了激活Ceph块设备驱动和导入配置中的块设备资源池，你必须重启OpenStack。对于基于Debian系统，在合适的节点执行下列命令：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo glance-control api restart</span><br><span class="line">sudo service nova-compute restart</span><br><span class="line">sudo service cinder-volume restart</span><br><span class="line">sudo service cinder-backup restart</span><br></pre></td></tr></table></figure>
<p>对于基于RedHa他系统，执行：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo service openstack-glance-api restart</span><br><span class="line">sudo service openstack-nova-compute restart</span><br><span class="line">sudo service openstack-cinder-volume restart</span><br><span class="line">sudo service openstack-cinder-backup restart</span><br></pre></td></tr></table></figure>
<p>一旦OpenStack上线运行，你应该能够创建卷并从中引导启动。  </p>
<h4 id="从块设备中引导启动"><a href="#从块设备中引导启动" class="headerlink" title="从块设备中引导启动"></a>从块设备中引导启动</h4><p>使用Cinder命令行工具你可以从image中创建一个卷：  </p>
<blockquote>
<p>cinder create –image-id {id of image} –display-name {name of volume} {size of volume}</p>
</blockquote>
<p>你可以使用<a href="https://docs.ceph.com/docs/master/rbd/qemu-rbd/#running-qemu-with-rbd" target="_blank" rel="noopener"><font color="red">qemu-img</font></a>进行格式转换。例如：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">qemu-img convert -f &#123;<span class="built_in">source</span>-format&#125; -O &#123;output-format&#125; &#123;<span class="built_in">source</span>-filename&#125; &#123;output-filename&#125;</span><br><span class="line">qemu-img convert -f qcow2 -O raw precise-cloudimg.img precise-cloudimg.raw</span><br></pre></td></tr></table></figure>
<p>当Glance和Cinder都使用Ceph块设备，因为image是写时复制克隆，所以它可以快速地创建一个新的卷。在OpenStack dashboard中，通过下列步骤，你可以从卷中启动：  </p>
<ol>
<li>创建一个新的实例</li>
<li>选择与写时复制克隆相关联的image</li>
<li>选择“从卷启动”</li>
<li>选择你创建的卷</li>
</ol>
<h2 id="Ceph-iSCSI网关"><a href="#Ceph-iSCSI网关" class="headerlink" title="Ceph iSCSI网关"></a>Ceph iSCSI网关</h2><p>iSCSI网关是Ceph存储与iSCSI标准融为一体，以提供高可用(HA)的iSCSI target，来将RADOS块设备导出为SCSI硬盘。iSCSI协议运行客户端（initiators）通过TCP/IP网络发送SCSI命令给SCSI存储设备（target）。这使得各种客户端，例如Microsoft Windows，可以访问Ceph存储集群。  </p>
<p>每个iSCSI网关都运行在Linux IO target内核子系统（LIO）内以提供iSCSI协议支持。LIO利用用户空间passthrough（TCMU）与Ceph的librbd库交互，并向iSCSI客户端导出RBD image。使用Ceph的iSCSI网关，你可以有效地运行一个完全集成的块设备基础设施，它具有传统存储区域网络（SAN）的所有特性和优点。  </p>
<p><img src="../images/Ceph_iSCSI_Gateway.png" alt="Ceph iSCSI GateWay">  </p>
<h3 id="ISCSI网关要求"><a href="#ISCSI网关要求" class="headerlink" title="ISCSI网关要求"></a>ISCSI网关要求</h3><p>要实现Ceph iSCSI网关，需要满足一些要求。高可用的Ceph iSCSI网关解决方案推荐使用2到4个iSCSI网关节点。  </p>
<p>关于推荐硬件，详见<a href="https://docs.ceph.com/docs/master/start/hardware-recommendations/#hardware-recommendations" target="_blank" rel="noopener"><font color="red">Hardware Recommendations</font></a>。  </p>
<p>对于Ceph Monitors或OSDs来说，iSCSI网关选项没有特殊说明，降低检查OSDs的默认定时器是很重要的，这将减少启动器超时的可能性。对于存储集群中的每个OSD节点，建议使用以下配置选项：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[osd]</span><br><span class="line">osd heartbeat grace = 20</span><br><span class="line">osd heartbeat interval = 5</span><br></pre></td></tr></table></figure>
<ul>
<li>使用Ceph Monitor在线更新：  </li>
</ul>
<blockquote>
<p>ceph tell \&lt;daemon_type&gt;.\<id> config set \&lt;parameter_name&gt; \&lt;new_value&gt;  </id></p>
</blockquote>
<blockquote>
<p>ceph tell osd.0 config set osd_heartbeat_grace 20<br>ceph tell osd.0 config set osd_heartbeat_interval 5  </p>
</blockquote>
<ul>
<li>在OSD节点在线更新  </li>
</ul>
<blockquote>
<p>ceph daemon \&lt;daemon_type&gt;.\<id> config set osd_client_watch_timeout 15  </id></p>
</blockquote>
<blockquote>
<p>ceph daemon osd.0 config set osd_heartbeat_grace 20<br>ceph daemon osd.0 config set osd_heartbeat_interval 5  </p>
</blockquote>
<p>更多Ceph配置选项，参见<a href="https://docs.ceph.com/docs/master/rados/configuration/ceph-conf/#configuring-ceph" target="_blank" rel="noopener"><font color="red">配置Ceph</font></a>。  </p>
<h3 id="ISCSI-Targets"><a href="#ISCSI-Targets" class="headerlink" title="ISCSI Targets"></a>ISCSI Targets</h3><p>传统上来说，对Ceph存储集群的块级别的访问仅限于QEMU和librbd，这样是OpenStack环境中采用的关键因素。从Ceph Luminous版本开始，块级别的访问正在扩展，以提供标准的iSCSI支持，允许更广泛的平台使用，并可能打开新的使用场景。  </p>
<ul>
<li>Red Hat Enterprise Linux/CentOS 7.5 (or newer); Linux kernel v4.16 (or newer)</li>
<li>使用ceph-ansible或使用命令行部署的工作Ceph存储集群</li>
<li>iSCSI网关节点，可以与OSD节点或专用节点共存</li>
<li>为iSCSI前端流量和Ceph后端流量分离网络子网  </li>
</ul>
<h4 id="使用ansible配置ISCSI-Target"><a href="#使用ansible配置ISCSI-Target" class="headerlink" title="使用ansible配置ISCSI Target"></a>使用ansible配置ISCSI Target</h4><p>Ceph iSCSI网关既是iSCSI target节点，也是Ceph client节点。Ceph iSCSI网关可以是一个单独的节点，也可以托管于Ceph对象存储磁盘（OSD）节点。完成以下步骤后，将安装并配置Ceph iSCSI网关进行基本操作。  </p>
<p><strong>要求</strong>：  </p>
<ul>
<li>运行中的Ceph Luminous(12.2.x)或更新的集群</li>
<li>Red Hat Enterprise Linux/CentOS 7.5 (or newer); Linux kernel v4.16 (or newer)</li>
<li>在所有iSCSI网关节点上安装<code>ceph-iscsi</code>包  </li>
</ul>
<p><strong>安装</strong>：  </p>
<p>在Ansible安装节点，可以是管理节点或专业部署节点，执行以下操作：  </p>
<ol>
<li>使用<code>root</code>安装<code>ceph-ansible</code>包：  </li>
</ol>
<pre><code>&gt; # yum install ceph-ansible
</code></pre><ol start="2">
<li><p>在<code>/etc/ansible/hosts</code>文件中未网关组添加条目：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[iscsigws]</span><br><span class="line">ceph-igw-1</span><br><span class="line">ceph-igw-2</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><strong>配置</strong>：  </p>
<p>ceph-ansible包在<code>/usr/share/ceph-ansible/group_vars/</code>目录下放置了一个名为<code>iscsigws.yml.sample</code>的文件。创建一个这个示例文件的副本，命名为<code>iscsigws.yml</code>。查看以下Ansible变量和说明，并进行相应更新。可在iscsigws.yml.sample文件查阅高级变量的完整列表。  </p>
<table>
<thead>
<tr>
<th style="text-align:center"><strong>变量</strong></th>
<th style="text-align:center"><strong>意义/目的</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">seed_monitor</td>
<td style="text-align:center">由于RADOS和rbd的调用，每个网关都需要访问ceph集群。这意味着iSCSI网关必须定义一个适当的/etc/ceph/目录。seed_monitor主机用于填充iSCSI网关的/etc/ceph/目录。</td>
</tr>
<tr>
<td style="text-align:center">cluster_name</td>
<td style="text-align:center">定义一个惯用的存储集群名称。</td>
</tr>
<tr>
<td style="text-align:center">gateway_keyring</td>
<td style="text-align:center">定义一个惯用的密钥环名称。</td>
</tr>
<tr>
<td style="text-align:center">deploy_setting</td>
<td style="text-align:center">如设置为True，则在运行playbook时部署设置。</td>
</tr>
<tr>
<td style="text-align:center">perform_system_checks</td>
<td style="text-align:center">这是一个布尔值，用于检查每个网关上的多路径和lvm配置设置。至少在第一次运行时，必须将其设置为true，以确保正确配置了multipathd和lvm。</td>
</tr>
<tr>
<td style="text-align:center">api_user</td>
<td style="text-align:center">API的用户名。默认为<em>admin</em>。</td>
</tr>
<tr>
<td style="text-align:center">api_password</td>
<td style="text-align:center">使用API的密码。默认为<em>admin</em>。</td>
</tr>
<tr>
<td style="text-align:center">api_port</td>
<td style="text-align:center">使用API的TCP端口。默认为5000.</td>
</tr>
<tr>
<td style="text-align:center">api_secure</td>
<td style="text-align:center">设为True时，必须使用TLS。默认为<em>false</em>。如果为True，用户必须创建必要的证书和密钥文件。详见gwcli man文件。</td>
</tr>
<tr>
<td style="text-align:center">trusted_ip_list</td>
<td style="text-align:center">有权访问API的IPV4或IPV6列表。默认情况下，只有iSCSI网关节点有权访问。</td>
</tr>
</tbody>
</table>
<p><strong>部署</strong>：  </p>
<p>在Ansible安装节点，执行以下操作：  </p>
<ol>
<li><p>使用root执行Ansible手册：  </p>
<blockquote>
<h1 id="cd-usr-share-ceph-ansible"><a href="#cd-usr-share-ceph-ansible" class="headerlink" title="cd /usr/share/ceph-ansible"></a>cd /usr/share/ceph-ansible</h1><h1 id="ansible-playbook-site-yml-–limit-iscsigws"><a href="#ansible-playbook-site-yml-–limit-iscsigws" class="headerlink" title="ansible-playbook site.yml –limit iscsigws"></a>ansible-playbook site.yml –limit iscsigws</h1></blockquote>
</li>
<li><p>从iSCSI网关节点验证配置：  </p>
<blockquote>
<h1 id="gwcli-ls"><a href="#gwcli-ls" class="headerlink" title="gwcli ls"></a>gwcli ls</h1></blockquote>
</li>
</ol>
<p><strong>服务管理</strong>：  </p>
<p>ceph-iscsi包安装了配置管理逻辑和名为<code>rbd-target-api</code>的systemd服务。启用服务后，rbd-target-api会在引导时启动并恢复Linux IO状态。在部署期间，Ansible playbook禁用target 服务。下面是使用rbd-target-api systemd服务交互的结果：  </p>
<blockquote>
<p># systemctl \&lt;start|stop|restart|reload&gt; rbd-target-api  </p>
</blockquote>
<ul>
<li>reload  </li>
</ul>
<p>$\qquad$reload请求强制rbd-target-api重读配置并在当前环境下应用它。这不是常用请求，因为更改将从Ansible平行部署到所有的iSCSI网关节点。</p>
<ul>
<li>stop  </li>
</ul>
<p>$\qquad$stop请求将关闭网关的进出口，丢弃到客户端的连接并且从内核中擦除当前的LIO连接。这会将iSCSI网关返回到干净状态。当客户端断开连接时，客户端多路径层将活动I / O重新调度到其他iSCSI网关。  </p>
<p><strong>移除配置</strong>：  </p>
<p>ceph-ansible包提供了一个Ansible手册，用于移除iSCSI网关配置和连接RBD images。Ansible手册是<code>/usr/share/ceph-ansible/purge_gateways.yml</code>。当执行Ansible playbook时，系统会提示选择清除操作的类型：  </p>
<p><em>lio</em>：  </p>
<p>在该模式下，所有iSCSI网关定义的LIO配置都将被清除。Ceph存储集群中创建的硬盘将保持不变。   </p>
<p><em>all</em>：  </p>
<p>当选则all时，与LIO配置一同移除还有iSCSI网关环境中定义的所有RBD images，其它不相关的RBD images并不会被移除。确保选择了正确的模式，该操作将删除所有数据。  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">[root@rh7-iscsi-client ceph-ansible]<span class="comment"># ansible-playbook purge_gateways.yml</span></span><br><span class="line">Which configuration elements should be purged? (all, lio or abort) [abort]: all</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">PLAY [Confirm removal of the iSCSI gateway configuration] *********************</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">GATHERING FACTS ***************************************************************</span><br><span class="line">ok: [localhost]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">TASK: [Exit playbook <span class="keyword">if</span> user aborted the purge] *******************************</span><br><span class="line">skipping: [localhost]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">TASK: [set_fact ] *************************************************************</span><br><span class="line">ok: [localhost]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">PLAY [Removing the gateway configuration] *************************************</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">GATHERING FACTS ***************************************************************</span><br><span class="line">ok: [ceph-igw-1]</span><br><span class="line">ok: [ceph-igw-2]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">TASK: [igw_purge | purging the gateway configuration] *************************</span><br><span class="line">changed: [ceph-igw-1]</span><br><span class="line">changed: [ceph-igw-2]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">TASK: [igw_purge | deleting configured rbd devices] ***************************</span><br><span class="line">changed: [ceph-igw-1]</span><br><span class="line">changed: [ceph-igw-2]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">PLAY RECAP ********************************************************************</span><br><span class="line">ceph-igw-1                 : ok=3    changed=2    unreachable=0    failed=0</span><br><span class="line">ceph-igw-2                 : ok=3    changed=2    unreachable=0    failed=0</span><br><span class="line">localhost                  : ok=2    changed=0    unreachable=0    failed=0</span><br></pre></td></tr></table></figure>
<h4 id="使用命令行接口配置iSCSI-Target"><a href="#使用命令行接口配置iSCSI-Target" class="headerlink" title="使用命令行接口配置iSCSI Target"></a>使用命令行接口配置iSCSI Target</h4><p>Ceph iSCSI网关既是iSCSI target节点也是Ceph client节点。Ceph iSCSI网关可以是一个单独的节点，也可以托管于Ceph对象存储磁盘（OSD）节点。完成以下步骤后，将安装并配置Ceph iSCSI网关进行基本操作。  </p>
<p><strong>要求</strong>：  </p>
<ul>
<li>运行中的Ceph Luminous(12.2.x)或更新的集群</li>
<li>Red Hat Enterprise Linux/CentOS 7.5 (or newer); Linux kernel v4.16 (or newer)</li>
<li>必须从你的Linux 发行版的软件仓库安装下列包：  <ul>
<li>targetcli-2.1.fb47 or newer package</li>
<li>python-rtslib-2.1.fb68 or newer package  </li>
<li>tcmu-runner-1.4.0 or newer package</li>
<li>ceph-iscsi-3.2 or newer package  </li>
</ul>
</li>
</ul>
<p>在继续<em>安装</em>章节前，需要在Ceph iSCSI网关节点执行以下步骤：  </p>
<ol>
<li>如果Ceph iSCSI网关没有和OSD节点在一起，从存储集群中运行的Ceph节点的<code>/etc/ceph</code>目录下复制Ceph配置文件到iSCSI网关节点。iSCSI网关节点上的Ceph配置文件必须存在于<code>/etc/ceph</code>目录下。</li>
<li>安装并配置<a href="http://docs.ceph.com/docs/master/start/quick-rbd/#install-ceph" target="_blank" rel="noopener"><font color="red">Ceph Command-line Interface</font></a>。</li>
<li>如果需要，打开防火墙上的TCP 3260和5000端口。</li>
<li>创建一个新的或使用已存在的RBD。  </li>
</ol>
<p><strong>安装</strong>：  </p>
<p>如果使用上游ceph-iscsi包，请遵循<a href="https://docs.ceph.com/docs/master/rbd/iscsi-target-cli-manual-install/" target="_blank" rel="noopener"><font color="red">手动安装说明</font></a>。  </p>
<p>基于rpm指令执行以下命令：  </p>
<ol>
<li><p>在所有iSCSI网关节点上，使用root 安装 ceph-iscsi包：  </p>
<blockquote>
<h1 id="yum-install-ceph-iscsi"><a href="#yum-install-ceph-iscsi" class="headerlink" title="yum install ceph-iscsi"></a>yum install ceph-iscsi</h1></blockquote>
</li>
<li><p>在所有iSCSI网关节点上，使用root 安装 tcmu-runner包：  </p>
<blockquote>
<h1 id="yum-install-tcmu-runner"><a href="#yum-install-tcmu-runner" class="headerlink" title="yum install tcmu-runner"></a>yum install tcmu-runner</h1></blockquote>
</li>
</ol>
<p><strong>构建</strong>：  </p>
<ol>
<li><p>gwcli需要一个名为rbd的池，因此它可以存储像iSCSI配置这样的元数据。检查这个池是否已经创建运行:  </p>
<blockquote>
<h1 id="ceph-osd-lspools"><a href="#ceph-osd-lspools" class="headerlink" title="ceph osd lspools"></a>ceph osd lspools</h1></blockquote>
<p>如果不存在，创建资源池的操作可在<a href="http://docs.ceph.com/docs/master/rados/operations/pools/" target="_blank" rel="noopener"><font color="red">RADOS pool operations page</font></a> 找到。  </p>
</li>
<li><p>在iSCSI网关节点上，使用root在<code>/etc/ceph</code>目录下创建名为<code>iscsi-gateway.cfg</code>文件：  </p>
<blockquote>
<h1 id="touch-etc-ceph-iscsi-gateway-cfg"><a href="#touch-etc-ceph-iscsi-gateway-cfg" class="headerlink" title="touch /etc/ceph/iscsi-gateway.cfg"></a>touch /etc/ceph/iscsi-gateway.cfg</h1></blockquote>
<ol>
<li>编辑<code>iscsi-gateway.cfg</code>文件，添加以下内容：  </li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[config]</span><br><span class="line"># Name of the Ceph storage cluster. A suitable Ceph configuration file allowing</span><br><span class="line"># access to the Ceph storage cluster from the gateway node is required, if not</span><br><span class="line"># colocated on an OSD node.</span><br><span class="line">cluster_name = ceph</span><br><span class="line"></span><br><span class="line"># Place a copy of the ceph cluster&apos;s admin keyring in the gateway&apos;s /etc/ceph</span><br><span class="line"># drectory and reference the filename here</span><br><span class="line">gateway_keyring = ceph.client.admin.keyring</span><br></pre></td></tr></table></figure>
<h1 id="API-settings"><a href="#API-settings" class="headerlink" title="API settings."></a>API settings.</h1><h1 id="The-API-supports-a-number-of-options-that-allow-you-to-tailor-it-to-your"><a href="#The-API-supports-a-number-of-options-that-allow-you-to-tailor-it-to-your" class="headerlink" title="The API supports a number of options that allow you to tailor it to your"></a>The API supports a number of options that allow you to tailor it to your</h1><h1 id="local-environment-If-you-want-to-run-the-API-under-https-you-will-need-to"><a href="#local-environment-If-you-want-to-run-the-API-under-https-you-will-need-to" class="headerlink" title="local environment. If you want to run the API under https, you will need to"></a>local environment. If you want to run the API under https, you will need to</h1><h1 id="create-cert-key-files-that-are-compatible-for-each-iSCSI-gateway-node-that-is"><a href="#create-cert-key-files-that-are-compatible-for-each-iSCSI-gateway-node-that-is" class="headerlink" title="create cert/key files that are compatible for each iSCSI gateway node, that is"></a>create cert/key files that are compatible for each iSCSI gateway node, that is</h1><h1 id="not-locked-to-a-specific-node-SSL-cert-and-key-files-must-be-called"><a href="#not-locked-to-a-specific-node-SSL-cert-and-key-files-must-be-called" class="headerlink" title="not locked to a specific node. SSL cert and key files must be called"></a>not locked to a specific node. SSL cert and key files <em>must</em> be called</h1><h1 id="‘iscsi-gateway-crt’-and-‘iscsi-gateway-key’-and-placed-in-the-‘-etc-ceph-‘-directory"><a href="#‘iscsi-gateway-crt’-and-‘iscsi-gateway-key’-and-placed-in-the-‘-etc-ceph-‘-directory" class="headerlink" title="‘iscsi-gateway.crt’ and ‘iscsi-gateway.key’ and placed in the ‘/etc/ceph/‘ directory"></a>‘iscsi-gateway.crt’ and ‘iscsi-gateway.key’ and placed in the ‘/etc/ceph/‘ directory</h1><h1 id="on-each-gateway-node-With-the-SSL-files-in-place-you-can-use-‘api-secure-true’"><a href="#on-each-gateway-node-With-the-SSL-files-in-place-you-can-use-‘api-secure-true’" class="headerlink" title="on each gateway node. With the SSL files in place, you can use ‘api_secure = true’"></a>on <em>each</em> gateway node. With the SSL files in place, you can use ‘api_secure = true’</h1><h1 id="to-switch-to-https-mode"><a href="#to-switch-to-https-mode" class="headerlink" title="to switch to https mode."></a>to switch to https mode.</h1><h1 id="To-support-the-API-the-bear-minimum-settings-are"><a href="#To-support-the-API-the-bear-minimum-settings-are" class="headerlink" title="To support the API, the bear minimum settings are:"></a>To support the API, the bear minimum settings are:</h1><p>api_secure = false</p>
<h1 id="Additional-API-configuration-options-are-as-follows-defaults-shown"><a href="#Additional-API-configuration-options-are-as-follows-defaults-shown" class="headerlink" title="Additional API configuration options are as follows, defaults shown."></a>Additional API configuration options are as follows, defaults shown.</h1><h1 id="api-user-admin"><a href="#api-user-admin" class="headerlink" title="api_user = admin"></a>api_user = admin</h1><h1 id="api-password-admin"><a href="#api-password-admin" class="headerlink" title="api_password = admin"></a>api_password = admin</h1><h1 id="api-port-5001"><a href="#api-port-5001" class="headerlink" title="api_port = 5001"></a>api_port = 5001</h1><h1 id="trusted-ip-list-192-168-0-10-192-168-0-11"><a href="#trusted-ip-list-192-168-0-10-192-168-0-11" class="headerlink" title="trusted_ip_list = 192.168.0.10,192.168.0.11"></a>trusted_ip_list = 192.168.0.10,192.168.0.11</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line">    2. 使用root，将文件复制到所有iSCSI网关节点。</span><br><span class="line">   3. 在所有iSCSI网关节点上，使用root用户，启用并开启API服务：  </span><br><span class="line"></span><br><span class="line">      &gt; \# systemctl daemon-reload</span><br><span class="line">      &gt; \# systemctl enable rbd-target-api</span><br><span class="line">      &gt; \# systemctl start rbd-target-api  </span><br><span class="line"></span><br><span class="line">**配置**：  </span><br><span class="line"></span><br><span class="line">gwcli将创建和配置iSCSI target和RBD images，并复制上一节的网关设置中的配置。较低层的工具，如targetcli和rbd，可以用来查询本地配置，但不应该用来修改它。下一节将演示如何创建iSCSI target并将RBD image导出为LUN 0。  </span><br><span class="line"></span><br><span class="line">   1. 在一个iSCSI网关节点，使用root，开启iSCSI网关命令行接口：  </span><br><span class="line"></span><br><span class="line">      &gt; \# gwcli</span><br><span class="line"></span><br><span class="line">   2. 转到iscsi-target 并创建一个名为`iqn.2003-01.com.redhat.iscsi-gw:iscsi-igw`的target：  </span><br><span class="line"></span><br><span class="line">      &gt; /&gt; cd /iscsi-target</span><br><span class="line">      &gt; /iscsi-target&gt; create iqn.2003-01.com.redhat.iscsi-gw:iscsi-igw</span><br><span class="line"></span><br><span class="line">   3. 创建iSCSI网关。下面使用的ip将用于iSCSI数据，如读和写命令。它们可以是trusted_ip_list中列出的用于管理操作的相同ip，但是建议使用不同的ip。  </span><br><span class="line"></span><br><span class="line">      &gt; /iscsi-target&gt; cd iqn.2003-01.com.redhat.iscsi-gw:iscsi-igw/gateways</span><br><span class="line">      &gt; /iscsi-target...-igw/gateways&gt;  create ceph-gw-1 10.172.19.21</span><br><span class="line">      &gt; /iscsi-target...-igw/gateways&gt;  create ceph-gw-2 10.172.19.22</span><br><span class="line"></span><br><span class="line">      如果不使用RHEL/CentOS或使用上游或ceph-iscsi-test内核，则必须使用skipcheck =true参数。这将避免红帽内核和rpm检查:  </span><br><span class="line"></span><br><span class="line">      &gt; /iscsi-target&gt; cd iqn.2003-01.com.redhat.iscsi-gw:iscsi-igw/gateways</span><br><span class="line">      &gt; /iscsi-target...-igw/gateways&gt;  create ceph-gw-1 10.172.19.21 skipchecks=true</span><br><span class="line">      &gt; /iscsi-target...-igw/gateways&gt;  create ceph-gw-2 10.172.19.22 skipchecks=true</span><br><span class="line"></span><br><span class="line">   4. 在rbd池中新增名为disk_1的RBD image：  </span><br><span class="line"></span><br><span class="line">      &gt; /iscsi-target...-igw/gateways&gt; cd /disks</span><br><span class="line">      &gt; /disks&gt; create pool=rbd image=disk_1 size=30G</span><br><span class="line"></span><br><span class="line">   5. 新建一个名为iqn.1994-05.com.redhat:rh7-client的initiator 客户端：  </span><br><span class="line"></span><br><span class="line">      &gt; /disks&gt; cd /iscsi-target/iqn.2003-01.com.redhat.iscsi-gw:iscsi-igw/hosts</span><br><span class="line">      &gt; /iscsi-target...eph-igw/hosts&gt;  create iqn.1994-05.com.redhat:rh7-client</span><br><span class="line"></span><br><span class="line">   6. 设置客户端的CHAP，用户名为myiscsiusername，密码为myiscsipassword：  </span><br><span class="line"></span><br><span class="line">      &gt; /iscsi-target...at:rh7-client&gt;  auth username=myiscsiusername password=myiscsipassword </span><br><span class="line"></span><br><span class="line">   7. 给客户端添加硬盘：  </span><br><span class="line"></span><br><span class="line">      &gt; /iscsi-target...at:rh7-client&gt; disk add rbd/disk_1  </span><br><span class="line"></span><br><span class="line">### 配置iSCSI Initiators  </span><br><span class="line"></span><br><span class="line">#### iSCSI Initiator for Linux  </span><br><span class="line"></span><br><span class="line">**准备**：  </span><br><span class="line"></span><br><span class="line">  * iscsi-initiator-utils包</span><br><span class="line">  * device-mapper-multipath包  </span><br><span class="line"></span><br><span class="line">**安装**：  </span><br><span class="line"></span><br><span class="line">安装iSCSI initiator和multipath工具：  </span><br><span class="line"></span><br><span class="line">&gt; \# yum install iscsi-initiator-utils device-mapper-multipath</span><br><span class="line"></span><br><span class="line">**配置**：  </span><br><span class="line"></span><br><span class="line">1.  创建默认的`/etc/multipath.conf`文件并启用multipathd服务：  </span><br><span class="line">  </span><br><span class="line">   &gt; \# mpathconf --enable --with_multipathd y</span><br><span class="line"></span><br><span class="line">2. 添加以下内容到`/etc/multipath.conf`文件：  </span><br><span class="line"></span><br><span class="line">```text</span><br><span class="line">devices &#123;</span><br><span class="line">        device &#123;</span><br><span class="line">                vendor                 &quot;LIO-ORG&quot;</span><br><span class="line">                hardware_handler       &quot;1 alua&quot;</span><br><span class="line">                path_grouping_policy   &quot;failover&quot;</span><br><span class="line">                path_selector          &quot;queue-length 0&quot;</span><br><span class="line">                failback               60</span><br><span class="line">                path_checker           tur</span><br><span class="line">                prio                   alua</span><br><span class="line">                prio_args              exclusive_pref_bit</span><br><span class="line">                fast_io_fail_tmo       25</span><br><span class="line">                no_path_retry          queue</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol start="3">
<li>重启multipathd服务：  </li>
</ol>
<blockquote>
<p># systemctl reload multipathd  </p>
</blockquote>
<p><strong>iSCIS发现及安装</strong>：  </p>
<ol>
<li>如果CHAP已安装在iSCSI网关，请通过更新相应的<code>/etc/iscsi/iscsid.conf</code>来提供CHAP的用户和密码。</li>
<li>发现target门户：  </li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># iscsiadm -m discovery -t st -p 192.168.56.101</span></span><br><span class="line">192.168.56.101:3260,1 iqn.2003-01.org.linux-iscsi.rheln1</span><br><span class="line">192.168.56.102:3260,2 iqn.2003-01.org.linux-iscsi.rheln1</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>登录：  </li>
</ol>
<blockquote>
<p># iscsiadm -m node -T iqn.2003-01.org.linux-iscsi.rheln1 -l  </p>
</blockquote>
<p><strong>Multipath IO安装</strong>：  </p>
<p>multipath守护进程（multipathd）将自动根据multipath.conf的设置来设置设备。运行multipath命令显示故障转移配置中的设备设置，每个路径都有一个优先级组。  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># multipath -ll</span></span><br><span class="line">mpathbt (360014059ca317516a69465c883a29603) dm-1 LIO-ORG ,IBLOCK</span><br><span class="line">size=1.0G features=<span class="string">'0'</span> hwhandler=<span class="string">'1 alua'</span> wp=rw</span><br><span class="line">|-+- policy=<span class="string">'queue-length 0'</span> prio=50 status=active</span><br><span class="line">| `- 28:0:0:1 sde  8:64  active ready running</span><br><span class="line">`-+- policy=<span class="string">'queue-length 0'</span> prio=10 status=enabled</span><br><span class="line">  `- 29:0:0:1 sdc  8:32  active ready running</span><br></pre></td></tr></table></figure>
<p>现在你可以像使用多路径iSCSI磁盘一样使用RBD image。  </p>
<h3 id="监视-iSCSI网关"><a href="#监视-iSCSI网关" class="headerlink" title="监视 iSCSI网关"></a>监视 iSCSI网关</h3><p>Ceph为iSCSI网关环境提供了一个额外的工具，用于监视导出的RADOS块设备(RBD) images的性能。  </p>
<p>gwtop工具是一个类似于top的工具，它显示通过iSCSI导出到客户端的RBD images的聚合性能指标。度量来自性能度量域代理(PMDA)。来自Linux-IO target(LIO) PMDA的信息用于列出每个导出的RBD image及其连接的客户机及其关联的I/O指标。  </p>
<p><strong>要求</strong>：  </p>
<ul>
<li>运行中的Ceph iSCSI网关  </li>
</ul>
<p><strong>安装</strong>：  </p>
<ol>
<li>使用root在每个iSCSI网关节点安装<code>ceph-iscsi-tools</code>包：  </li>
</ol>
<blockquote>
<p># yum install ceph-iscsi-tools  </p>
</blockquote>
<ol start="2">
<li>使用root在每个iSCSI网关节点安装<code>co-pilot</code>包：  </li>
</ol>
<blockquote>
<p># yum install pcp</p>
</blockquote>
<ol start="3">
<li>使用root在每个iSCSI网关节点安装LIO PMDA包：  </li>
</ol>
<blockquote>
<p># yum install pcp-pmda-lio  </p>
</blockquote>
<ol start="4">
<li>使用root在每个iSCSI网关节点上启用并开启performance co-pilot服务：  </li>
</ol>
<blockquote>
<p># systemctl enable pmcd<br># systemctl start pmcd  </p>
</blockquote>
<ol start="5">
<li>使用root注册pcp-pmda-lio代理：  </li>
</ol>
<blockquote>
<p>cd /var/lib/pcp/pmdas/lio<br>./Install  </p>
</blockquote>
<p>默认情况下，默认情况下，gwtop假定iSCSI网关配置对象存储在rbd池中名为gateway.conf的RADOS对象中。此配置定义了用于收集性能统计信息的iSCSI网关。这可以通过使用-g或-c标志覆盖。参见gwtop –help 了解更多细节。  </p>
<p>LIO配置决定从性能副驾驶中提取哪种类型的性能统计信息。当gwtop启动时，它查看LIO配置，如果找到用户空间磁盘，则gwtop自动选择LIO收集器。  </p>
<p><strong>“gwtop”输出示例</strong>  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">gwtop  2/2 Gateways   CPU% MIN:  4 MAX:  5    Network Total In:    2M  Out:    3M   10:20:00</span><br><span class="line">Capacity:   8G    Disks:   8   IOPS:  503   Clients:  1   Ceph: HEALTH_OK          OSDs:   3</span><br><span class="line">Pool.Image       Src    Size     iops     rMB/s     wMB/s   Client</span><br><span class="line">iscsi.t1703             500M        0      0.00      0.00</span><br><span class="line">iscsi.testme1           500M        0      0.00      0.00</span><br><span class="line">iscsi.testme2           500M        0      0.00      0.00</span><br><span class="line">iscsi.testme3           500M        0      0.00      0.00</span><br><span class="line">iscsi.testme5           500M        0      0.00      0.00</span><br><span class="line">rbd.myhost_1      T       4G      504      1.95      0.00   rh460p(CON)</span><br><span class="line">rbd.test_2                1G        0      0.00      0.00</span><br><span class="line">rbd.testme              500M        0      0.00      0.00</span><br></pre></td></tr></table></figure>
<p>在客户端列中，(CON)表示iSCSI启动程序(客户端)当前已登录到iSCSI网关。如果显示-multi-，则多个客户机被映射到单个RBD image。</p>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/sheldon_blog/tags/Ceph/" rel="tag"><i class="fa fa-tag"></i> Ceph</a>
            <!--<a href="/sheldon_blog/tags/Ceph/" rel="tag"># Ceph</a>-->
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/sheldon_blog/passages/ceph/" rel="next" title="ceph相关概念">
                <i class="fa fa-chevron-left"></i> ceph相关概念
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="comments">
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/sheldon_blog/images/avatar.jpg" alt="Sheldon Lu">
            
              <p class="site-author-name" itemprop="name">Sheldon Lu</p>
              <div class="site-description motion-element" itemprop="description">静下心来写点东西</div>
          </div>
          
          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/sheldon_blog/archives/">
                
                    <span class="site-state-item-count">24</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/sheldon_blog/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">6</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/sheldon_blog/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">17</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/sheldon-lu" title="GitHub &rarr; https://github.com/sheldon-lu" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
            </div>
          

          <div id="music163player">
            <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="450" src="//music.163.com/outchain/player?type=0&id=2743277027&auto=1&height=430">
            <!--<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=548556767&auto=1&height=66">-->
            </iframe>
          </div>

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#Ceph块设备"><span class="nav-number">1.</span> <span class="nav-text">Ceph块设备</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#基本的块设备命令"><span class="nav-number">2.</span> <span class="nav-text">基本的块设备命令</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#创建块设备资源池"><span class="nav-number">2.1.</span> <span class="nav-text">创建块设备资源池</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#创建块设备用户"><span class="nav-number">2.2.</span> <span class="nav-text">创建块设备用户</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#创建块设备image"><span class="nav-number">2.3.</span> <span class="nav-text">创建块设备image</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#罗列块设备image"><span class="nav-number">2.4.</span> <span class="nav-text">罗列块设备image</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#检索image信息"><span class="nav-number">2.5.</span> <span class="nav-text">检索image信息</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#重置块设备image大小"><span class="nav-number">2.6.</span> <span class="nav-text">重置块设备image大小</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#删除块设备image"><span class="nav-number">2.7.</span> <span class="nav-text">删除块设备image</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#恢复块设备image"><span class="nav-number">2.8.</span> <span class="nav-text">恢复块设备image</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#块设备操作"><span class="nav-number">3.</span> <span class="nav-text">块设备操作</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#快照"><span class="nav-number">3.1.</span> <span class="nav-text">快照</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#CEPHX-笔记"><span class="nav-number">3.1.1.</span> <span class="nav-text">CEPHX 笔记</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#快照基本操作"><span class="nav-number">3.1.2.</span> <span class="nav-text">快照基本操作</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#创建快照"><span class="nav-number">3.1.2.1.</span> <span class="nav-text">创建快照</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#罗列快照"><span class="nav-number">3.1.2.2.</span> <span class="nav-text">罗列快照</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#回滚快照"><span class="nav-number">3.1.2.3.</span> <span class="nav-text">回滚快照</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#删除快照"><span class="nav-number">3.1.2.4.</span> <span class="nav-text">删除快照</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#清空快照"><span class="nav-number">3.1.2.5.</span> <span class="nav-text">清空快照</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#分层"><span class="nav-number">3.1.3.</span> <span class="nav-text">分层</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#分层入门"><span class="nav-number">3.1.3.1.</span> <span class="nav-text">分层入门</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#保护快照"><span class="nav-number">3.1.3.2.</span> <span class="nav-text">保护快照</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#克隆快照"><span class="nav-number">3.1.3.3.</span> <span class="nav-text">克隆快照</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#解保快照"><span class="nav-number">3.1.3.4.</span> <span class="nav-text">解保快照</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#罗列快照的子快照"><span class="nav-number">3.1.3.5.</span> <span class="nav-text">罗列快照的子快照</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#平整克隆的image"><span class="nav-number">3.1.3.6.</span> <span class="nav-text">平整克隆的image</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RBD-镜像"><span class="nav-number">3.2.</span> <span class="nav-text">RBD 镜像</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#资源池配置"><span class="nav-number">3.2.1.</span> <span class="nav-text">资源池配置</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#启用镜像功能"><span class="nav-number">3.2.1.1.</span> <span class="nav-text">启用镜像功能</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#禁用镜像"><span class="nav-number">3.2.1.2.</span> <span class="nav-text">禁用镜像</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#新增集群伙伴"><span class="nav-number">3.2.1.3.</span> <span class="nav-text">新增集群伙伴</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#移除伙伴集群"><span class="nav-number">3.2.1.4.</span> <span class="nav-text">移除伙伴集群</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#数据池"><span class="nav-number">3.2.1.5.</span> <span class="nav-text">数据池</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Image-配置"><span class="nav-number">3.2.2.</span> <span class="nav-text">Image 配置</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#启用image日志支持"><span class="nav-number">3.2.2.1.</span> <span class="nav-text">启用image日志支持</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#启用image镜像"><span class="nav-number">3.2.2.2.</span> <span class="nav-text">启用image镜像</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#禁用image镜像"><span class="nav-number">3.2.2.3.</span> <span class="nav-text">禁用image镜像</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#image-晋级和降级"><span class="nav-number">3.2.2.4.</span> <span class="nav-text">image 晋级和降级</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#强制image同步"><span class="nav-number">3.2.2.5.</span> <span class="nav-text">强制image同步</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#镜像状态"><span class="nav-number">3.2.3.</span> <span class="nav-text">镜像状态</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RBD-mirror-守护进程"><span class="nav-number">3.2.4.</span> <span class="nav-text">RBD-mirror 守护进程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#image实时迁移"><span class="nav-number">3.3.</span> <span class="nav-text">image实时迁移</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#准备迁移"><span class="nav-number">3.3.1.</span> <span class="nav-text">准备迁移</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#执行迁移"><span class="nav-number">3.3.2.</span> <span class="nav-text">执行迁移</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#提交迁移"><span class="nav-number">3.3.3.</span> <span class="nav-text">提交迁移</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#中断迁移"><span class="nav-number">3.3.4.</span> <span class="nav-text">中断迁移</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RBD-持久缓存"><span class="nav-number">3.4.</span> <span class="nav-text">RBD 持久缓存</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#共享的、只读父image缓存"><span class="nav-number">3.4.1.</span> <span class="nav-text">共享的、只读父image缓存</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#启用RBD共享只读父image缓存"><span class="nav-number">3.4.1.1.</span> <span class="nav-text">启用RBD共享只读父image缓存</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#不可变对象缓存守护进程"><span class="nav-number">3.4.2.</span> <span class="nav-text">不可变对象缓存守护进程</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#librbd-配置设置"><span class="nav-number">3.5.</span> <span class="nav-text">librbd 配置设置</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#缓存设置"><span class="nav-number">3.5.1.</span> <span class="nav-text">缓存设置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#预读设置"><span class="nav-number">3.5.2.</span> <span class="nav-text">预读设置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#image-特性"><span class="nav-number">3.5.3.</span> <span class="nav-text">image 特性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#QOS设置"><span class="nav-number">3.5.4.</span> <span class="nav-text">QOS设置</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RBD重播"><span class="nav-number">3.6.</span> <span class="nav-text">RBD重播</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Ceph块设备第三方集成"><span class="nav-number">4.</span> <span class="nav-text">Ceph块设备第三方集成</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#内核模块操作"><span class="nav-number">4.1.</span> <span class="nav-text">内核模块操作</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#获取image列表"><span class="nav-number">4.1.1.</span> <span class="nav-text">获取image列表</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#映射块设备"><span class="nav-number">4.1.2.</span> <span class="nav-text">映射块设备</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#展示已映射的块设备"><span class="nav-number">4.1.3.</span> <span class="nav-text">展示已映射的块设备</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#取消映射"><span class="nav-number">4.1.4.</span> <span class="nav-text">取消映射</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#QEMU-与块设备"><span class="nav-number">4.2.</span> <span class="nav-text">QEMU 与块设备</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#使用指南"><span class="nav-number">4.2.1.</span> <span class="nav-text">使用指南</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用QEMU创建images"><span class="nav-number">4.2.2.</span> <span class="nav-text">使用QEMU创建images</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用QEMU重置image大小"><span class="nav-number">4.2.3.</span> <span class="nav-text">使用QEMU重置image大小</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用QEMU检索image信息"><span class="nav-number">4.2.4.</span> <span class="nav-text">使用QEMU检索image信息</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#使用rbd运行QEMU"><span class="nav-number">4.2.5.</span> <span class="nav-text">使用rbd运行QEMU</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#启用-Discard-Trim"><span class="nav-number">4.2.6.</span> <span class="nav-text">启用 Discard/Trim</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#QEMU缓存选项"><span class="nav-number">4.2.7.</span> <span class="nav-text">QEMU缓存选项</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#libvirt与Ceph-RBD"><span class="nav-number">4.3.</span> <span class="nav-text">libvirt与Ceph RBD</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#配置Ceph"><span class="nav-number">4.3.1.</span> <span class="nav-text">配置Ceph</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#准备VM管理员"><span class="nav-number">4.3.2.</span> <span class="nav-text">准备VM管理员</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#创建一个VM"><span class="nav-number">4.3.3.</span> <span class="nav-text">创建一个VM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#配置VM"><span class="nav-number">4.3.4.</span> <span class="nav-text">配置VM</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#总结"><span class="nav-number">4.3.5.</span> <span class="nav-text">总结</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#块设备与OpenStack"><span class="nav-number">4.4.</span> <span class="nav-text">块设备与OpenStack</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#创建资源池"><span class="nav-number">4.4.1.</span> <span class="nav-text">创建资源池</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#配置OpenStack-Ceph客户端"><span class="nav-number">4.4.2.</span> <span class="nav-text">配置OpenStack Ceph客户端</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#安装Ceph客户端包"><span class="nav-number">4.4.2.1.</span> <span class="nav-text">安装Ceph客户端包</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#安装Ceph-客户端认证"><span class="nav-number">4.4.2.2.</span> <span class="nav-text">安装Ceph 客户端认证</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#配置OpenStack使用Ceph"><span class="nav-number">4.4.3.</span> <span class="nav-text">配置OpenStack使用Ceph</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#配置Glance"><span class="nav-number">4.4.3.1.</span> <span class="nav-text">配置Glance</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#KILO-and-After"><span class="nav-number">4.4.3.1.1.</span> <span class="nav-text">KILO and After</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#启用镜像的写时复制"><span class="nav-number">4.4.3.1.2.</span> <span class="nav-text">启用镜像的写时复制</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#禁用缓存管理（所有OpenStack发行版）"><span class="nav-number">4.4.3.1.3.</span> <span class="nav-text">禁用缓存管理（所有OpenStack发行版）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#镜像属性"><span class="nav-number">4.4.3.1.4.</span> <span class="nav-text">镜像属性</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#配置Cinder"><span class="nav-number">4.4.3.2.</span> <span class="nav-text">配置Cinder</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#配置Cinder备份"><span class="nav-number">4.4.3.3.</span> <span class="nav-text">配置Cinder备份</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#为附加Ceph-RBD块设备配置Nova"><span class="nav-number">4.4.3.4.</span> <span class="nav-text">为附加Ceph RBD块设备配置Nova</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#配置Nova"><span class="nav-number">4.4.3.5.</span> <span class="nav-text">配置Nova</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#重启OpenStack"><span class="nav-number">4.4.3.6.</span> <span class="nav-text">重启OpenStack</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#从块设备中引导启动"><span class="nav-number">4.4.3.7.</span> <span class="nav-text">从块设备中引导启动</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ceph-iSCSI网关"><span class="nav-number">4.5.</span> <span class="nav-text">Ceph iSCSI网关</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ISCSI网关要求"><span class="nav-number">4.5.1.</span> <span class="nav-text">ISCSI网关要求</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ISCSI-Targets"><span class="nav-number">4.5.2.</span> <span class="nav-text">ISCSI Targets</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#使用ansible配置ISCSI-Target"><span class="nav-number">4.5.2.1.</span> <span class="nav-text">使用ansible配置ISCSI Target</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#cd-usr-share-ceph-ansible"><span class="nav-number">5.</span> <span class="nav-text">cd /usr/share/ceph-ansible</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ansible-playbook-site-yml-–limit-iscsigws"><span class="nav-number">6.</span> <span class="nav-text">ansible-playbook site.yml –limit iscsigws</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#gwcli-ls"><span class="nav-number">7.</span> <span class="nav-text">gwcli ls</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#使用命令行接口配置iSCSI-Target"><span class="nav-number">7.0.0.1.</span> <span class="nav-text">使用命令行接口配置iSCSI Target</span></a></li></ol></li></ol><li class="nav-item nav-level-1"><a class="nav-link" href="#yum-install-ceph-iscsi"><span class="nav-number">8.</span> <span class="nav-text">yum install ceph-iscsi</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#yum-install-tcmu-runner"><span class="nav-number">9.</span> <span class="nav-text">yum install tcmu-runner</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ceph-osd-lspools"><span class="nav-number">10.</span> <span class="nav-text">ceph osd lspools</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#touch-etc-ceph-iscsi-gateway-cfg"><span class="nav-number">11.</span> <span class="nav-text">touch /etc/ceph/iscsi-gateway.cfg</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#API-settings"><span class="nav-number">12.</span> <span class="nav-text">API settings.</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#The-API-supports-a-number-of-options-that-allow-you-to-tailor-it-to-your"><span class="nav-number">13.</span> <span class="nav-text">The API supports a number of options that allow you to tailor it to your</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#local-environment-If-you-want-to-run-the-API-under-https-you-will-need-to"><span class="nav-number">14.</span> <span class="nav-text">local environment. If you want to run the API under https, you will need to</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#create-cert-key-files-that-are-compatible-for-each-iSCSI-gateway-node-that-is"><span class="nav-number">15.</span> <span class="nav-text">create cert/key files that are compatible for each iSCSI gateway node, that is</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#not-locked-to-a-specific-node-SSL-cert-and-key-files-must-be-called"><span class="nav-number">16.</span> <span class="nav-text">not locked to a specific node. SSL cert and key files must be called</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#‘iscsi-gateway-crt’-and-‘iscsi-gateway-key’-and-placed-in-the-‘-etc-ceph-‘-directory"><span class="nav-number">17.</span> <span class="nav-text">‘iscsi-gateway.crt’ and ‘iscsi-gateway.key’ and placed in the ‘/etc/ceph/‘ directory</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#on-each-gateway-node-With-the-SSL-files-in-place-you-can-use-‘api-secure-true’"><span class="nav-number">18.</span> <span class="nav-text">on each gateway node. With the SSL files in place, you can use ‘api_secure = true’</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#to-switch-to-https-mode"><span class="nav-number">19.</span> <span class="nav-text">to switch to https mode.</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#To-support-the-API-the-bear-minimum-settings-are"><span class="nav-number">20.</span> <span class="nav-text">To support the API, the bear minimum settings are:</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Additional-API-configuration-options-are-as-follows-defaults-shown"><span class="nav-number">21.</span> <span class="nav-text">Additional API configuration options are as follows, defaults shown.</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#api-user-admin"><span class="nav-number">22.</span> <span class="nav-text">api_user = admin</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#api-password-admin"><span class="nav-number">23.</span> <span class="nav-text">api_password = admin</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#api-port-5001"><span class="nav-number">24.</span> <span class="nav-text">api_port = 5001</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#trusted-ip-list-192-168-0-10-192-168-0-11"><span class="nav-number">25.</span> <span class="nav-text">trusted_ip_list = 192.168.0.10,192.168.0.11</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#监视-iSCSI网关"><span class="nav-number">25.0.1.</span> <span class="nav-text">监视 iSCSI网关</span></a></li></ol></li></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  


        
        <script>
//暂时储存文章中的内容
var div = $('.post-body');
//暂时储存目录的内容
var toc=$('.post-toc-wrap')
function password() {
  if(''){
  //将文章内容删除
    div.remove();
 //将目录删除 
    toc.remove();
  //将文章删除后，向原来文章的地方添加，应该出现的提示用户输入密码的样式
  //下面这里的第一个用textarea是因为如果在手机端的时候只能显示一部分文字，
  //只是拓展:input里面的字只能显示一行，不会自动换行，目前上网搜索没有发现好的办法，所以用了textarea，右下角的小三角通过resize:none 去掉。
   $('.post-header').after('<textarea class="description" value="Please enter your password and press enter to build" style="border: none;display: block;' +'width: 60%;margin: 0 auto;text-align: center;outline: none;margin-bottom: 50px;resize:none ">
      Please enter your password and press enter to build</textarea>' +
      '<div class="qiang" style="height: 100px;width: 60%;margin:0 auto">' +
      '<input class="password"  type="text" value="" style="border: none;display: block;border-bottom: 1px solid #ccc;' +
      'margin: 0 auto;outline: none;width:95%"/>' +
      '</div>')
      //绑定点击事件，如果是点击的.password 这个div就改变样式，如果是document中除了div之外的其他任何元素，就变回原来的样式。
    document.onclick = function (event) {
      var e = event || window.event;
      var elem = e.srcElement || e.target;

      while (elem) {
        if (elem != document) {
          if (elem.className == "password") {
            $(".password").animate({paddingTop:"30px",width:"100%",borderWidth:"2px"},300)
            return;
          }
          elem = elem.parentNode;
        } else {
          $(".password").animate({paddingTop:"0px",width:"95%",borderWidth:"1px"},300)
          return;
        }
      }
    }
    //绑定enter键按下后离开的事件
    $(document).keyup(function(event){
      if(event.keyCode ==13&&$('.password').length>0){
        //console.log($('.password').val())
        //console.log('')
        if ($('.password').val() == '') {
        //恢复文章内容
          (div).appendTo($(".post-header"))
          //恢复目录
          toc.appendTo($(".sidebar-inner"))
                 //删除本页面的输入密码组件
           $(".description").remove();
          $(".qiang").remove();
          $(".password").remove();
          //重新处理pjax事件,如果没有加pjax的从下面这行起到下面的else之间的代码需要去掉。
          //图片懒加载，没有加入此功能的这个函数需要去掉
	          $('img').lazyload({
	             placeholder: '../images/loading.gif',
	             effect: 'fadeIn',
	             threshold : 100,
	             failure_limit : 20,
	             skip_invisible : false
	           });
	           //pjax后出现文章不显示，没有pjax的下面四行需要去掉
	            $(".post-block").css({opacity:1});
	            $(".post-header").css({opacity:1});
	            $(".post-body").css({opacity:1});
	            $(".pagination").css({opacity:1});
        }else {
          alert("Sorry, the password is wrong.")
        }
      }
      //将document的keyup移除，防止在pjax的情况下会重复绑定事件
    });
  }
}
password();
</script>
      </div>
      <script>
//暂时储存文章中的内容
var div = $('.post-body');
//暂时储存目录的内容
var toc=$('.post-toc-wrap')
function password() {
  if(''){
  //将文章内容删除
    div.remove();
 //将目录删除 
    toc.remove();
  //将文章删除后，向原来文章的地方添加，应该出现的提示用户输入密码的样式
  //下面这里的第一个用textarea是因为如果在手机端的时候只能显示一部分文字，
  //只是拓展:input里面的字只能显示一行，不会自动换行，目前上网搜索没有发现好的办法，所以用了textarea，右下角的小三角通过resize:none 去掉。
   $('.post-header').after('<textarea class="description" value="Please enter your password and press enter to build" style="border: none;display: block;' +'width: 60%;margin: 0 auto;text-align: center;outline: none;margin-bottom: 50px;resize:none ">
      Please enter your password and press enter to build</textarea>' +
      '<div class="qiang" style="height: 100px;width: 60%;margin:0 auto">' +
      '<input class="password"  type="text" value="" style="border: none;display: block;border-bottom: 1px solid #ccc;' +
      'margin: 0 auto;outline: none;width:95%"/>' +
      '</div>')
      //绑定点击事件，如果是点击的.password 这个div就改变样式，如果是document中除了div之外的其他任何元素，就变回原来的样式。
    document.onclick = function (event) {
      var e = event || window.event;
      var elem = e.srcElement || e.target;

      while (elem) {
        if (elem != document) {
          if (elem.className == "password") {
            $(".password").animate({paddingTop:"30px",width:"100%",borderWidth:"2px"},300)
            return;
          }
          elem = elem.parentNode;
        } else {
          $(".password").animate({paddingTop:"0px",width:"95%",borderWidth:"1px"},300)
          return;
        }
      }
    }
    //绑定enter键按下后离开的事件
    $(document).keyup(function(event){
      if(event.keyCode ==13&&$('.password').length>0){
        //console.log($('.password').val())
        //console.log('')
        if ($('.password').val() == '') {
        //恢复文章内容
          (div).appendTo($(".post-header"))
          //恢复目录
          toc.appendTo($(".sidebar-inner"))
                 //删除本页面的输入密码组件
           $(".description").remove();
          $(".qiang").remove();
          $(".password").remove();
          //重新处理pjax事件,如果没有加pjax的从下面这行起到下面的else之间的代码需要去掉。
          //图片懒加载，没有加入此功能的这个函数需要去掉
	          $('img').lazyload({
	             placeholder: '../images/loading.gif',
	             effect: 'fadeIn',
	             threshold : 100,
	             failure_limit : 20,
	             skip_invisible : false
	           });
	           //pjax后出现文章不显示，没有pjax的下面四行需要去掉
	            $(".post-block").css({opacity:1});
	            $(".post-header").css({opacity:1});
	            $(".post-body").css({opacity:1});
	            $(".pagination").css({opacity:1});
        }else {
          alert("Sorry, the password is wrong.")
        }
      }
      //将document的keyup移除，防止在pjax的情况下会重复绑定事件
    });
  }
}
password();
</script>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <!--<i class="fa fa-user"></i>-->
    <i class="fa fa-heart" aria-hidden="true"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Sheldon Lu</span>
  
  

  
</div>

<!--

  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a><span>　<i class="fa fa-bomb"></i></span>
  <span id="showDays"></span>
  </div>

-->
<span><i class="fa fa-bomb"></i></span>
<span id="showDays"></span>

<div>
<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<!--<span id="busuanzi_container_site_pv" style='display:none'>
    本站总访问量 <span id="busuanzi_value_site_pv"></span> 次
    <span class="post-meta-divider">|</span>
</span>-->
<span id="busuanzi_container_site_uv" style="display:none">
    有<span id="busuanzi_value_site_uv"></span>人看过我的博客啦
</span>
</div>



<script>
  var seconds = 1000;
  var minutes = seconds * 60;
  var hours = minutes * 60;
  var days = hours * 24;
  var years = days * 365;
  var birthDay = Date.UTC(2019,03,15,00,00,00); // 这里设置建站时间
  setInterval(function() {
    var today = new Date();
    var todayYear = today.getFullYear();
    var todayMonth = today.getMonth()+1;
    var todayDate = today.getDate();
    var todayHour = today.getHours();
    var todayMinute = today.getMinutes();
    var todaySecond = today.getSeconds();
    var now = Date.UTC(todayYear,todayMonth,todayDate,todayHour,todayMinute,todaySecond);
    var diff = now - birthDay;
    var diffYears = Math.floor(diff/years);
    var diffDays = Math.floor((diff/days)-diffYears*365);
    var diffHours = Math.floor((diff-(diffYears*365+diffDays)*days)/hours);
    var diffMinutes = Math.floor((diff-(diffYears*365+diffDays)*days-diffHours*hours)/minutes);
    var diffSeconds = Math.floor((diff-(diffYears*365+diffDays)*days-diffHours*hours-diffMinutes*minutes)/seconds);
      document.getElementById('showDays').innerHTML="本站已运行 "+diffYears+" 年 "+diffDays+" 天 "+diffHours+" 小时 "+diffMinutes+" 分钟 "+diffSeconds+" 秒";
  }, 1000);
</script>
        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>














  
    
    
  
  <script color="0,0,255" opacity="0.5" zindex="-1" count="99" src="/sheldon_blog/lib/canvas-nest/canvas-nest.min.js"></script>













  
  <script src="/sheldon_blog/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/sheldon_blog/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/sheldon_blog/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/sheldon_blog/js/src/utils.js?v=7.0.1"></script>

  <script src="/sheldon_blog/js/src/motion.js?v=7.0.1"></script>



  
  


  <script src="/sheldon_blog/js/src/schemes/muse.js?v=7.0.1"></script>




  
  <script src="/sheldon_blog/js/src/scrollspy.js?v=7.0.1"></script>
<script src="/sheldon_blog/js/src/post-details.js?v=7.0.1"></script>



  


  <script src="/sheldon_blog/js/src/next-boot.js?v=7.0.1"></script>


  

  

  

  
  

<script src="//cdn1.lncld.net/static/js/3.11.1/av-min.js"></script>



<script src="//unpkg.com/valine/dist/Valine.min.js"></script>

<script>
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: 'wcykO4AM4RdvFXlzAJAOVgGz-gzGzoHsz',
    appKey: '1okhim5n5Pq6KwHuFKmTRC9t',
    placeholder: 'Just go go',
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: false,
    lang: '' || 'zh-cn'
  });
</script>




  


  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/sheldon_blog/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  
  
  <script>
    
    function addCount(Counter) {
      var $visitors = $('.leancloud_visitors');
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();

      Counter('get', '/classes/Counter', { where: JSON.stringify({ url }) })
        .done(function({ results }) {
          if (results.length > 0) {
            var counter = results[0];
            
            Counter('put', '/classes/Counter/' + counter.objectId, JSON.stringify({ time: { '__op': 'Increment', 'amount': 1 } }))
            
              .done(function() {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.time + 1);
              })
            
              .fail(function ({ responseJSON }) {
                console.log('Failed to save Visitor num, with error message: ' + responseJSON.error);
              })
          } else {
            
              Counter('post', '/classes/Counter', JSON.stringify({ title: title, url: url, time: 1 }))
                .done(function() {
                  var $element = $(document.getElementById(url));
                  $element.find('.leancloud-visitors-count').text(1);
                })
                .fail(function() {
                  console.log('Failed to create');
                });
            
          }
        })
        .fail(function ({ responseJSON }) {
          console.log('LeanCloud Counter Error: ' + responseJSON.code + ' ' + responseJSON.error);
        });
    }
    

    $(function() {
      $.get('https://app-router.leancloud.cn/2/route?appId=' + 'wcykO4AM4RdvFXlzAJAOVgGz-gzGzoHsz')
        .done(function({ api_server }) {
          var Counter = function(method, url, data) {
            return $.ajax({
              method: method,
              url: 'https://' + api_server + '/1.1' + url,
              headers: {
                'X-LC-Id': 'wcykO4AM4RdvFXlzAJAOVgGz-gzGzoHsz',
                'X-LC-Key': '1okhim5n5Pq6KwHuFKmTRC9t',
                'Content-Type': 'application/json',
              },
              data: data
            });
          };
          
            addCount(Counter);
          
        });
    });
  </script>



  

  

  

  

  

  

  

  

  

  

  

  

  

</body>
</html>
