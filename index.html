<!--崩溃欺骗-->
<script type="text/javascript" src="/js/src/crash_cheat.js"></script>
<!DOCTYPE html>












  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
















  <meta name="baidu-site-verification" content="pM3LIEUO3X">









<link rel="stylesheet" href="/sheldon_blog/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/sheldon_blog/css/main.css?v=7.0.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/sheldon_blog/images/apple-touch-icon-next.png?v=7.0.1">


  <link rel="icon" type="image/png" sizes="32x32" href="/sheldon_blog/images/favicon-32x32-next.png?v=7.0.1">


  <link rel="icon" type="image/png" sizes="16x16" href="/sheldon_blog/images/favicon-16x16-next.png?v=7.0.1">


  <link rel="mask-icon" href="/sheldon_blog/images/logo.svg?v=7.0.1" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/sheldon_blog/',
    scheme: 'Mist',
    version: '7.0.1',
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false,"dimmer":false},
    back2top: true,
    back2top_sidebar: false,
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="静下心来写点东西">
<meta name="keywords" content="sheldon">
<meta property="og:type" content="website">
<meta property="og:title" content="Sheldon_Lu">
<meta property="og:url" content="https://sheldon-lu.github.io/index.html">
<meta property="og:site_name" content="Sheldon_Lu">
<meta property="og:description" content="静下心来写点东西">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Sheldon_Lu">
<meta name="twitter:description" content="静下心来写点东西">





  
  
  <link rel="canonical" href="https://sheldon-lu.github.io/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>Sheldon_Lu</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/sheldon_blog/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Sheldon_Lu</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/sheldon_blog/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-about">

    
    
    
      
    

    

    <a href="/sheldon_blog/about/" rel="section"><i class="menu-item-icon fa fa-fw fa-user"></i> <br>About</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-tags">

    
    
    
      
    

    

    <a href="/sheldon_blog/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>Tags</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-categories">

    
    
    
      
    

    

    <a href="/sheldon_blog/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>Categories</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/sheldon_blog/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>

      
      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>Search</a>
        </li>
      
    </ul>
  

  
    

  

  
    <div class="site-search">
      
  <div class="popup search-popup local-search-popup">
  <div class="local-search-header clearfix">
    <span class="search-icon">
      <i class="fa fa-search"></i>
    </span>
    <span class="popup-btn-close">
      <i class="fa fa-times-circle"></i>
    </span>
    <div class="local-search-input-wrapper">
      <input autocomplete="off" placeholder="Searching..." spellcheck="false" type="text" id="local-search-input">
    </div>
  </div>
  <div id="local-search-result"></div>
</div>



    </div>
  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://sheldon-lu.github.io/sheldon_blog/passages/安静写点东西/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Sheldon Lu">
      <meta itemprop="description" content="静下心来写点东西">
      <meta itemprop="image" content="/sheldon_blog/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sheldon_Lu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/sheldon_blog/passages/安静写点东西/" class="post-title-link" itemprop="url">安静写点东西</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <i class="fa fa-thumb-tack"></i>
              <font color="green">置顶</font>
              <span class="post-meta-divider">|</span>
            

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-04-06 20:09:17 / Modified: 21:06:25" itemprop="dateCreated datePublished" datetime="2019-04-06T20:09:17+08:00">2019-04-06</time>
            

            
              

              
            
          </span>

          

          
            
            
              
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">Comments: </span>
                <a href="/sheldon_blog/passages/安静写点东西/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/sheldon_blog/passages/安静写点东西/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/sheldon_blog/passages/安静写点东西/" class="leancloud_visitors" data-flag-title="安静写点东西">
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">Views: </span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">3.2k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">3 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          Welcome to my blog, Write whatever you want.
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/sheldon_blog/passages/安静写点东西/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://sheldon-lu.github.io/sheldon_blog/passages/block/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Sheldon Lu">
      <meta itemprop="description" content="静下心来写点东西">
      <meta itemprop="image" content="/sheldon_blog/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sheldon_Lu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/sheldon_blog/passages/block/" class="post-title-link" itemprop="url">ceph-block</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-09-02 21:34:50" itemprop="dateCreated datePublished" datetime="2019-09-02T21:34:50+08:00">2019-09-02</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-09-03 22:46:01" itemprop="dateModified" datetime="2019-09-03T22:46:01+08:00">2019-09-03</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/sheldon_blog/categories/Ceph/" itemprop="url" rel="index"><span itemprop="name">Ceph</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">Comments: </span>
                <a href="/sheldon_blog/passages/block/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/sheldon_blog/passages/block/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/sheldon_blog/passages/block/" class="leancloud_visitors" data-flag-title="ceph-block">
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">Views: </span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">51k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">47 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Ceph块设备"><a href="#Ceph块设备" class="headerlink" title="Ceph块设备"></a>Ceph块设备</h1><p>块是一个字节序列（例如，一个512字节的数据块）。基于块的存储接口是最常见的存储数据的方法，它通常基于旋转介质，像硬盘、CD、软盘，甚至传统的9道磁带。  </p>
<h1 id="基本的块设备命令"><a href="#基本的块设备命令" class="headerlink" title="基本的块设备命令"></a>基本的块设备命令</h1><p>rbd命令可以让你创建、罗列、审查和删除块设备image。你也可以用它来克隆image、创建快照、回滚快照、查看快照等。关于rbd命令使用细节，可查看<a href="https://docs.ceph.com/docs/master/man/8/rbd/" target="_blank" rel="noopener"><font color="red">RBD - Manage RADOS Block Device(RBD) Images</font></a>来了解详情。  </p>
<h2 id="创建块设备资源池"><a href="#创建块设备资源池" class="headerlink" title="创建块设备资源池"></a>创建块设备资源池</h2><ol>
<li>在管理节点，使用ceph工具<font color="red">创建一个资源池</font>。</li>
<li>在管理节点，使用rbd工具RBD来初始化资源池：</li>
</ol>
<blockquote>
<p>rbd pool init \<pool-name></pool-name></p>
</blockquote>
<h2 id="创建块设备用户"><a href="#创建块设备用户" class="headerlink" title="创建块设备用户"></a>创建块设备用户</h2><p>除非另有说明，否则rbd命令将使用管理员ID访问Ceph集群。此ID允许对集群进行完全的管理访问。建议尽可能使用限制更严格的用户。  </p>
<p><font color="red">创建Ceph用户</font>，可以使用Ceph命令<code>auth get-or-create</code>，需要提供用户名称、监视器和OSD：</p>
<blockquote>
<p>ceph auth get-or-create client.{ID} mon ‘profile rbd’ osd ‘profile {profile name} [pool={pool-name}] [, profile …]’  </p>
</blockquote>
<p>例如，创建一个命名称为qemu用户ID，拥有读写权限访问vms资源池，只读images资源池，执行下列命令：  </p>
<blockquote>
<p>ceph auth get-or-create client.qemu mon ‘profile rbd’ osd ‘profile rbd pool=vms,profile rbd-read-only pool=images’</p>
</blockquote>
<p>ceph auth get-or-create 命令的输出是新增用户的密钥环，可以写入到<em>/etc/ceph/cepn.client.{ID}.keyring</em>文件中。  </p>
<h2 id="创建块设备image"><a href="#创建块设备image" class="headerlink" title="创建块设备image"></a>创建块设备image</h2><p>在你将块设备挂载到节点之前，你必须先在Ceph存储集群中创建一个image。可以使用以下命令创建块设备image：  </p>
<blockquote>
<p>rbd create –size {megabytes} {pool-name}/{image-name}</p>
</blockquote>
<p>例如，在名称为<code>swimmingpool</code>的资源池中创建一个名称为<code>bar</code>的1Gimage来存储消息，执行以下命令：  </p>
<blockquote>
<p>rbd create –size 1024 swimmingpool/bar</p>
</blockquote>
<p>如果在创建image时你不指明资源池，image将被存储在默认的资源池<code>rbd</code>中。例如，在默认的资源池<code>rbd</code>中创建一个名称为<code>foo</code>的1G大小的image：  </p>
<blockquote>
<p>rbd create –size 1024 foo</p>
</blockquote>
<h2 id="罗列块设备image"><a href="#罗列块设备image" class="headerlink" title="罗列块设备image"></a>罗列块设备image</h2><p>罗列<code>rbd</code>资源池中的块设备image，可以执行以下命令：  </p>
<blockquote>
<p>rbd ls</p>
</blockquote>
<p>罗列指定资源池中的块设备image，可以执行以下命令，但需要将<code>{poolname}</code>替换为指定的资源池名称：  </p>
<blockquote>
<p>rbd ls {poolname}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd ls swimmingpool  </p>
</blockquote>
<p>罗列<code>rbd</code>资源池回收站中的待删除的块设备，执行以下命令：  </p>
<blockquote>
<p>rbd trash ls</p>
</blockquote>
<p>罗列指定资源池回收站中待删除的块设备，可以执行以下命令，但需要将<code>{poolname}</code>替换为指定的资源池名称：  </p>
<blockquote>
<p>rbd trash ls {poolname}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd trash ls swimming</p>
</blockquote>
<h2 id="检索image信息"><a href="#检索image信息" class="headerlink" title="检索image信息"></a>检索image信息</h2><p>检索指定image的信息，可以执行以下命令，但需要将<code>{image-name}</code>替换为指定的image名称：</p>
<blockquote>
<p>rbd info {image-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd info foo  </p>
</blockquote>
<p>检索指定资源池中image的信息，可以执行以下命令，但需要将<code>{image-name}</code>替换成image名称，<code>{pool-name}</code>替换成资源池名称：  </p>
<blockquote>
<p>rbd info {pool-name}/{image-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd info swimmingpool/bar</p>
</blockquote>
<h2 id="重置块设备image大小"><a href="#重置块设备image大小" class="headerlink" title="重置块设备image大小"></a>重置块设备image大小</h2><p><font color="red">Ceph块设备</font>是精简配置的。在你开始在其上保存数据之前它们不会占用任何物理存储。但是它们确实具有你使用<code>--size</code>操作指定的最大容量。如果你想增加（或者减小）Ceph块设备的最大尺寸，可以执行以下命令：  </p>
<blockquote>
<p>rbd resize –size 2048 foo (to increase)<br>rbd resize –size 2048 foo –allow-shrink (to decrease)</p>
</blockquote>
<h2 id="删除块设备image"><a href="#删除块设备image" class="headerlink" title="删除块设备image"></a>删除块设备image</h2><p>删除块设备image，可以使用以下命令，但需要将<code>{image-name}</code>替换为你想要删除的image名称：  </p>
<blockquote>
<p>rbd rm {image-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd rm foo</p>
</blockquote>
<p>删除指定资源池中的块设备，可以执行以下命令，但需要将<code>{image-name}</code>替换成你想要删除的image名称，<code>{pool-name}</code>替换成资源池名称：  </p>
<blockquote>
<p>rbd rm {pool-name}/{image-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd rm swimming/bar</p>
</blockquote>
<p>要从资源池回收站中移除块设备，可以执行以下命令，但需要将<code>{image-name}</code>替换成你想要删除的image名称，<code>{pool-name}</code>替换成资源池名称：  </p>
<blockquote>
<p>rbd trash mv {pool-name}/{image-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd trash mv swimming/bar</p>
</blockquote>
<p>要从资源池回收站中删除块设备，可以执行以下命令，但需要将<code>{image-id}</code>替换成你想要删除的image的id，<code>{pool-name}</code>替换成资源池名称：  </p>
<blockquote>
<p>rbd trash rm {pool-name}/{image-id}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd trash rm swimming/2bf4474bodc51</p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Note:</span><br><span class="line">    · 你可以将image移动到回收站中，即使它拥有快照或其副本正在使用中，但是不能将它从回收站中删除。</span><br><span class="line">    · 你可以使用 --expires-at 来设置滞留时间（默认为当前时间），如果尚未到达滞留时间，除非你使用 --force，否则它不会被删除</span><br></pre></td></tr></table></figure>
<h2 id="恢复块设备image"><a href="#恢复块设备image" class="headerlink" title="恢复块设备image"></a>恢复块设备image</h2><p>恢复<code>rbd</code>资源池中待删除的块设备，可以使用以下命令，但需要将<code>{image-id}</code>替换为imageid：  </p>
<blockquote>
<p>rbd trash restore {image-id}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd trash restore 2bf4474b0dc51</p>
</blockquote>
<p>恢复指定资源池中待删除的块设备，可以使用以下命令，但需要将<code>{image-id}</code>替换为imageid，<code>{pool-name}</code>替换为资源池名称：  </p>
<blockquote>
<p>rbd trash restore {pool-name}/{image-id}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd trash restore swimming/2bf4474b0dc51  </p>
</blockquote>
<p>你也可以恢复image时使用 –image 来重命名称它：  </p>
<p>例如：  </p>
<blockquote>
<p>rbd trash restore swimming/2bf4474b0dc51 –image new-name  </p>
</blockquote>
<h1 id="块设备操作"><a href="#块设备操作" class="headerlink" title="块设备操作"></a>块设备操作</h1><h2 id="快照"><a href="#快照" class="headerlink" title="快照"></a>快照</h2><p>快照是指定时间点image状态的一种只读拷贝。Ceph块设备的一种高级操作是你可以闯将一个image的快照来保存image状态的历史记录。Ceph还支持快照分层，允许你快速轻松地克隆image（例如，VMimage）。Ceph使用rbd命令和更高级的接口，包括<font color="red">QEMU</font>，<font color="red">libvirt</font>，<font color="red">OpenStack</font>和<font color="red">CloudStack</font>，来支持快照。  </p>
<h3 id="CEPHX-笔记"><a href="#CEPHX-笔记" class="headerlink" title="CEPHX 笔记"></a>CEPHX 笔记</h3><p>当cephx启用（默认启用）时，则你必须指定一个用户名称或ID以及包含有用户对应的密钥的密钥环的路径。详情参见<a href="https://docs.ceph.com/docs/master/rados/operations/user-management/#user-management" target="_blank" rel="noopener"><font color="red">用户管理</font></a>。你也可以将CEPH_ARGS添加到环境变量中来避免重复输入以下参数：  </p>
<blockquote>
<p>rbd –id {user-ID} –keyring=/path/to/secret [commands]<br>rbd –name {username} –keyring=/path/to/secret [commands]  </p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd –id admin –keyring=/etc/ceph/ceph.keyring [commands]<br>rbd –name client.admin –keyring=/etc/ceph/ceph.keyring [commands]</p>
</blockquote>
<h3 id="快照基本操作"><a href="#快照基本操作" class="headerlink" title="快照基本操作"></a>快照基本操作</h3><p>接下来介绍如何在命令行下使用rbd命令来创建、罗列和删除快照。  </p>
<h4 id="创建快照"><a href="#创建快照" class="headerlink" title="创建快照"></a>创建快照</h4><p>使用rbd创建快照，snap create额外选项包括资源池名称和image名称。  </p>
<blockquote>
<p>rbd snap create {pool-name}/{image-name}@{snap-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd snap create rbd/foo@snamname</p>
</blockquote>
<h4 id="罗列快照"><a href="#罗列快照" class="headerlink" title="罗列快照"></a>罗列快照</h4><p>要罗列image的快照，需要额外指定资源池名称和image名称：  </p>
<blockquote>
<p>rbd snap ls {pool-name}/{image-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd snap ls rbd/foo</p>
</blockquote>
<h4 id="回滚快照"><a href="#回滚快照" class="headerlink" title="回滚快照"></a>回滚快照</h4><p>使用rbd回滚快照，snap rollback操作需要额外指明资源池名称、image名称和快照名称：  </p>
<blockquote>
<p>rbd snap rollback {pool-name}/{image-name}@{snap-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd snap rollback rbd/foo@snapname</p>
</blockquote>
<h4 id="删除快照"><a href="#删除快照" class="headerlink" title="删除快照"></a>删除快照</h4><p>使用rbd删除快照，snap rm操作需要额外指明资源池名称、image名称和快照名称：  </p>
<blockquote>
<p>rbd snap rm {pool-name}/{image-name}@{snap-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd snap rm rbd/foo@snapname</p>
</blockquote>
<h4 id="清空快照"><a href="#清空快照" class="headerlink" title="清空快照"></a>清空快照</h4><p>使用rbd删除image的所有快照，snap purge操作需要额外指明资源池名称、image名称：  </p>
<blockquote>
<p>rbd snap purge {pool-name}/{image-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd snap purge rbd/foo</p>
</blockquote>
<h3 id="分层"><a href="#分层" class="headerlink" title="分层"></a>分层</h3><p>Ceph支持创建多个块设备快照的写时复制克隆。快照分层可以使Ceph块设备客户端快速地创建image。例如，你可以创建一个写有Linux VM的块设备image；然后，快照image，保护快照，创建任意数量的写时复制克隆体。因为快照是只读的，克隆快照简化了语义，使得快速创建克隆体成为可能。  </p>
<p><img src="../images/snapshot_clone.png" alt="snapshot clone">  </p>
<p>每个克隆的image（子）都存储着父image的引用，这使得克隆的image可以打开父快照并读取它。  </p>
<p>快照的写时复制克隆行为跟其它的Ceph块设备image一样。你可以读、写、克隆和重设克隆的image的大小。克隆的image没有特殊的限制。然而，快照的写时复制克隆指的是快照，所以在你克隆快照之前你必须保护它。下面的示意图描述的就是这个过程。  </p>
<h4 id="分层入门"><a href="#分层入门" class="headerlink" title="分层入门"></a>分层入门</h4><p>Ceph块设备分层是个简单的操作。你必须有一个image，必须创建这个image的快照，必须保护这个快照。一旦已完成了这些步骤，你就可以开始克隆快照了。  </p>
<p><img src="../images/clone_a_snapshot.png" alt="clone a snapshot"></p>
<p>克隆的image保留了父快照的引用，并包含了资源池ID、imageID和快照ID。包含资源池ID意味着可以将快照从一个资源池克隆到另一个资源池中的image。  </p>
<ol>
<li><strong>Image Template</strong>：块设备分层的一个常见的使用场景是创建一个主image和一个快照，作为克隆的模板。例如，用户可能创建一个Linux发行版（例如，Ubuntu 12.04）的image，并为它创建快照。用户可能会周期性地更新image并创建新的快照（例如，<code>sudo apt-get update</code>，<code>sudo apt-get upgrade</code>，<code>sudo apt-get dist-upgrade</code>之后使用<code>rbd snap create</code>创建新的快照）。随着image的完善，用户可以克隆快照中的任何一个。</li>
</ol>
<ol start="2">
<li><strong>Extended Template</strong>：更高级的使用场景是包含扩展模板image，提供比基础image更多的信息。例如，用户可能克隆一个image（例如，VM模板）并且安装其它的软件（例如，数据库，内容管理系统，分析系统），然后快照扩展后的image，它本身可以像基础image一样被更新。  </li>
</ol>
<ol start="3">
<li><strong>Template Pool</strong>：使用块设备分层的一个方法是创建一个资源池，其中包括作为模板的主image以及那些模板的快照。然后你可以将制度权限扩展到其他用户，这样他们就可以克隆快照，但不能在资源池中写入和执行。  </li>
</ol>
<ol start="4">
<li><strong>Image Migration/Recovery</strong>：使用块设备分层的一个方法从一个资源池迁移或恢复到另一个资源池。  </li>
</ol>
<h4 id="保护快照"><a href="#保护快照" class="headerlink" title="保护快照"></a>保护快照</h4><p>克隆体可以访问父快照。如果用户不小心删除了父快照，那么所有的克隆体都将损坏。为了防止数据丢失，在你克隆快照前你<strong>必须</strong>保护它。  </p>
<blockquote>
<p>rbd snap protect {pool-name}/{image-name}@{snapshot-name}  </p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd snap protect rbd/my-image@my-snapshot  </p>
</blockquote>
<h4 id="克隆快照"><a href="#克隆快照" class="headerlink" title="克隆快照"></a>克隆快照</h4><p>要克隆快照，你需要额外说明的信息有父资源池、image和快照，以及子资源池和image名称称。在你克隆快照之前你<strong>必须</strong>先保护它。</p>
<blockquote>
<p>rbd clone {pool-name}/{parent-image}@{snap-name} {pool-name}/{child-image-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd clone rbd/my-image@my-snapshot rbd/new-image</p>
</blockquote>
<h4 id="解保快照"><a href="#解保快照" class="headerlink" title="解保快照"></a>解保快照</h4><p>在你删除快照之前，你必选先接触保护。另外，你<strong>不能</strong>删除具有克隆体引用的快照。在你删除快照之前，你<strong>必须</strong>平整该快照的所有克隆。</p>
<blockquote>
<p>rbd snap unprotect {pool-name}/{image-name}@{snapshot-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd snap unprotect rbd/my-image@my-snapshot</p>
</blockquote>
<h4 id="罗列快照的子快照"><a href="#罗列快照的子快照" class="headerlink" title="罗列快照的子快照"></a>罗列快照的子快照</h4><p>要罗列一个快照的子快照，可以执行以下命令：  </p>
<blockquote>
<p>rbd children {pool-name}/{image-name}@{snapshot-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd childre rbd/my-image@my-snapshot  </p>
</blockquote>
<h4 id="平整克隆的image"><a href="#平整克隆的image" class="headerlink" title="平整克隆的image"></a>平整克隆的image</h4><p>克隆的image保留了到父快照的引用。当你移除子克隆体到父快照的引用时，通过从快照拷贝信息到克隆体，你可以高效地“平整”image。平整image花费的时间随着快照体积的增大而增大。要想删除快照，你必须先平整它的子image。  </p>
<blockquote>
<p>rbd flatten {pool-name}/{image-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>rbd flatten rbd/new-image  </p>
</blockquote>
<h2 id="RBD-镜像"><a href="#RBD-镜像" class="headerlink" title="RBD 镜像"></a>RBD 镜像</h2><p>RBDimage可以在两个Ceph集群间异步备份。该能力利用了RBD日志image特性来保证集群间的crash-consistent复制。镜像功能需要在同伴集群中的每一个对应的pool上进行配置，可设定自动备份某个存储池内的所有images或仅备份images的一个特定子集。用rbd命令来配置镜像功能。rbd-mirror守护进程负责从远端集群拉取image的更新，并写入本地集群的对应image中。  </p>
<p>根据复制的需要，RBD镜像可以配置为单向或者双向复制：  </p>
<ul>
<li><strong>单向复制</strong>：当数据仅从主集群镜像到从集群时，rbd-mirror守护进程只运行在从集群。</li>
<li><strong>双向复制</strong>：当数据从一个集群上的主映像镜像到另一个集群上的非主映像(反之亦然)时，rd -mirror守护进程在两个集群上运行。  </li>
</ul>
<h3 id="资源池配置"><a href="#资源池配置" class="headerlink" title="资源池配置"></a>资源池配置</h3><p>下面的程序说明如何使用rbd命令执行基本的管理任务来配置镜像功能。镜像功能需要在同伴集群中的每一个对应的pool上进行配置。  </p>
<p>资源池的配置操作应在所有的同伴集群上执行。为了清晰起见，这些过程假设可以从单个主机访问两个集群，分别称为“local”和“remote”。  </p>
<p>有关如何连接到不同Ceph集群的详细信息，请参阅<a href="https://docs.ceph.com/docs/master/man/8/rbd" target="_blank" rel="noopener"><font color="red">rbd</font></a>手册页。  </p>
<h4 id="启用镜像功能"><a href="#启用镜像功能" class="headerlink" title="启用镜像功能"></a>启用镜像功能</h4><p>使用rbd启用镜像，需要<code>mirror pool enable</code>命令，指明资源池名称和镜像模式：  </p>
<blockquote>
<p>rbd mirror pool enable {pool-name} {mode}</p>
</blockquote>
<p>镜像模式可以是pool或image：  </p>
<ul>
<li><strong>pool</strong>：当配置为pool模式时，带有日志特性的资源池中的所有image都将被镜像。</li>
<li><strong>image</strong>：当配置为image模式时，每个image的镜像功能都需要被<a href="https://docs.ceph.com/docs/master/rbd/rbd-mirroring/#enable-image-mirroring" target="_blank" rel="noopener"><font color="red">显示启用</font></a>。</li>
</ul>
<p>例如：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ rbd --cluster <span class="built_in">local</span> mirror pool <span class="built_in">enable</span> image-pool pool</span><br><span class="line">$ rbd --cluster remote mirror pool enabel image-pool pool</span><br></pre></td></tr></table></figure>
<h4 id="禁用镜像"><a href="#禁用镜像" class="headerlink" title="禁用镜像"></a>禁用镜像</h4><p>使用rbd禁用镜像，需要额外的<code>mirror pool disable</code>命令和资源池名称：  </p>
<blockquote>
<p>rbd mirror pool disable {pool-name}</p>
</blockquote>
<p>以这种方式在池上禁用镜像时，对于已明确启用镜像的任何映像（池内），也将禁用镜像。  </p>
<p>例如：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ rbd --cluster <span class="built_in">local</span> mirror pool <span class="built_in">disable</span> image-pool</span><br><span class="line">$ rbd --cluster remote mirror pool <span class="built_in">disable</span> image-pool</span><br></pre></td></tr></table></figure>
<h4 id="新增集群伙伴"><a href="#新增集群伙伴" class="headerlink" title="新增集群伙伴"></a>新增集群伙伴</h4><p>为了让<code>rbd-mirror</code>守护进程发现它的伙伴集群，伙伴集群需要被注册到资源池中。使用rbd新增镜像伙伴Ceph集群，需要额外的<code>mirror pool peer</code>添加命令、资源池名称和集群说明：  </p>
<blockquote>
<p>rbd mirror pool peer add {pool-name} {client-name}@{cluster-name}</p>
</blockquote>
<p>例如：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ rbd --cluster <span class="built_in">local</span> mirror pool peer add image-pool client.remote@remote</span><br><span class="line">$ rbd --cluster remote mirror pool peer add image-pool client.local@<span class="built_in">local</span></span><br></pre></td></tr></table></figure>
<p>默认情况下，<code>rbd-mirror</code>守护进程需要有访问位于<em>/etc/ceph/{cluster-name}.conf</em>的Ceph配置的权限，它提供了伙伴集群的监视器地址；此外还有位于默认或者配置的密钥环检索路径（例如，/etc/ceph/{cluster-name}.{client-name}.keyring）下的密钥环的访问权限。  </p>
<p>另外，伙伴集群的监视器或客户端密钥可以安全地存储在本地Ceph监视器的confi-key存储中。要在添加伙伴镜像时指定伙伴集群的连接属性，请使用<code>--remote-mon-host</code>和<code>--remote-key-file</code>选项。例如：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ rbd --cluster <span class="built_in">local</span> mirror pool peer add image-pool client.remote@remote --remote-mon-host 192.168.1.1,192.168.1.2 --remote-key-file &lt; (<span class="built_in">echo</span> <span class="string">'AQAeuZdbMMoBChAAcj++/XUxNOLFaWdtTREEsw=='</span>)</span><br><span class="line">$ rbd --cluster <span class="built_in">local</span> mirror pool info image-pool --all</span><br><span class="line">Mode: pool</span><br><span class="line">Peers:</span><br><span class="line">  UUID                                 NAME   CLIENT        MON_HOST                KEY</span><br><span class="line">  587b08db-3d33-4f32-8af8-421e77abb081 remote client.remote 192.168.1.1,192.168.1.2 AQAeuZdbMMoBChAAcj++/XUxNOLFaWdtTREEsw==</span><br></pre></td></tr></table></figure>
<h4 id="移除伙伴集群"><a href="#移除伙伴集群" class="headerlink" title="移除伙伴集群"></a>移除伙伴集群</h4><p>要使用rbd移除镜像伙伴Ceph集群，需要额外的<code>mirror pool peer remove</code>命令、资源池名称和伙伴的UUID（可从<code>rbd mirror pool info</code>命令获取）：  </p>
<blockquote>
<p>rbd mirror pool peer remove {pool-name} {peer-uuid}</p>
</blockquote>
<p>例如：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ rbd --cluster <span class="built_in">local</span> mirror pool peer remove image-pool 55672766-c02b-4729-8567-f13a66893445</span><br><span class="line">$ rbd --cluster remote mirror pool peer remove image-pool 60c0e299-b38f-4234-91f6-eed0a367be08</span><br></pre></td></tr></table></figure>
<h4 id="数据池"><a href="#数据池" class="headerlink" title="数据池"></a>数据池</h4><p>在目标集群上创建images时，<code>rbd-mirror</code>收集如下数据池：  </p>
<ol>
<li>如果目标集群有配置好的默认数据池（<code>rbd_default_data_pool</code>配置选项），那么这个数据池会被使用。</li>
<li>否则，如果源image使用单独的数据池，且目标集群上存在同名称的数据池，则使用该池。</li>
<li>如果以上都不成立，则不会设置数据池  </li>
</ol>
<h3 id="Image-配置"><a href="#Image-配置" class="headerlink" title="Image 配置"></a>Image 配置</h3><p>与资源池配置不同，image配置只需针对单个镜像的伙伴集群进行操作。  </p>
<p>镜像RBD image被指定为主image或非主image。这是image的属性而不是池的属性。被指定为非主image的image不可修改。  </p>
<p>在image上第一次启用镜像时，image会自动提升为主image（如果镜像模式为池模式，且image启用了日志image特性，则会隐式地将image提升为主image，或者通过rbd命令<a href="https://docs.ceph.com/docs/master/rbd/rbd-mirroring/#enable-image-mirroring" target="_blank" rel="noopener">显式启用</a>镜像）。  </p>
<h4 id="启用image日志支持"><a href="#启用image日志支持" class="headerlink" title="启用image日志支持"></a>启用image日志支持</h4><p>RBD镜像使用RBD日志特性来确保复制的image总是保持crash-consistent。在image可以被镜像到伙伴集群之前，日志特性必须启用。改特性可以在image创建时通过使用rbd命令，提供<code>--image-feature exclusive-lock,journaling</code>选项来启用。  </p>
<p>或者，日志特性可以在预先存在的RBD images上启用。要使用rbd启用日志，需要额外的<code>feature enable</code>命令、资源池名称、image名称和特性名称：  </p>
<blockquote>
<p>rbd feature enable {pool-name}/{image-name} {feature-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>$ rbd –cluster local feature enable image-pool/image-1 journaling</p>
</blockquote>
<h4 id="启用image镜像"><a href="#启用image镜像" class="headerlink" title="启用image镜像"></a>启用image镜像</h4><p>如果镜像是以image池的image模式配置的，则需要显式地为池中的每个image启用镜像。要使用rbd为特定的image启用镜像，需要额外的<code>mirror image enable</code>命令、资源池名称和image名称：  </p>
<blockquote>
<p>rbd mirror image enable {pool-name}/{image-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>$ rbd –cluster local mirror image enable image-pool/image-1</p>
</blockquote>
<h4 id="禁用image镜像"><a href="#禁用image镜像" class="headerlink" title="禁用image镜像"></a>禁用image镜像</h4><p>要使用rbd禁用特定的image镜像，需要额外的<code>mirror image disable</code>命令、资源池名称和image名称：  </p>
<blockquote>
<p>rbd mirror image disable {pool-name}/{image-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>$ rbd –cluster local mirror image disable image-pool/image-1</p>
</blockquote>
<h4 id="image-晋级和降级"><a href="#image-晋级和降级" class="headerlink" title="image 晋级和降级"></a>image 晋级和降级</h4><p>在故障转移场景中，需要将主节点转移到伙伴集群的image中，应该停止对主image的访问（例如，关闭VM或从VM中移除关联的设备），降级当前的主image，提升新的主image，且恢复对备用集群上image的访问。  </p>
<p>要使用rbd将指定的image降级为non-primary，需要额外的<code>mirror image demote</code>命令、资源池名称和image名称：  </p>
<blockquote>
<p>rbd mirror image demote {pool-name}/{image-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>$ rbd –cluster local mirror image demote image-pool/image-1</p>
</blockquote>
<p>要是用rbd将池中的所有主image降级为non-primary，需要额外的<code>mirror pool demote</code>命令和资源池名称：  </p>
<blockquote>
<p>rbd mirror pool demote {pool-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>$ rbd –cluster local mirror pool demote image-pool  </p>
</blockquote>
<p>要使用rbd将指定的image升级为primary，需要额外的<code>mirror image promote</code>命令、资源池名称和image名称：  </p>
<blockquote>
<p>rbd mirror image promote [–force] {pool-name}/{image-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>$ rbd –cluster remote mirror image promote image-pool/image-1  </p>
</blockquote>
<p>要是用rbd将池中的所有的non-primary image升级为primary，需要额外的<code>mirror pool promote</code>命令和资源池名称：  </p>
<blockquote>
<p>rbd mirror pool promote [–force] {pool-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>$ rbd –cluster local mirror pool promote image-pool  </p>
</blockquote>
<h4 id="强制image同步"><a href="#强制image同步" class="headerlink" title="强制image同步"></a>强制image同步</h4><p>如果rbd-mirror守护进程检测到有裂脑事件，则在更正之前它不会镜像受到影响的image。要恢复image镜像，首先要降级已过期的image，然后请求同步到主image。要使用rbd请求同步image，需要额外的<code>mirror image resync</code>命令，池名称和image名称：  </p>
<blockquote>
<p>rbd mirror image resync {pool-name}/{image-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>$ rbd mirror image resync image-pool/image-1</p>
</blockquote>
<h3 id="镜像状态"><a href="#镜像状态" class="headerlink" title="镜像状态"></a>镜像状态</h3><p>伙伴集群的副本状态存储在每个主镜像中。该状态可被<code>mirror image status</code>和<code>mirror pool status</code>命令检索到。  </p>
<p>要使用rbd命令请求镜像状态，需要额外的<code>mirror image status</code>命令，池名称和image名称：  </p>
<blockquote>
<p>rbd mirror image status {pool-name}/{image-nmae}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>$ rbd mirror image status image-pool/image-1</p>
</blockquote>
<p>要使用rbd命令请求镜像池的总体状态，需要额外的<code>mirror pool status</code>命令和池名称：  </p>
<blockquote>
<p>rbd mirror pool status {pool-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>$ rbd mirror pool status image-pool</p>
</blockquote>
<h3 id="RBD-mirror-守护进程"><a href="#RBD-mirror-守护进程" class="headerlink" title="RBD-mirror 守护进程"></a>RBD-mirror 守护进程</h3><p>两个rbd-mirror守护进程负责监听远端image日志、伙伴集群和重放本地集群的日志事件。RBD image日志特性安装出现的顺序记录下所有对image的修改。这确保了远程镜像的crash-consistent在本地可以。  </p>
<p>每个rbd-mirror守护进程应该使用唯一的Ceph用户ID。要使用ceph创建一个Ceph用户，需要额外的<code>auth get-or-create</code>命令，用户名、监视器上限和OSD上限：  </p>
<blockquote>
<p>ceph auth get-or-create client.rbd-mirror.{unique id} mon ‘profile rbd-mirror’ osd ‘profile rbd’</p>
</blockquote>
<p>通过指定用户ID作为守护进程实例，systemd可以管理rbd-mirror守护进程：  </p>
<blockquote>
<p>systemctl enable ceph-rbd-mirror@rbd-mirror.{unique id}</p>
</blockquote>
<p>rbd-mirror也可以在前台通过rbd-mirror命令启动：  </p>
<blockquote>
<p>rbd-mirror -f –log-file={log_path}  </p>
</blockquote>
<h2 id="image实时迁移"><a href="#image实时迁移" class="headerlink" title="image实时迁移"></a>image实时迁移</h2><p>RBD images可以在同一集群的不同资源池或不同image格式和布局之间实时迁移。启动时，源image会被深拷贝到目标image上，提取所有快照历史记录，并可选择保留到源image的父image的任何链接，以帮助保持稀疏性。  </p>
<p>新的目标image在使用的同时赋值操作可以安全地在后台运行。在准备迁移之前，需要暂时停止使用源image，这有助于确保该image的客户端被更新为指向新的目标image。  </p>
<p><img src="../images/Live_migration.png" alt="image live migration">  </p>
<p>实时迁移过程由三步组成：  </p>
<ol>
<li><strong>准备迁移</strong>：初始步骤创建新的目标image并交叉链接源image和目标image。与<font color="red">分层映像</font>相似，尝试读取目标image中未初始化的区域将在内部重定向到源image，而写入目标image中未初始化区域将在内部将重叠的源image块深拷贝到目标image。</li>
<li><strong>执行迁移</strong>：将所有初始化的块从源image深拷贝到目标，该操作在后台执行。此步骤可以在客户端活跃访问新目标image时执行。</li>
<li><strong>结束迁移</strong>：一旦后台迁移操作完成，就可以提交或者中断迁移。提交迁移将删除源与目标image之间的交叉链接并移除源image。中断迁移将移除交叉链接并移除目标image。  </li>
</ol>
<h3 id="准备迁移"><a href="#准备迁移" class="headerlink" title="准备迁移"></a>准备迁移</h3><p>执行<code>rbd migration prepare</code>命令初始化实时迁移操作，需要体统源与目标images：  </p>
<blockquote>
<p>$ rbd migration prepare migration_source [migration_target]  </p>
</blockquote>
<p><code>rbd migration prepare</code>命令接收与<code>rbd create</code>命令相同的布局选项,它允许更改磁盘上不可更改的image布局。如果只是更改磁盘上的布局，<em>migration_target</em>可省略，沿用原始image名称。  </p>
<p>准备进行实时迁移前，所有使用源image的客户端都必须停止使用。如果发现有任何客户端以读/写模式打开image，准备步骤将会失败。一旦准备步骤完成，客户端可以使用心得目标image名称重启。尝试使用源image重启的客户端将重启失败。  </p>
<p><code>rbd status</code>命令可以展示实时迁移的当前状态：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">$ rbd status migration_target</span><br><span class="line">Watchers: none</span><br><span class="line">Migration:</span><br><span class="line">            <span class="built_in">source</span>: rbd/migration_source (5e2cba2f62e)</span><br><span class="line">            destination: rbd/migration_target (5e2ed95ed806)</span><br><span class="line">            state: prepared</span><br></pre></td></tr></table></figure>
<p>注意，源image将被移动到RBD回收站中，以防迁移过程中的误操作：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ rbd info migration_source</span><br><span class="line">rbd: error opening image migration_source: (2) No such file or directory</span><br><span class="line">$ rbd trash ls --all</span><br><span class="line">5e2cba2f62e migration_source</span><br></pre></td></tr></table></figure>
<h3 id="执行迁移"><a href="#执行迁移" class="headerlink" title="执行迁移"></a>执行迁移</h3><p>准备完实时迁移之后，image块必须从源image中复制到目标image中。执行<code>rbd migration execute</code>命令可以实现上述操作：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ rbd migration execute migration_target</span><br><span class="line">Image migration: 100% complete...done.</span><br></pre></td></tr></table></figure>
<p><code>rbd status</code>命令也可提供深拷贝过程中迁移块操作的反馈：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">$ rbd status migration_target</span><br><span class="line">Watchers:</span><br><span class="line">    watcher=1.2.3.4:0/3695551461 client.123 cookie=123</span><br><span class="line">Migration:</span><br><span class="line">            <span class="built_in">source</span>: rbd/migration_source (5e2cba2f62e)</span><br><span class="line">            destination: rbd/migration_target (5e2ed95ed806)</span><br><span class="line">            state: executing (32% complete)</span><br></pre></td></tr></table></figure>
<h3 id="提交迁移"><a href="#提交迁移" class="headerlink" title="提交迁移"></a>提交迁移</h3><p>一旦实时迁移完成从源image到目标image所有数据块的深拷贝操作，迁移操作就可以提交：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">$ rbd status migration_target</span><br><span class="line">Watchers: none</span><br><span class="line">Migration:</span><br><span class="line">            <span class="built_in">source</span>: rbd/migration_source (5e2cba2f62e)</span><br><span class="line">            destination: rbd/migration_target (5e2ed95ed806)</span><br><span class="line">            state: executed</span><br><span class="line">$ rbd migration commit migration_target</span><br><span class="line">Commit image migration: 100% complete...done.</span><br></pre></td></tr></table></figure>
<p>如果<em>migration_source</em>image是一个或者多个克隆体的父节点，在确保所有的克隆体都停用后需要额外提供 <em>-force</em>选项。  </p>
<p>提交迁移将删除源与目标image之间的交叉链接并移除源image：  </p>
<blockquote>
<p>$ rbd trash list –all</p>
</blockquote>
<h3 id="中断迁移"><a href="#中断迁移" class="headerlink" title="中断迁移"></a>中断迁移</h3><p>如果你想回退准备或执行步骤，执行<code>rbd migration abort</code>命令可以回退迁移过程：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ rbd migration abort migration_target</span><br><span class="line">Abort image migration: 100% complete...done.</span><br></pre></td></tr></table></figure>
<p>中断迁移将删除目标image，并访问要恢复的原始源image：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ rbd ls</span><br><span class="line">migration_source</span><br></pre></td></tr></table></figure>
<h2 id="RBD-持久缓存"><a href="#RBD-持久缓存" class="headerlink" title="RBD 持久缓存"></a>RBD 持久缓存</h2><h3 id="共享的、只读父image缓存"><a href="#共享的、只读父image缓存" class="headerlink" title="共享的、只读父image缓存"></a>共享的、只读父image缓存</h3><p>从父image<font color="red">克隆的RBD images</font>通常只修改了image中的一小部分。例如，在VDI工作模式下，VMs是从同一基础image克隆的，仅主机名称和IP地址不同。在启动阶段，从集群里重新读取的父image其实很大部分是重复的。如果本地有父image的缓存，不仅可以加快读取速度、降低等待事件，还能减少客户机到集群的网络流量。RBD共享的只读父image缓存需要在<em>ceph.conf</em>中显式启用。ceph-immmutable-object-cache守护进程负责将父内容缓存到本地磁盘上，将来对该数据的读取将从本地缓存提供服务。  </p>
<p><img src="../images/RBD_Persistent_Cache_00.png" alt="RBD Persistent Cache">  </p>
<h4 id="启用RBD共享只读父image缓存"><a href="#启用RBD共享只读父image缓存" class="headerlink" title="启用RBD共享只读父image缓存"></a>启用RBD共享只读父image缓存</h4><p>要启用RBD共享只读父image缓存，需要在你的ceph.conf文件中的[client]部分添加如下设置：  </p>
<blockquote>
<p>rbd parent cache enabled = true  </p>
</blockquote>
<h3 id="不可变对象缓存守护进程"><a href="#不可变对象缓存守护进程" class="headerlink" title="不可变对象缓存守护进程"></a>不可变对象缓存守护进程</h3><p>ceph-immmutable-object-cache守护进程负责将父内容缓存到本地缓存目录上。为了获得更好的性能，建议使用SSD作为底层存储介质。  </p>
<p>守护进程的关键组件有：  </p>
<p>$\qquad$1. <strong>Domain socket based IPC</strong>：守护进程将在启动时监听本地域套接字，并等待来自librbd客户端的连接。<br>$\qquad$2. <strong>LRU based promotion/demotion policy</strong>：守护进程将维护每个缓存文件上缓存命中的内存统计信息。如果容量达到配置的阈值，它将降级冷缓存。<br>$\qquad$3. <strong>File-based caching store</strong>：守护进程将维护一个基于缓存存储的简单文件。在升级是，将从RADOS集群获取RADOS对象并存储到本地缓存目录中。  </p>
<p>在打开每个克隆的rbd image时，librbd会尝试通过它的域套接字来连接缓存守护进程。如果成功连接，librbd将在后续读取时自动与守护进程进行检查。如果有未缓存的读操作，守护进程将把RADOS对象提升到本地缓存目录，所以下次对该对象的读操作将从本地文件提供服务。守护进程还会维护简答的LRU统计数据，因此如果没有足够的容量，它将删除一些冷缓存文件。  </p>
<p>以下是一些与下列设置相对应的重要缓存选项:  </p>
<ul>
<li>immutable_object_cache_path 不可变对象缓存数据目录</li>
<li>immutable_object_cache_max_size 不可变缓存最大值</li>
<li>immutable_object_cache_watermark 缓存水印。如果容量达到这个水印，守护进程将会根据LRU统计数据删除冷缓存。  </li>
</ul>
<p>ceph-immutable-object-cache守护进程可在可选的ceph-immutable-object-cache分发包中使用。  </p>
<p>ceph-immutable-object-cache守护进程应该使用唯一的Ceph用户ID。要<font color="red">要创建Ceph用户</font>，使用ceph auth get-or-create命令，需要提供用户名，监视器权限，OSD权限：  </p>
<blockquote>
<p>ceph auth get-or-create client.ceph-immutable-object-cache.{unique id} mon ‘allow r’ osd ‘profile rbd-read-only’  </p>
</blockquote>
<p>作为守护进程实例，ceph-immutable-object-cache守护进程可以通过systemd使用指定的用户ID进行管理：  </p>
<blockquote>
<p>systemctl enable ceph-immutable-object-cache@immutable-object-cache.{unique id}</p>
</blockquote>
<p>ceph-immutable-object-cache守护进程也可在前台通过ceph-immutable-object-cache命令执行：  </p>
<blockquote>
<p>ceph-immutable-object-cache -f –log-file={log_path}  </p>
</blockquote>
<h2 id="librbd-配置设置"><a href="#librbd-配置设置" class="headerlink" title="librbd 配置设置"></a>librbd 配置设置</h2><p>详情参见<a href="https://docs.ceph.com/docs/master/rbd" target="_blank" rel="noopener"><font color="red">块设备</font></a>。  </p>
<h3 id="缓存设置"><a href="#缓存设置" class="headerlink" title="缓存设置"></a>缓存设置</h3><p>Ceph块设备的用户空间实现（即librbd）无法使用Linux页面缓存，因此她包含了自己的内存缓存，名为“RBD缓存”。RBD缓存行为类似于性能良好的硬盘缓存。当系统发送中断或刷新请求时，所有的数据都会被写入OSDs。这意味着回写式缓存与使用性能良好的物理硬盘一样安全，其中的VM可以正确地发送刷新（Linux内核&gt;=2.6.32）。缓存使用Least Recently Used(LRU)算法，并且在回写模式下，为了更高的吞吐量它可以合并相邻的请求。  </p>
<blockquote>
<p>内核缓存： Ceph块设备的内核驱动可以使用Linux页面缓存提升性能。  </p>
</blockquote>
<p>librbd缓存默认启用并三种不同的缓存策略：write-around，write-back和write-through。在write-around和write-back策略下写操作立即返回，除非有超过RBD缓存最大未写入字节写到存储集群中。write-around策略不同于write-back在于它不尝试为从缓存中读取请求提供服务，因此在写入工作负载下拥有更高的性能。在write-through策略下，只有在所有副本中数据都落盘时才会返回，但读操作可能来自缓存。  </p>
<p>在收到刷新请求之前，缓存的行为类似write-through缓存，以确保旧操作系统不发送刷新请求来保证crash-consistent。  </p>
<p>如果librbd缓存禁用，读写操作将直接作用于存储集群，写操作只有在所有副本中数据都落盘时才会返回。  </p>
<p>RBD的设置应被设置在你的ceph.conf配置文件中[client]部分，设置包括：  </p>
<p>rbd cache：  </p>
<p>$\qquad$ <strong>描述</strong>：启用RADOS块设备(RBD)缓存<br>$\qquad$ <strong>类型</strong>：Boolean<br>$\qquad$ <strong>必要</strong>：否<br>$\qquad$ <strong>默认值</strong>：true  </p>
<p>rbd cache policy：  </p>
<p>$\qquad$ <strong>描述</strong>：为librbd选择缓存策略<br>$\qquad$ <strong>类型</strong>：Enum<br>$\qquad$ <strong>必要</strong>：否<br>$\qquad$ <strong>默认值</strong>：writearound<br>$\qquad$ <strong>可选值</strong>：writearound，writeback，writethrough</p>
<p>rbd cache writethrough until flush：  </p>
<p>$\qquad$ <strong>描述</strong>：从write-through模式开始，在收到第一个刷新请求后切换到write-back。启用这个选项是一个保守但安全的设置，以防运行在rbd上的vm太老而无法发送刷新，就像2.6.32之前Linux中的virtio驱动程序一样。<br>$\qquad$ <strong>类型</strong>：Boolean<br>$\qquad$ <strong>必要</strong>：否<br>$\qquad$ <strong>默认值</strong>：true   </p>
<p>rbd cache size：  </p>
<p>$\qquad$ <strong>描述</strong>：RBD缓存字节数<br>$\qquad$ <strong>类型</strong>：64-bit Integer<br>$\qquad$ <strong>必要</strong>：否<br>$\qquad$ <strong>默认值</strong>：32 MiB<br>$\qquad$ <strong>策略</strong>：write-back和write-through  </p>
<p>rbd cache max dirty：  </p>
<p>$\qquad$ <strong>描述</strong>：在缓存中触发write-back的dirty字节限制，如果是0，使用write-through缓存<br>$\qquad$ <strong>类型</strong>：64-bit Integer<br>$\qquad$ <strong>必要</strong>：否<br>$\qquad$ <strong>限制</strong>：必须小于rbd cache size<br>$\qquad$ <strong>默认值</strong>：24 MiB<br>$\qquad$ <strong>策略</strong>：write-through和write-back  </p>
<p>rbd cache target dirty：  </p>
<p>$\qquad$ <strong>描述</strong>：在缓存开始向数据存储写入数据之前的target dirty。不阻塞对缓存的写入<br>$\qquad$ <strong>类型</strong>：64-bit Integer<br>$\qquad$ <strong>必要</strong>：否<br>$\qquad$ <strong>限制</strong>：必须小于rbd cache max dirty<br>$\qquad$ <strong>默认值</strong>：16 MiB<br>$\qquad$ <strong>策略</strong>：write-back  </p>
<p>rbd cache max dirty age：  </p>
<p>$\qquad$ <strong>描述</strong>：writeback开始前dirty数据在缓存中的保留秒数<br>$\qquad$ <strong>类型</strong>：Float<br>$\qquad$ <strong>必要</strong>：否<br>$\qquad$ <strong>默认值</strong>：1.0<br>$\qquad$ <strong>策略</strong>：write-back    </p>
<h3 id="预读设置"><a href="#预读设置" class="headerlink" title="预读设置"></a>预读设置</h3><p>librbd支持预读/预处理以优化小、有序的读取。在VM的情况下，这通常应该由客户机系统来处理，但引导加载器可能不会发出有效的读取操作。如果禁用缓存或缓存策略是write-around，预读操作将自动禁用。  </p>
<p>rbd readahead trigger requests  </p>
<p>$\qquad$ <strong>描述</strong>：触发预读操作的有序读请求的数量<br>$\qquad$ <strong>类型</strong>：Integer<br>$\qquad$ <strong>必要</strong>：否<br>$\qquad$ <strong>默认值</strong>：10  </p>
<p>rbd readahead max bytes  </p>
<p>$\qquad$ <strong>描述</strong>：最大预读请求数量。如果为0，禁用预读<br>$\qquad$ <strong>类型</strong>：64-bit Integer<br>$\qquad$ <strong>必要</strong>：否<br>$\qquad$ <strong>默认值</strong>：512 KiB  </p>
<p>rbd readahead disable after bytes  </p>
<p>$\qquad$ <strong>描述</strong>：当从RBD image读取到如此多字节后，该image的预读操作将被禁用直至它关闭。这允许客户机系统在启动后接管预读。如果是0，预读一直启用。<br>$\qquad$ <strong>类型</strong>：64-bit Integer<br>$\qquad$ <strong>必要</strong>：否<br>$\qquad$ <strong>默认值</strong>：50 MiB  </p>
<h3 id="image-特性"><a href="#image-特性" class="headerlink" title="image 特性"></a>image 特性</h3><p>RBD支持高级特性，可以在创建image时通过命令行指定，也可以通过Ceph配置文件通过‘rbd_default_features = \<sum of feature numeric values>’ or ‘rbd_default_features = \<comma-delimited list of cli values>’来指定。  </comma-delimited></sum></p>
<p>Layering  </p>
<p>$\qquad$ <strong>描述</strong>：分层使你可以使用克隆<br>$\qquad$ <strong>内部值</strong>：1<br>$\qquad$ <strong>CLI值</strong>：layering<br>$\qquad$ <strong>引入</strong>：v0.70(Emperor)<br>$\qquad$ <strong>KRBD支持</strong>：从v3.10<br>$\qquad$ <strong>默认</strong>：是  </p>
<p>Striping v2  </p>
<p>$\qquad$ <strong>描述</strong>：条带化将数据传播到多个对象。条带化有助于提高顺序读写工作负载的并行性。<br>$\qquad$ <strong>内部值</strong>：2<br>$\qquad$ <strong>CLI值</strong>：striping<br>$\qquad$ <strong>引入</strong>：v0.70(Emperor)<br>$\qquad$ <strong>KRBD支持</strong>：从v3.10<br>$\qquad$ <strong>默认</strong>：是  </p>
<p>Exclusive locking  </p>
<p>$\qquad$ <strong>描述</strong>：如果启用，它要求客户端在对象上执行写入前获取所。仅当单个客户端同时访问image时，才应启用独占锁。<br>$\qquad$ <strong>内部值</strong>：4<br>$\qquad$ <strong>CLI值</strong>：exclusive-lock<br>$\qquad$ <strong>引入</strong>：v0.92(Hammer)<br>$\qquad$ <strong>KRBD支持</strong>：从v4.9<br>$\qquad$ <strong>默认</strong>：是   </p>
<p>Object map  </p>
<p>$\qquad$ <strong>描述</strong>：对象映射的支持基于独占锁的支持。块设备是精简配置的，意味着，它们只存储实际存在的数据。对象映射支持帮助追踪哪些对象实际存在（已将数据存储在驱动上）。启用对象映射支持可加速用于克隆的I/O操作、导入和导出稀疏映像、删除。<br>$\qquad$ <strong>内部值</strong>：8<br>$\qquad$ <strong>CLI值</strong>：object-map<br>$\qquad$ <strong>引入</strong>：v0.92(Hammer)<br>$\qquad$ <strong>KRBD支持</strong>：从v5.3<br>$\qquad$ <strong>默认</strong>：是   </p>
<p>Fast-diff  </p>
<p>$\qquad$ <strong>描述</strong>：Fast-diff的支持依赖于对象映射的支持和独占锁的支持。它想对象映射添加了另一个属性，这使得生成image快照之间的差异和快照实际数据使用之间的差异要快很多。<br>$\qquad$ <strong>内部值</strong>：16<br>$\qquad$ <strong>CLI值</strong>：fast-diff<br>$\qquad$ <strong>引入</strong>：v9.0.1(Infernalis)<br>$\qquad$ <strong>KRBD支持</strong>：从v5.3<br>$\qquad$ <strong>默认</strong>：是   </p>
<p>Deep-flatten  </p>
<p>$\qquad$ <strong>描述</strong>：Deep-flatten使rbd flatten工作于image本身之外的所有快照。没有它，image快照仍然依赖于父级，因此父级在快照被删除之前是不可删除的。Deep-flatten使父类独立于它的克隆，即使它们有快照。<br>$\qquad$ <strong>内部值</strong>：32<br>$\qquad$ <strong>CLI值</strong>：deep-flatten<br>$\qquad$ <strong>引入</strong>：v9.0.2(Infernalis)<br>$\qquad$ <strong>KRBD支持</strong>：从v5.1<br>$\qquad$ <strong>默认</strong>：是   </p>
<p>Journaling  </p>
<p>$\qquad$ <strong>描述</strong>：日志的支持依赖于独占锁的支持。日志按照出现的顺序记录下所有对image的修改。RBD镜像利用日志将crash-consistent image复制到远程集群。<br>$\qquad$ <strong>内部值</strong>：64<br>$\qquad$ <strong>CLI值</strong>：journaling<br>$\qquad$ <strong>引入</strong>：v10.0.1(Jewel)<br>$\qquad$ <strong>KRBD支持</strong>：否<br>$\qquad$ <strong>默认</strong>：否  </p>
<p>Data Pool  </p>
<p>$\qquad$ <strong>描述</strong>：在erasure-coded池中，需要将image数据块对象存储在于image元数据分开的池中。<br>$\qquad$ <strong>内部值</strong>：128<br>$\qquad$ <strong>引入</strong>：v11.1.1(Kraken)<br>$\qquad$ <strong>KRBD支持</strong>：从v4.11<br>$\qquad$ <strong>默认</strong>：否  </p>
<p>Operations  </p>
<p>$\qquad$ <strong>描述</strong>：用于限制旧客户端对image执行某些维护操作（例如克隆、创建快照）。<br>$\qquad$ <strong>内部值</strong>：256<br>$\qquad$ <strong>引入</strong>：v13.0.2(Mimic)<br>$\qquad$ <strong>KRBD支持</strong>：从v4.16  </p>
<p>Migrating  </p>
<p>$\qquad$ <strong>描述</strong>：用于限制旧客户端在image处于迁移状态是打开image。<br>$\qquad$ <strong>内部值</strong>512<br>$\qquad$ <strong>引入</strong>：v14.0.1(Nautilus)<br>$\qquad$ <strong>KRBD支持</strong>：否  </p>
<h3 id="QOS设置"><a href="#QOS设置" class="headerlink" title="QOS设置"></a>QOS设置</h3><p>librbd支持限制每个image IO，由以下设置控制。  </p>
<p>rbd qos iops limit  </p>
<p>$\qquad$ <strong>描述</strong>：每秒IO操作的极限<br>$\qquad$ <strong>类型</strong>：Unsigned Integer<br>$\qquad$ <strong>必须</strong>：否<br>$\qquad$ <strong>默认</strong>：0   </p>
<p>rbd qos bps limit  </p>
<p>$\qquad$ <strong>描述</strong>：每秒IO字节的极限<br>$\qquad$ <strong>类型</strong>：Unsigned Integer<br>$\qquad$ <strong>必须</strong>：否<br>$\qquad$ <strong>默认</strong>：0   </p>
<p>rbd qos read iops limit  </p>
<p>$\qquad$ <strong>描述</strong>：每秒读操作的极限<br>$\qquad$ <strong>类型</strong>：Unsigned Integer<br>$\qquad$ <strong>必须</strong>：否<br>$\qquad$ <strong>默认</strong>：0   </p>
<p>rbd qos write iops limit  </p>
<p>$\qquad$ <strong>描述</strong>：每秒写操作的极限<br>$\qquad$ <strong>类型</strong>：Unsigned Integer<br>$\qquad$ <strong>必须</strong>：否<br>$\qquad$ <strong>默认</strong>：0   </p>
<p>rbd qos read bps limit  </p>
<p>$\qquad$ <strong>描述</strong>：每秒读取字节的极限<br>$\qquad$ <strong>类型</strong>：Unsigned Integer<br>$\qquad$ <strong>必须</strong>：否<br>$\qquad$ <strong>默认</strong>：0   </p>
<p>rbd qos write bps limit  </p>
<p>$\qquad$ <strong>描述</strong>：每秒写入字节的极限<br>$\qquad$ <strong>类型</strong>：Unsigned Integer<br>$\qquad$ <strong>必须</strong>：否<br>$\qquad$ <strong>默认</strong>：0   </p>
<p>rbd qos iops burst  </p>
<p>$\qquad$ <strong>描述</strong>：IO操作的爆炸极限<br>$\qquad$ <strong>类型</strong>：Unsigned Integer<br>$\qquad$ <strong>必须</strong>：否<br>$\qquad$ <strong>默认</strong>：0   </p>
<p>rbd qos bps burst  </p>
<p>$\qquad$ <strong>描述</strong>：IO字节的爆炸极限<br>$\qquad$ <strong>类型</strong>：Unsigned Integer<br>$\qquad$ <strong>必须</strong>：否<br>$\qquad$ <strong>默认</strong>：0   </p>
<p>rbd qos read iops burst  </p>
<p>$\qquad$ <strong>描述</strong>：读操作的爆炸极限<br>$\qquad$ <strong>类型</strong>：Unsigned Integer<br>$\qquad$ <strong>必须</strong>：否<br>$\qquad$ <strong>默认</strong>：0  </p>
<p>rbd qos write iops burst  </p>
<p>$\qquad$ <strong>描述</strong>：写操作的爆炸极限<br>$\qquad$ <strong>类型</strong>：Unsigned Integer<br>$\qquad$ <strong>必须</strong>：否<br>$\qquad$ <strong>默认</strong>：0   </p>
<p>rbd qos read bps burst  </p>
<p>$\qquad$ <strong>描述</strong>：读字节的爆炸极限<br>$\qquad$ <strong>类型</strong>：Unsigned Integer<br>$\qquad$ <strong>必须</strong>：否<br>$\qquad$ <strong>默认</strong>：0   </p>
<p>rbd qos write bps burst  </p>
<p>$\qquad$ <strong>描述</strong>：写字节的爆炸极限<br>$\qquad$ <strong>类型</strong>：Unsigned Integer<br>$\qquad$ <strong>必须</strong>：否<br>$\qquad$ <strong>默认</strong>：0   </p>
<p>rbd qos schedule tick min  </p>
<p>$\qquad$ <strong>描述</strong>：QOS的最小时间调度（以毫秒为单位）<br>$\qquad$ <strong>类型</strong>：Unsigned Integer<br>$\qquad$ <strong>必须</strong>：否<br>$\qquad$ <strong>默认</strong>：50  </p>
<h2 id="RBD重播"><a href="#RBD重播" class="headerlink" title="RBD重播"></a>RBD重播</h2><p>RBD replay是一系列捕获和重播RBD工作负载的工具。要捕获RBD工作负载，客户端必须安装lttng-tools，且librbd必须是v0.87(Giant)或之后的版本。要重播RBD工作负载，客户端上的librbd必须是v0.87(Giant)或之后的版本。  </p>
<p>捕获和重播需要三步：  </p>
<ol>
<li>捕获记录。确保捕获pthread_id上下文：  </li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p traces</span><br><span class="line">lttng create -o traces librbd</span><br><span class="line">lttng <span class="built_in">enable</span>-event -u <span class="string">'librbd:*'</span></span><br><span class="line">lttng add-context -u -t pthread_id</span><br><span class="line">lttng start</span><br><span class="line"><span class="comment"># run RBD workload here</span></span><br><span class="line">lttng stop</span><br></pre></td></tr></table></figure>
<ol start="2">
<li>使用<font color="red">rbd-replay-prep</font>处理记录：  </li>
</ol>
<blockquote>
<p>rbd-replay-prep traces/ust/uid/<em>/</em> replay.bin  </p>
</blockquote>
<ol start="3">
<li>使用<font color="red">rbd-replay</font>重播记录。使用只读，直到你知道它在做你想在的：  </li>
</ol>
<blockquote>
<p>rbd-replay –read-only replay.bin  </p>
</blockquote>
<p>重新播放的工作负载不必针对相同的RBD映像，甚至不必针对与捕获的工作负载相同的集群。为了说明差异，您可能需要使用rbd-replay的-pool和-map-image选项。  </p>
<h1 id="Ceph块设备第三方集成"><a href="#Ceph块设备第三方集成" class="headerlink" title="Ceph块设备第三方集成"></a>Ceph块设备第三方集成</h1><h2 id="内核模块操作"><a href="#内核模块操作" class="headerlink" title="内核模块操作"></a>内核模块操作</h2><h3 id="获取image列表"><a href="#获取image列表" class="headerlink" title="获取image列表"></a>获取image列表</h3><p>要挂载块设备image，首先需要获取image列表：  </p>
<blockquote>
<p>rbd list</p>
</blockquote>
<h3 id="映射块设备"><a href="#映射块设备" class="headerlink" title="映射块设备"></a>映射块设备</h3><p>使用rbd映射image名称到内核模块。你必须指定image名称，资源池名称和用户名。如果尚未加载rbd内核模块，rbd将为你加载它：  </p>
<blockquote>
<p>sudo rbd device map {pool-name}/{image-name} –id {user-name}  </p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>sudo rbd device map rbd/myimage –id admin  </p>
</blockquote>
<p>如果你使用<font color="red">cephx</font>认证，你必须额外指明密码。它可能来自密钥环或包含密码的文件：  </p>
<blockquote>
<p>sudo rbd device map rbd/myimage –id admin –keyring /path/to/keyring<br>sudo rbd device map rbd/myimage –id admin –keyfile /path/to/file  </p>
</blockquote>
<h3 id="展示已映射的块设备"><a href="#展示已映射的块设备" class="headerlink" title="展示已映射的块设备"></a>展示已映射的块设备</h3><p>要使用rbd展示映射到内核模块的块设备image，指定参数<code>device list</code>：  </p>
<blockquote>
<p>rbd device list  </p>
</blockquote>
<h3 id="取消映射"><a href="#取消映射" class="headerlink" title="取消映射"></a>取消映射</h3><p>要使用rbd命令取消块设备image映射，需要指定<code>device unmap</code>参数和设备名称（例如，按照惯例，有块设备image同名）：  </p>
<blockquote>
<p>sudo rbd device unmap /dev/rbd/{poolname}/{imagename}  </p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>sudo rbd device unmap /dev/rbd/rbd/foo</p>
</blockquote>
<h2 id="QEMU-与块设备"><a href="#QEMU-与块设备" class="headerlink" title="QEMU 与块设备"></a>QEMU 与块设备</h2><p>最常见的Ceph块设备使用场景是为虚拟机提供块设备images。例如，用户可以在理想的配置中使用OS和相关软件创建一个“黄金”image。然后用户创建一个这个image的快照。最后，用户克隆这个快照（通常是很多次）。详见<a href="https://docs.ceph.com/docs/master/rbd/rbd-snapshot/" target="_blank" rel="noopener"><font color="red">Snapshots</font></a>。对快照进行写时复制的能力意味着Ceph可以快速地将块设备image提供给虚拟机，因为客户端不必在每次启用一个新的虚拟机时下载整个image。  </p>
<p><img src="../images/qemu.png" alt="QEMU and Ceph Block">  </p>
<p>Ceph块设备可以跟QEMU虚拟机融为一体。关于QEMU详情，请查看<a href="http://wiki.qemu.org/Main_Page" target="_blank" rel="noopener"><font color="red">QEMU Open Source Processor Emulator</font></a>。QEMU文件参见<a href="http://wiki.qemu.org/Manual" target="_blank" rel="noopener"><font color="red">文档</font></a>，安装细节，参见<a href="https://docs.ceph.com/docs/master/install" target="_blank" rel="noopener"><font color="red">安装</font></a>。  </p>
<h3 id="使用指南"><a href="#使用指南" class="headerlink" title="使用指南"></a>使用指南</h3><p>QEMU命令行期望你指定资源池名称和image名称。你也可以指明快照名称。  </p>
<p>QEMU假设Ceph配置文件存放在默认的路径(即/etc/ceph/$cluster.conf)，且你可以使用默认的client.admin用户执行命令，除非你指明其它的Ceph配置文件路径或用户。当指定用户是，QEMU只使用ID而不是全部的TYPE:ID。详见<a href="https://docs.ceph.com/docs/master/rados/operations/user-management#user" target="_blank" rel="noopener"><font color="red">用户管理-用户</font></a>。在用户ID之前不用使用client类型的前缀（例如，client.），否则你会收到认证错误。使用<code>:id={user}</code>选项时你应该持有管理员用户或其他用户的密钥，密钥文件应该存储在默认路径（即，/etc/ceph）或具有合适的文件所有权和权限的本地路径。用法格式如下：  </p>
<blockquote>
<p>qemu-img {command} [options] rbd:{pool-name}/{image-name}[@snapshot-name][:option1=value1][:option2=value2…]  </p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>qemu-img {command} [options] rbd:glance-pool/maipo:id=glance:conf=/etc/ceph/ceph.conf</p>
</blockquote>
<h3 id="使用QEMU创建images"><a href="#使用QEMU创建images" class="headerlink" title="使用QEMU创建images"></a>使用QEMU创建images</h3><p>你可以从QEMU中创建块设备image。你必须指明rbd，资源池名称以及你想要创建的image名称，同时你还必须指明image大小。  </p>
<blockquote>
<p>qemu-img create -f raw rbd:{pool-name}/{image-name} {size}  </p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>qemu-img create -f raw rbd:data/foo 10G</p>
</blockquote>
<h3 id="使用QEMU重置image大小"><a href="#使用QEMU重置image大小" class="headerlink" title="使用QEMU重置image大小"></a>使用QEMU重置image大小</h3><p>你可以从QEMU中重置块设备image的大小。你必须指明rbd，资源池名称以及你想重置大小的image的名称。你还必须指明image的大小。  </p>
<blockquote>
<p>qemu-img resize rbd:{pool-name}/{image-name} {size}  </p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>qemu-img resize rbd:data/foo 10G  </p>
</blockquote>
<h3 id="使用QEMU检索image信息"><a href="#使用QEMU检索image信息" class="headerlink" title="使用QEMU检索image信息"></a>使用QEMU检索image信息</h3><p>你可以从QEMU中检索块设备image的信息。你必须指明rbd，资源池名称以及image名称。  </p>
<blockquote>
<p>qemu-img info rbd:{pool-name}/{image-name}</p>
</blockquote>
<p>例如：  </p>
<blockquote>
<p>qemu-img info rbd:data/foo  </p>
</blockquote>
<h3 id="使用rbd运行QEMU"><a href="#使用rbd运行QEMU" class="headerlink" title="使用rbd运行QEMU"></a>使用rbd运行QEMU</h3><p>QEMU可以将块设备从主机上传递给客户机，但从QEMU 0.15起，不再需要将image映射为主机上的块设备了。取而代之的是，QEMU可以通过librbd直接访问作为虚拟块设备的image。这样做更高效，因为这样避免了上下文切换的额外开销，同时也能利用<font color="red">RBD缓存</font>。  </p>
<p>你可使用<code>qemu-img</code>将已存在的虚拟机image转换成Ceph的块设备image。例如，加入你有一个qcow2的镜像，你可以执行：  </p>
<blockquote>
<p>qemu-img convert -f qcow2 -O raw debian_squeeze.qcow2 rbd:data/squeeze</p>
</blockquote>
<p>要从这个镜像启动虚拟机，你可以执行：  </p>
<blockquote>
<p>qemu -m 1024 -drive format=raw,file=rbd:data/squeeze  </p>
</blockquote>
<p><font color="red">RBD缓存</font>可以显著地提升性能。从QEMU 1.2起，QEMU的缓存选项可以控制librbd缓存：  </p>
<blockquote>
<p>qemu -m 1024 -drive format=rbd,file=rbd:data/squeeze,cache=writeback  </p>
</blockquote>
<p>如果你使用的是旧版的QEMU，你可以将librbd缓存配置（跟任何Ceph配置选项一样）设置为“文件”参数的一部分：  </p>
<blockquote>
<p>qemu -m 1024 -drive format=raw,file=rbd:data/squeeze:rbd_cache=true,cache=writeback  </p>
</blockquote>
<h3 id="启用-Discard-Trim"><a href="#启用-Discard-Trim" class="headerlink" title="启用 Discard/Trim"></a>启用 Discard/Trim</h3><p>从Ceph 0.46版本和QEMU 1.1版本开始，Ceph块设备支持丢弃操作。这意味着客户机可以发送裁剪（Trim）请求来让Ceph块设备回收未使用的空间。在客户机使用discard选项挂载ext4或XFS可以启用该功能。  </p>
<p>为了在客户机上可用，块设备必须显式启用该功能。要做到这些，你必须指明<code>discard_granularity</code>与设备关联：  </p>
<blockquote>
<p>qemu -m 1024 -drive format=raw,file=rbd:data/squeeze,id=drive1,if=none -device driver=ide-hd,drive=drive1,discard_granularity=512  </p>
</blockquote>
<p>注意，这里使用的是IDE设备。virtio设备不支持丢弃。  </p>
<p>如果使用libvirt，使用virsh编辑你的libvirt守护进程的配置文件，添加<code>xmlns:qemu</code>。然后，添加<code>qemu:commandline</code>块作为守护进程的子进程。下面的示例说明如何将emu id=的两个设备设置为不同的discard_particle值。  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;domain type=&apos;kvm&apos; xmlns:qemu=&apos;http://libvirt.org/schemas/domain/qemu/1.0&apos;&gt;</span><br><span class="line">        &lt;qemu:commandline&gt;</span><br><span class="line">                &lt;qemu:arg value=&apos;-set&apos;/&gt;</span><br><span class="line">                &lt;qemu:arg value=&apos;block.scsi0-0-0.discard_granularity=4096&apos;/&gt;</span><br><span class="line">                &lt;qemu:arg value=&apos;-set&apos;/&gt;</span><br><span class="line">                &lt;qemu:arg value=&apos;block.scsi0-0-1.discard_granularity=65536&apos;/&gt;</span><br><span class="line">        &lt;/qemu:commandline&gt;</span><br><span class="line">&lt;/domain&gt;</span><br></pre></td></tr></table></figure>
<h3 id="QEMU缓存选项"><a href="#QEMU缓存选项" class="headerlink" title="QEMU缓存选项"></a>QEMU缓存选项</h3><p>QEMU的缓存选项与下面的Ceph<font color="red">RBD缓存</font>设置相符合。  </p>
<p>Writeback：  </p>
<blockquote>
<p>rbd_cache = true</p>
</blockquote>
<p>Writethrough：  </p>
<blockquote>
<p>rbd_cache = true<br>rbd_cache_max_dirty = 0  </p>
</blockquote>
<p>None:  </p>
<blockquote>
<p>rbd_cache = false  </p>
</blockquote>
<p>QEMU的缓存设置会覆盖Ceph的缓存设置（包括在Ceph配置文件中明确设置的配置）。    </p>
<h2 id="libvirt与Ceph-RBD"><a href="#libvirt与Ceph-RBD" class="headerlink" title="libvirt与Ceph RBD"></a>libvirt与Ceph RBD</h2><p>libvirt库在管理程序接口和使用它们的软件应用程序之间创建了一个虚拟机抽象层。使用libvirt，在许多不同的管理程序间，开发者和系统管理员可以关注于通用管理框架、通用API和通用shell接口（即，virsh），包括：  </p>
<ul>
<li>QEMU/KVM</li>
<li>XEN</li>
<li>LXC</li>
<li>VirtualBox</li>
<li>等等  </li>
</ul>
<p>Ceph块设备支持QEMU/KVM。你可以通过使用libvirt接口的软件来使用Ceph块设备。下图描述了libvirt和QEMU如何通过librbd来使用Ceph块设备。  </p>
<p><img src="../images/Ceph_libvirt.png" alt="Ceph with libvirt">  </p>
<p>最常见的需要libvirt提供Ceph块设备的使用场景是云解决方案，例如OpenStack和CloudStack。云解决方案使用libvirt与QEMU/KVM进行交互，QEMU/KVM通过librbd与Ceph块设备进行交互。详见<a href="https://docs.ceph.com/docs/master/rbd/rbd-openstack" target="_blank" rel="noopener"><font color="red">块设备与OpenStack</font></a>和<a href="https://docs.ceph.com/docs/master/rbd/rbd-cloudstack" target="_blank" rel="noopener"><font color="red">块设备与CloudStack</font></a>。安装详见<a href="https://docs.ceph.com/docs/master/install" target="_blank" rel="noopener"><font color="red">安装</font></a>。  </p>
<p>你也可以通过Libvirt、virsh和libvirt API来使用Ceph块设备。详见<a href="http://www.libvirt.org/" target="_blank" rel="noopener"><font color="red">libvirt虚拟化API</font></a>。  </p>
<p>要使用Ceph块设备创建VMs，可使用以下章节中的操作。在接下来的示例中，我们使用libvirt-pool作为资源池名称，client.libvirt作为用户名，new-libvirt-image作为image名称。当然你可以使用你喜欢的名称，但请确保在随后的操作中执行命令时替换这些名称。  </p>
<h3 id="配置Ceph"><a href="#配置Ceph" class="headerlink" title="配置Ceph"></a>配置Ceph</h3><p>要配置Ceph以便libvirt使用，执行下列步骤：  </p>
<ol>
<li><a href="https://docs.ceph.com/docs/master/rados/operations/pools#create-a-pool" target="_blank" rel="noopener"><font color="red">创建资源池</font></a>。下面的示例是创建一个拥有128个归置组的名为libvirt-pool的资源池：  </li>
</ol>
<blockquote>
<p>ceph osd pool create libvirt-pool 128 128</p>
</blockquote>
<p>确认资源池已存在：  </p>
<blockquote>
<p>ceph osd lspools</p>
</blockquote>
<ol start="2">
<li>使用rbd工具RBD初始化资源池：  </li>
</ol>
<blockquote>
<p>rbd pool init \<pool-name></pool-name></p>
</blockquote>
<ol start="3">
<li><a href="https://docs.ceph.com/docs/master/rados/operations/user-management#add-a-user" target="_blank" rel="noopener"><font color="red">创建Ceph用户</font></a>。下面的示例是使用名为client.libvirt的用户和引用libvirt-pool：  </li>
</ol>
<blockquote>
<p>ceph auth get-or-create client.libvirt mon ‘profile rbd’ osd ‘profile rbd pool=libvirt-pool’</p>
</blockquote>
<p>确认用户已存在：  </p>
<blockquote>
<p>ceph auth ls</p>
</blockquote>
<ol start="4">
<li>使用QEMU在你的RBD池中<a href="https://docs.ceph.com/docs/master/rbd/qemu-rbd#creating-images-with-qemu" target="_blank" rel="noopener"><font color="red">创建image</font></a>。下面吗的示例是引用libvirt-pool创建名为new-libvirt-image的image：  </li>
</ol>
<blockquote>
<p>qemu-img create -f rbd rbd:libvirt-pool/new-libvirt-image 2G</p>
</blockquote>
<p>确认image以存在：  </p>
<blockquote>
<p>rbd -p libvirt-pool ls</p>
</blockquote>
<blockquote>
<p>你也可以使用[<font color="red">rbd create</font>]来创建image，但我们建议确保QEMU可以正常工作。  </p>
</blockquote>
<h3 id="准备VM管理员"><a href="#准备VM管理员" class="headerlink" title="准备VM管理员"></a>准备VM管理员</h3><p>你可能在没有VM管理员的情况下使用libvirt，但你会发现使用virt-manager来创建你的第一域更简单。  </p>
<ol>
<li>安装虚拟机管理员。详见[<font red="red">KVM/VirtManager</font>]。  </li>
</ol>
<blockquote>
<p>sudo apt-get install virt-manager</p>
</blockquote>
<ol start="2">
<li>如果需要，下载系统镜像。</li>
<li>启用虚拟机管理：  </li>
</ol>
<blockquote>
<p>sudo virt-manager</p>
</blockquote>
<h3 id="创建一个VM"><a href="#创建一个VM" class="headerlink" title="创建一个VM"></a>创建一个VM</h3><p>要使用virt-manager创建一个VM，执行以下步骤：  </p>
<ol>
<li>按下<strong>Create New Virtual Machine</strong>按钮</li>
<li>命名新的虚拟机域。在接下来的示例中，我们使用libvirt-virtual-machine名称。你也可以使用你想要的名称，但要确保在接下的命令行和配置示例中替换你选则的名称。  </li>
</ol>
<blockquote>
<p>libvirt-virtual-machine</p>
</blockquote>
<ol start="3">
<li>导入镜像</li>
</ol>
<blockquote>
<p>/path/to/image/recent-linux.img</p>
</blockquote>
<blockquote>
<p>注意，导入最近的镜像。某些老的镜像可能无法重新扫描为正确的虚拟设备。</p>
</blockquote>
<ol start="4">
<li>配置从启动VM</li>
<li>你可以使用<code>virsh list</code>来确认VM域已存在</li>
</ol>
<blockquote>
<p>sudo virsh list</p>
</blockquote>
<ol start="6">
<li>登录VM(root/root)</li>
<li>配置VM使用Ceph前需要先关闭</li>
</ol>
<h3 id="配置VM"><a href="#配置VM" class="headerlink" title="配置VM"></a>配置VM</h3><p>在为Ceph配置VM时，在适当的地方使用virsh是很重要的。此外，virsh命令通常需要root权限（例如，sudo）且不会返回适当的结果，也不会通知你需要root权限。关于virsh命令的引用，详见<a href="http://www.libvirt.org/virshcmdref.html" target="_blank" rel="noopener"><font color="red">Virsh Command Reference</font></a>。  </p>
<ol>
<li>使用virsh edit打开配置文件。  </li>
</ol>
<blockquote>
<p>sudo virsh edit {vm-domain-name}</p>
</blockquote>
<p>\<devices>下应该有\<disk>条目。  </disk></devices></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&lt;devices&gt;</span><br><span class="line">        &lt;emulator&gt;/usr/bin/kvm&lt;/emulator&gt;</span><br><span class="line">        &lt;disk type=&apos;file&apos; device=&apos;disk&apos;&gt;</span><br><span class="line">                &lt;driver name=&apos;qemu&apos; type=&apos;raw&apos;/&gt;</span><br><span class="line">                &lt;source file=&apos;/path/to/image/recent-linux.img&apos;/&gt;</span><br><span class="line">                &lt;target dev=&apos;vda&apos; bus=&apos;virtio&apos;/&gt;</span><br><span class="line">                &lt;address type=&apos;drive&apos; controller=&apos;0&apos; bus=&apos;0&apos; unit=&apos;0&apos;/&gt;</span><br><span class="line">        &lt;/disk&gt;</span><br></pre></td></tr></table></figure>
<p>使用OS镜像的路径替换<code>/path/to/image/recent-linux.img</code>。使用更快的virtio的最小内核版本为2.6.25。详见<a href="http://www.linux-kvm.org/page/Virtio" target="_blank" rel="noopener"><font color="red">Virtio</font></a>。  </p>
<blockquote>
<p><strong>重要</strong>：使用<code>sudo virsh edit</code>而不是文本编辑器。如果你使用文本编辑器编辑<code>/etc/libvirt/qemu</code>目录下的配置文件，libvirt可能不会感知到改变。如果<code>/etc/libvirt/qemu</code>目录下的XML文件内容与<code>sudo virsh dumpxml {vm-domain-name}</code>的返回结果有差异，你的VM可能不会正常工作。  </p>
</blockquote>
<ol start="2">
<li>将你创建的Ceph RBD image作为\<disk>条目添加  </disk></li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&lt;disk type=&apos;network&apos; device=&apos;disk&apos;&gt;</span><br><span class="line">        &lt;source protocol=&apos;rbd&apos; name=&apos;libvirt-pool/new-libvirt-image&apos;&gt;</span><br><span class="line">                &lt;host name=&apos;&#123;monitor-host&#125;&apos; port=&apos;6789&apos;/&gt;</span><br><span class="line">        &lt;/source&gt;</span><br><span class="line">        &lt;target dev=&apos;vda&apos; bus=&apos;virtio&apos;/&gt;</span><br><span class="line">&lt;/disk&gt;</span><br></pre></td></tr></table></figure>
<p>使用你的主机名替换{monitor-host}，替换资源池或image名称也是必要。你也可以为你的Ceph监视器添加多个\<host>条目。逻辑设备的属性可在你的VM的<code>/dev</code>目录下找到。bus属性选项指示要模拟的硬盘设备的类型。驱动的有效设置是明确的（即，“ide”，“scsi”，“virtio”，“xen”，“usb”或“sata”）。  </host></p>
<p>\<disk>元素及其子元素和属性，详见<a href="http://www.libvirt.org/formatdomain.html#elementsDisks" target="_blank" rel="noopener"><font color="red">Disks</font></a>。  </disk></p>
<ol start="3">
<li>保存文件</li>
<li>如果你的Ceph存储集群已启用<a href="https://docs.ceph.com/docs/master/rados/configuration/auth-config-ref" target="_blank" rel="noopener"><font color="red">Ceph 认证</font></a>（默认启用），你必须生成密码  </li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">cat &gt; secret.xml &lt;&lt;EOF</span><br><span class="line">&lt;secret ephemeral=<span class="string">'no'</span> private=<span class="string">'no'</span>&gt;</span><br><span class="line">        &lt;usage <span class="built_in">type</span>=<span class="string">'ceph'</span>&gt;</span><br><span class="line">                &lt;name&gt;client.libvirt secret&lt;/name&gt;</span><br><span class="line">        &lt;/usage&gt;</span><br><span class="line">&lt;/secret&gt;</span><br><span class="line">EOF</span><br></pre></td></tr></table></figure>
<ol start="5">
<li>定义密码  </li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo virsh secret-define --file secret.xml</span><br><span class="line">&lt;uuid of secret is output here&gt;</span><br></pre></td></tr></table></figure>
<ol start="6">
<li>获取并保存client.libvirt密钥到文件中  </li>
</ol>
<blockquote>
<p>ceph auth get-key client.libvirt | sudo tee client.libvirt.key</p>
</blockquote>
<ol start="7">
<li>设置密码的UUID  </li>
</ol>
<blockquote>
<p>sudo virsh secret-set-value –secret {uuid of secret} –base64 $(cat client.libvirt.key) &amp;&amp; rm client.libvirt.key secret.xml</p>
</blockquote>
<p>你必须通过手动添加下面的\<auth>条目到你之前创建的\<disk>元素的方式来设置密码（UUID的值用从上述命令得到的结果替换）。  </disk></auth></p>
<blockquote>
<p>sudo virsh edit {vm-domain-name}  </p>
</blockquote>
<p>然后添加\<auth>\</auth>元素到域配置文件中：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">&lt;/source&gt;</span><br><span class="line">&lt;auth username=&apos;libvirt&apos;&gt;</span><br><span class="line">        &lt;secret type=&apos;ceph&apos; uuid=&apos;9ec59067-fdbc-a6c0-03ff-df165c0587b8&apos;/&gt;</span><br><span class="line">&lt;/auth&gt;</span><br><span class="line">&lt;target ...</span><br></pre></td></tr></table></figure>
<blockquote>
<p><strong>注意</strong>：示例ID是libvirt，而不是第二部<font color="red">配置Ceph</font>生成的Ceph名称 client.libvirt。确保你使用的Ceph名称的ID组件是你生成的。如果由于某些原因你需要重新生成密码。在你再次执行<code>sudo virsh secret-set-value</code>命令之前，你必须要先执行<code>sudo virsh secret-undefine {uuid}</code>命令。  </p>
</blockquote>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>在你配置你的VM使用Ceph之前，你可以先启动VM。建议先确认VM和Ceph状态，你可以使用下面的操作。  </p>
<ol>
<li>检查Ceph是否运行：  </li>
</ol>
<blockquote>
<p>ceph health  </p>
</blockquote>
<ol start="2">
<li>检查VM是否运行：  </li>
</ol>
<blockquote>
<p>sudo virsh list</p>
</blockquote>
<ol start="3">
<li>检查VM是否可以访问Ceph。使用的VM域替换{vm-domain-name}：  </li>
</ol>
<blockquote>
<p>sudo virsh qemu-monitor-command –hmp {vm-domain-name} ‘info block’  </p>
</blockquote>
<ol start="4">
<li>检查<code>&lt;target dev=&#39;hdb&#39; bus=&#39;ide&#39;/&gt;</code>下的驱动是否存在/dev或/proc/partitions下：  </li>
</ol>
<blockquote>
<p>ls /dev<br>cat /proc/patitions</p>
</blockquote>
<p>如果每一步都成功，那么你可以在你的VM中使用Ceph块设备了。</p>
<h2 id="块设备与OpenStack"><a href="#块设备与OpenStack" class="headerlink" title="块设备与OpenStack"></a>块设备与OpenStack</h2><p>你可以通过libvirt在OpenStack中使用Ceph块设备images，将QEMU接口配置为librbd。Ceph在集群中将块设备images条带化为对象，这意味着大型的Ceph块设备images拥有比单独的服务器更好的性能！  </p>
<p>要在OpenStack中使用Ceph块设备，首先你必须安装QEMU，libvirt和OpenStack。我们建议你使用单独的物理设备安装OpenStack。OpenStack建议最小8GB RAM和四核处理器。下图描绘了OpenStack/Ceph的技术栈。  </p>
<p><img src="../images/Ceph_OpenStack.png" alt="Ceph with OpenStack">  </p>
<p>OpenStack的三个部分与Ceph块设备集成：  </p>
<ul>
<li><strong>Images</strong>：OpenStack Glance管理着VMs的镜像。镜像是不可变的。OpenStack将镜像视为二进制块并对应的下载它们。</li>
<li><strong>Volumes</strong>：Volumes是块设备。OpenStack使用volume引导VMs，或挂载volumes来运行VMs。OpenStack使用Cinder服务管理volumes。</li>
<li><strong>Guest Disks</strong>：客户硬盘时客户操作系统的硬盘。默认情况下，当你启动虚拟机时，它的硬盘作为文件出现在虚拟机监控程序的文件系统上（通常在/var/lib/nova/instances/\<uuid>/下）。在OpenStack Havana之前，在Ceph中启动VM的唯一方法是使用Cinder的boot-from-volume功能。然后，现在不使用Cinder，直接在Ceph中启动每个虚拟机成为可能，这是非常有利的，应为它允许你使用实时迁移过程轻松地执行维护操作。此外，如果你的管理程序失效，还可以方便地触发nova疏散并在其它地方几乎无缝地运行虚拟机。  </uuid></li>
</ul>
<p>你也可以在Ceph块设备中使用OpenStack Glance来存储镜像，并且你可以通过写时复制克隆一个image来使用Cinder引导VM。  </p>
<p>接下来的详细介绍Glance，Cinder和Nova，尽管它们不必一起使用。你可以在本地磁盘运行VM时将image存储到Ceph块设备中，反之亦可。  </p>
<h3 id="创建资源池"><a href="#创建资源池" class="headerlink" title="创建资源池"></a>创建资源池</h3><p>默认情况下，Ceph块设备使用rbd资源池。你也可以使用其它可用的资源池。我们建议你问Cinder创建一个资源池，也为Glance创建一个资源池。确保你的Ceph集群在运行中，然后创建资源池：  </p>
<blockquote>
<p>ceph osd pool create volumes 128<br>ceph osd pool create images 128<br>ceph osd pool create backups 128<br>ceph osd pool create vms 128  </p>
</blockquote>
<p>资源池中归置组数量的说明详见<a href="https://docs.ceph.com/docs/master/rados/operations/pools#createpool" target="_blank" rel="noopener"><font color="red">Create a Pool</font></a>，资源池中你应该设置的归置组数量详见<a href="https://docs.ceph.com/docs/master/rados/operations/placement-groups" target="_blank" rel="noopener"><font color="red">Placement Groups</font></a>。  </p>
<p>新创建的资源池在使用之前必须初始化。使用rbd工具初始化这些资源池：  </p>
<blockquote>
<p>rbd pool init volumes<br>rbd pool init images<br>rbd pool init backups<br>rbd pool init vms</p>
</blockquote>
<h3 id="配置OpenStack-Ceph客户端"><a href="#配置OpenStack-Ceph客户端" class="headerlink" title="配置OpenStack Ceph客户端"></a>配置OpenStack Ceph客户端</h3><p>运行glance-api、cinder-volume、nova-compute和cinder-backup的节点作为Ceph客户端。每个节点都需要ceph.conf文件：  </p>
<blockquote>
<p>ssh {your-openstack-server} sudo tee /etc/ceph/ceph.conf &lt;/etc/ceph/ceph.conf  </p>
</blockquote>
<h4 id="安装Ceph客户端包"><a href="#安装Ceph客户端包" class="headerlink" title="安装Ceph客户端包"></a>安装Ceph客户端包</h4><p>在glance-api节点，你需要librbd的Python包：  </p>
<blockquote>
<p>sudo apt-get install python-rbd<br>sudo yum install python-rbd</p>
</blockquote>
<p>在nove-compute、cinder-backup和cinder-volume节点，需要同时安装Python包和客户端命令行工具：  </p>
<blockquote>
<p>sudo apt-get install ceph-common<br>sudo yum install ceph-common</p>
</blockquote>
<h4 id="安装Ceph-客户端认证"><a href="#安装Ceph-客户端认证" class="headerlink" title="安装Ceph 客户端认证"></a>安装Ceph 客户端认证</h4><p>如果你已经启用了<a href="https://docs.ceph.com/docs/master/rados/configuration/auth-config-ref/#enabling-disabling-cephx" target="_blank" rel="noopener"><font color="red">cephx authentication</font></a>,需要为Nova/Cinder和Glance创建用户。执行以下命令：  </p>
<blockquote>
<p>ceph auth get-or-create client.glance mon ‘profile rbd’ osd ‘profile rbd pool=images’<br>ceph auth get-or-create client.cinder mon ‘profile rbd’ osd ‘profile rbd pool=volumes, profile rbd pool=vms, profile rbd-read-only pool=images’<br>ceph auth get-or-create client.cinder-backup mon ‘profile rbd’ osd ‘profile rbd pool=backups’</p>
</blockquote>
<p>将client.cinder、client.glance、client.cinder-backup的密钥环添加到合适的节点并改变它们的从属关系：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ceph auth get-or-create client.glance | ssh &#123;your-glance-api-server&#125; sudo tee /etc/ceph/ceph.client.glance.keyring</span><br><span class="line">ssh &#123;your-glance-api-server&#125; sudo chown glance:glance /etc/ceph/ceph.client.glance.keyring</span><br><span class="line">ceph auth get-or-create client.cinder | ssh &#123;your-volume-server&#125; sudo tee /etc/ceph/ceph.client.cinder.keyring</span><br><span class="line">ssh &#123;your-cinder-volume-server&#125; sudo chown cinder:cinder /etc/ceph/ceph.client.cinder.keyring</span><br><span class="line">ceph auth get-or-create client.cinder-backup | ssh &#123;your-cinder-backup-server&#125; sudo tee /etc/ceph/ceph.client.cinder-backup.keyring</span><br><span class="line">ssh &#123;your-cinder-backup-server&#125; sudo chown cinder:cinder /etc/ceph/ceph.client.cinder-backup.keyring</span><br></pre></td></tr></table></figure>
<p>运行nova-compute的节点需要nova-compute进程的密钥环文件：  </p>
<blockquote>
<p>ceph auth get-or-create client.cinder | ssh {your-nova-compute-server} sudo tee /etc/ceph/ceph.client.cinder.keyring</p>
</blockquote>
<p>在libvirt中，也需要存储client.cinder用户的密码。在从Cinder附加一个块设备时，libvirt需要它来访问集群。  </p>
<p>在运行nova-compute的节点创建临时密码副本：  </p>
<blockquote>
<p>ceph auth get-key client.cinder | ssh {your-compute-node} tee client.cinder.key</p>
</blockquote>
<p>然后，在计算节点上，添加密码到libvirt中并移除临时密钥副本：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">uuidgen</span><br><span class="line">457eb676-33da-42ec-9a8c-9293d545c337</span><br><span class="line"></span><br><span class="line">cat &gt; secret.xml &lt;&lt;EOF</span><br><span class="line">&lt;secret ephemeral=<span class="string">'no'</span> private=<span class="string">'no'</span>&gt;</span><br><span class="line">  &lt;uuid&gt;457eb676-33da-42ec-9a8c-9293d545c337&lt;/uuid&gt;</span><br><span class="line">  &lt;usage <span class="built_in">type</span>=<span class="string">'ceph'</span>&gt;</span><br><span class="line">    &lt;name&gt;client.cinder secret&lt;/name&gt;</span><br><span class="line">  &lt;/usage&gt;</span><br><span class="line">&lt;/secret&gt;</span><br><span class="line">EOF</span><br><span class="line">sudo virsh secret-define --file secret.xml</span><br><span class="line">Secret 457eb676-33da-42ec-9a8c-9293d545c337 created</span><br><span class="line">sudo virsh secret-set-value --secret 457eb676-33da-42ec-9a8c-9293d545c337 --base64 $(cat client.cinder.key) &amp;&amp; rm client.cinder.key secret.xml</span><br></pre></td></tr></table></figure>
<p>保存密码的UUID以便后续配置nova-compute。  </p>
<h3 id="配置OpenStack使用Ceph"><a href="#配置OpenStack使用Ceph" class="headerlink" title="配置OpenStack使用Ceph"></a>配置OpenStack使用Ceph</h3><h4 id="配置Glance"><a href="#配置Glance" class="headerlink" title="配置Glance"></a>配置Glance</h4><p>Glance可以使用多个后端来存储镜像。要默认使用Ceph块设备，如下配置Glance。  </p>
<h5 id="KILO-and-After"><a href="#KILO-and-After" class="headerlink" title="KILO and After"></a>KILO and After</h5><p>编辑<code>/etc/glance/glance-api.conf</code>并新增如下[glance_store]章节：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">[glance_store]</span><br><span class="line">stores = rbd</span><br><span class="line">default_store = rbd</span><br><span class="line">rbd_store_pool = images</span><br><span class="line">rbd_store_user = glance</span><br><span class="line">rbd_store_ceph_conf = /etc/ceph/ceph.conf</span><br><span class="line">rbd_store_chunk_size = 8</span><br></pre></td></tr></table></figure>
<p>更多有关Glance中可用的配置选项，请查看OpenStack配置应用：<a href="http://docs.openstack.org/" target="_blank" rel="noopener"><font color="red">http://docs.openstack.org/</font></a>。  </p>
<h5 id="启用镜像的写时复制"><a href="#启用镜像的写时复制" class="headerlink" title="启用镜像的写时复制"></a>启用镜像的写时复制</h5><p>注意这会通过Glance的API公开后端位置，所以启用此选项的终端不应公开访问。  </p>
<p>除了Mitaka外的所有OpenStack发行版  </p>
<p>如果你想启用镜像的写时复制克隆，在[DEFAULT]章节下新增：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">show_image_direct_url = True</span><br></pre></td></tr></table></figure>
<h5 id="禁用缓存管理（所有OpenStack发行版）"><a href="#禁用缓存管理（所有OpenStack发行版）" class="headerlink" title="禁用缓存管理（所有OpenStack发行版）"></a>禁用缓存管理（所有OpenStack发行版）</h5><p>假设你的配置文件中有<code>flavor = keystone+cachemanagement</code>，禁用Glance的缓存管理以免在<code>/var/lib/glance/image-cache/</code>下缓存镜像：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[paste_deploy]</span><br><span class="line">flavor = keystone</span><br></pre></td></tr></table></figure>
<h5 id="镜像属性"><a href="#镜像属性" class="headerlink" title="镜像属性"></a>镜像属性</h5><p>我们建议为你的镜像使用如下属性：  </p>
<ul>
<li>hw_scsi_model=virtio-scsi：添加virtio-scsi控制，获得更好的性能，支持丢弃选项</li>
<li>hw_disk_bus=scsi：连接每个Cinder块设备到控制器</li>
<li>hw_qemu_guest_agent=yes：启用QEMU客户代理</li>
<li>os_require_quiesce=yes：通过QEMU客户代理发送fs-freeze/thaw请求  </li>
</ul>
<h4 id="配置Cinder"><a href="#配置Cinder" class="headerlink" title="配置Cinder"></a>配置Cinder</h4><p>OpenStack需要一个驱动与Ceph块设备进行交互。你必须为块设备指明资源池名称。在你的OpenStack节点上，编辑<code>/etc/cinder/cinder.conf</code>，添加：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">[DEFAULT]</span><br><span class="line">...</span><br><span class="line">enabled_backends = ceph</span><br><span class="line">glance_api_version = 2</span><br><span class="line">...</span><br><span class="line">[ceph]</span><br><span class="line">volume_driver = cinder.volume.drivers.rbd.RBDDriver</span><br><span class="line">volume_backend_name = ceph</span><br><span class="line">rbd_pool = volumes</span><br><span class="line">rbd_ceph_conf = /etc/ceph/ceph.conf</span><br><span class="line">rbd_flatten_volume_from_snapshot = false</span><br><span class="line">rbd_max_clone_depth = 5</span><br><span class="line">rbd_store_chunk_size = 4</span><br><span class="line">rados_connect_timeout = -1</span><br></pre></td></tr></table></figure>
<p>如果你已经启用了<a href="https://docs.ceph.com/docs/master/rados/configuration/auth-config-ref/#enabling-disabling-cephx" target="_blank" rel="noopener"><font color="red">cephx authentication</font></a>，如之前所述，你还要配置你添加的密码的用户和UUID：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[ceph]</span><br><span class="line">...</span><br><span class="line">rbd_user = cinder</span><br><span class="line">rbd_secret_uuid = 457eb676-33da-42ec-9a8c-9293d545c337</span><br></pre></td></tr></table></figure>
<p>注意，如果你配置了多个Cinder后端，在[DEFAULT]章节下必须设置glance_api_version = 2。  </p>
<h4 id="配置Cinder备份"><a href="#配置Cinder备份" class="headerlink" title="配置Cinder备份"></a>配置Cinder备份</h4><p>OpenStack Cinder Backup要求一个特殊的守护进程，所以别忘了安装它。在你的Cinder Backup节点，编辑<code>/etc/cinder/cinder.conf</code>并新增：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">backup_driver = cinder.backup.drivers.ceph</span><br><span class="line">backup_ceph_conf = /etc/ceph/ceph.conf</span><br><span class="line">backup_ceph_user = cinder-backup</span><br><span class="line">backup_ceph_chunk_size = 134217728</span><br><span class="line">backup_ceph_pool = backups</span><br><span class="line">backup_ceph_stripe_unit = 0</span><br><span class="line">backup_ceph_stripe_count = 0</span><br><span class="line">restore_discard_excess_bytes = true</span><br></pre></td></tr></table></figure>
<h4 id="为附加Ceph-RBD块设备配置Nova"><a href="#为附加Ceph-RBD块设备配置Nova" class="headerlink" title="为附加Ceph RBD块设备配置Nova"></a>为附加Ceph RBD块设备配置Nova</h4><p>为了附件Cinder设备（无论是普通的块设备还是从volume中引导），你必须告诉Nova（和libvirt）在附加设备时引用的是哪个用户和UUID。在连接和认证Ceph集群时，libvirt将使用这个用户。  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[libvirt]</span><br><span class="line">...</span><br><span class="line">rbd_user = cinder</span><br><span class="line">rbd_secret_uuid = 457eb676-33da-42ec-9a8c-9293d545c337</span><br></pre></td></tr></table></figure>
<p>这两个标志会被Nova临时后端使用。  </p>
<h4 id="配置Nova"><a href="#配置Nova" class="headerlink" title="配置Nova"></a>配置Nova</h4><p>为了直接从Ceph中引导所有的虚拟机，你必须为Nova配置临时后端。  </p>
<p>建议在你的Ceph配置文件中（从Giant开始默认启用）启用RBD缓存。例外，启用管理员套接字在定位问题时将带来很多好处。使用Ceph块设备时为每一个虚拟机设置一个套接字将有助于调查性能和错误行为。  </p>
<p>像下面这样反问套接字：  </p>
<blockquote>
<p>ceph daemon /var/run/ceph/ceph-client.cinder.19195.32310016.asok help</p>
</blockquote>
<p>现在，在每个计算节点上编辑你的Ceph配置文件：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">[client]</span><br><span class="line">    rbd cache = true</span><br><span class="line">    rbd cache writethrough until flush = true</span><br><span class="line">    admin socket = /var/run/ceph/guests/$cluster-$type.$id.$pid.$cctid.asok</span><br><span class="line">    log file = /var/log/qemu/qemu-guest-$pid.log</span><br><span class="line">    rbd concurrent management ops = 20</span><br></pre></td></tr></table></figure>
<p>配置这些路径的权限：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mkdir -p /var/run/ceph/guests/ /var/<span class="built_in">log</span>/qemu/</span><br><span class="line">chown qemu:libvirtd /var/run/ceph/guests /var/<span class="built_in">log</span>/qemu/</span><br></pre></td></tr></table></figure>
<p>注意，QEMU用户和libvirt组可能因系统而异。提供的例子是基于RedHat系统工作的。  </p>
<h4 id="重启OpenStack"><a href="#重启OpenStack" class="headerlink" title="重启OpenStack"></a>重启OpenStack</h4><p>为了激活Ceph块设备驱动和导入配置中的块设备资源池，你必须重启OpenStack。对于基于Debian系统，在合适的节点执行下列命令：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo glance-control api restart</span><br><span class="line">sudo service nova-compute restart</span><br><span class="line">sudo service cinder-volume restart</span><br><span class="line">sudo service cinder-backup restart</span><br></pre></td></tr></table></figure>
<p>对于基于RedHa他系统，执行：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sudo service openstack-glance-api restart</span><br><span class="line">sudo service openstack-nova-compute restart</span><br><span class="line">sudo service openstack-cinder-volume restart</span><br><span class="line">sudo service openstack-cinder-backup restart</span><br></pre></td></tr></table></figure>
<p>一旦OpenStack上线运行，你应该能够创建卷并从中引导启动。  </p>
<h4 id="从块设备中引导启动"><a href="#从块设备中引导启动" class="headerlink" title="从块设备中引导启动"></a>从块设备中引导启动</h4><p>使用Cinder命令行工具你可以从image中创建一个卷：  </p>
<blockquote>
<p>cinder create –image-id {id of image} –display-name {name of volume} {size of volume}</p>
</blockquote>
<p>你可以使用<a href="https://docs.ceph.com/docs/master/rbd/qemu-rbd/#running-qemu-with-rbd" target="_blank" rel="noopener"><font color="red">qemu-img</font></a>进行格式转换。例如：  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">qemu-img convert -f &#123;<span class="built_in">source</span>-format&#125; -O &#123;output-format&#125; &#123;<span class="built_in">source</span>-filename&#125; &#123;output-filename&#125;</span><br><span class="line">qemu-img convert -f qcow2 -O raw precise-cloudimg.img precise-cloudimg.raw</span><br></pre></td></tr></table></figure>
<p>当Glance和Cinder都使用Ceph块设备，因为image是写时复制克隆，所以它可以快速地创建一个新的卷。在OpenStack dashboard中，通过下列步骤，你可以从卷中启动：  </p>
<ol>
<li>创建一个新的实例</li>
<li>选择与写时复制克隆相关联的image</li>
<li>选择“从卷启动”</li>
<li>选择你创建的卷</li>
</ol>
<h2 id="Ceph-iSCSI网关"><a href="#Ceph-iSCSI网关" class="headerlink" title="Ceph iSCSI网关"></a>Ceph iSCSI网关</h2><p>iSCSI网关是Ceph存储与iSCSI标准融为一体，以提供高可用(HA)的iSCSI target，来将RADOS块设备导出为SCSI硬盘。iSCSI协议运行客户端（initiators）通过TCP/IP网络发送SCSI命令给SCSI存储设备（target）。这使得各种客户端，例如Microsoft Windows，可以访问Ceph存储集群。  </p>
<p>每个iSCSI网关都运行在Linux IO target内核子系统（LIO）内以提供iSCSI协议支持。LIO利用用户空间passthrough（TCMU）与Ceph的librbd库交互，并向iSCSI客户端导出RBD image。使用Ceph的iSCSI网关，你可以有效地运行一个完全集成的块设备基础设施，它具有传统存储区域网络（SAN）的所有特性和优点。  </p>
<p><img src="../images/Ceph_iSCSI_Gateway.png" alt="Ceph iSCSI GateWay">  </p>
<h3 id="ISCSI网关要求"><a href="#ISCSI网关要求" class="headerlink" title="ISCSI网关要求"></a>ISCSI网关要求</h3><p>要实现Ceph iSCSI网关，需要满足一些要求。高可用的Ceph iSCSI网关解决方案推荐使用2到4个iSCSI网关节点。  </p>
<p>关于推荐硬件，详见<a href="https://docs.ceph.com/docs/master/start/hardware-recommendations/#hardware-recommendations" target="_blank" rel="noopener"><font color="red">Hardware Recommendations</font></a>。  </p>
<p>对于Ceph Monitors或OSDs来说，iSCSI网关选项没有特殊说明，降低检查OSDs的默认定时器是很重要的，这将减少启动器超时的可能性。对于存储集群中的每个OSD节点，建议使用以下配置选项：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[osd]</span><br><span class="line">osd heartbeat grace = 20</span><br><span class="line">osd heartbeat interval = 5</span><br></pre></td></tr></table></figure>
<ul>
<li>使用Ceph Monitor在线更新：  </li>
</ul>
<blockquote>
<p>ceph tell \&lt;daemon_type&gt;.\<id> config set \&lt;parameter_name&gt; \&lt;new_value&gt;  </id></p>
</blockquote>
<blockquote>
<p>ceph tell osd.0 config set osd_heartbeat_grace 20<br>ceph tell osd.0 config set osd_heartbeat_interval 5  </p>
</blockquote>
<ul>
<li>在OSD节点在线更新  </li>
</ul>
<blockquote>
<p>ceph daemon \&lt;daemon_type&gt;.\<id> config set osd_client_watch_timeout 15  </id></p>
</blockquote>
<blockquote>
<p>ceph daemon osd.0 config set osd_heartbeat_grace 20<br>ceph daemon osd.0 config set osd_heartbeat_interval 5  </p>
</blockquote>
<p>更多Ceph配置选项，参见<a href="https://docs.ceph.com/docs/master/rados/configuration/ceph-conf/#configuring-ceph" target="_blank" rel="noopener"><font color="red">配置Ceph</font></a>。  </p>
<h3 id="ISCSI-Targets"><a href="#ISCSI-Targets" class="headerlink" title="ISCSI Targets"></a>ISCSI Targets</h3><p>传统上来说，对Ceph存储集群的块级别的访问仅限于QEMU和librbd，这样是OpenStack环境中采用的关键因素。从Ceph Luminous版本开始，块级别的访问正在扩展，以提供标准的iSCSI支持，允许更广泛的平台使用，并可能打开新的使用场景。  </p>
<ul>
<li>Red Hat Enterprise Linux/CentOS 7.5 (or newer); Linux kernel v4.16 (or newer)</li>
<li>使用ceph-ansible或使用命令行部署的工作Ceph存储集群</li>
<li>iSCSI网关节点，可以与OSD节点或专用节点共存</li>
<li>为iSCSI前端流量和Ceph后端流量分离网络子网  </li>
</ul>
<h4 id="使用ansible配置ISCSI-Target"><a href="#使用ansible配置ISCSI-Target" class="headerlink" title="使用ansible配置ISCSI Target"></a>使用ansible配置ISCSI Target</h4><p>Ceph iSCSI网关既是iSCSI target节点，也是Ceph client节点。Ceph iSCSI网关可以是一个单独的节点，也可以托管于Ceph对象存储磁盘（OSD）节点。完成以下步骤后，将安装并配置Ceph iSCSI网关进行基本操作。  </p>
<p><strong>要求</strong>：  </p>
<ul>
<li>运行中的Ceph Luminous(12.2.x)或更新的集群</li>
<li>Red Hat Enterprise Linux/CentOS 7.5 (or newer); Linux kernel v4.16 (or newer)</li>
<li>在所有iSCSI网关节点上安装<code>ceph-iscsi</code>包  </li>
</ul>
<p><strong>安装</strong>：  </p>
<p>在Ansible安装节点，可以是管理节点或专业部署节点，执行以下操作：  </p>
<ol>
<li>使用<code>root</code>安装<code>ceph-ansible</code>包：  </li>
</ol>
<pre><code>&gt; # yum install ceph-ansible
</code></pre><ol start="2">
<li><p>在<code>/etc/ansible/hosts</code>文件中未网关组添加条目：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">[iscsigws]</span><br><span class="line">ceph-igw-1</span><br><span class="line">ceph-igw-2</span><br></pre></td></tr></table></figure>
</li>
</ol>
<p><strong>配置</strong>：  </p>
<p>ceph-ansible包在<code>/usr/share/ceph-ansible/group_vars/</code>目录下放置了一个名为<code>iscsigws.yml.sample</code>的文件。创建一个这个示例文件的副本，命名为<code>iscsigws.yml</code>。查看以下Ansible变量和说明，并进行相应更新。可在iscsigws.yml.sample文件查阅高级变量的完整列表。  </p>
<table>
<thead>
<tr>
<th style="text-align:center"><strong>变量</strong></th>
<th style="text-align:center"><strong>意义/目的</strong></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">seed_monitor</td>
<td style="text-align:center">由于RADOS和rbd的调用，每个网关都需要访问ceph集群。这意味着iSCSI网关必须定义一个适当的/etc/ceph/目录。seed_monitor主机用于填充iSCSI网关的/etc/ceph/目录。</td>
</tr>
<tr>
<td style="text-align:center">cluster_name</td>
<td style="text-align:center">定义一个惯用的存储集群名称。</td>
</tr>
<tr>
<td style="text-align:center">gateway_keyring</td>
<td style="text-align:center">定义一个惯用的密钥环名称。</td>
</tr>
<tr>
<td style="text-align:center">deploy_setting</td>
<td style="text-align:center">如设置为True，则在运行playbook时部署设置。</td>
</tr>
<tr>
<td style="text-align:center">perform_system_checks</td>
<td style="text-align:center">这是一个布尔值，用于检查每个网关上的多路径和lvm配置设置。至少在第一次运行时，必须将其设置为true，以确保正确配置了multipathd和lvm。</td>
</tr>
<tr>
<td style="text-align:center">api_user</td>
<td style="text-align:center">API的用户名。默认为<em>admin</em>。</td>
</tr>
<tr>
<td style="text-align:center">api_password</td>
<td style="text-align:center">使用API的密码。默认为<em>admin</em>。</td>
</tr>
<tr>
<td style="text-align:center">api_port</td>
<td style="text-align:center">使用API的TCP端口。默认为5000.</td>
</tr>
<tr>
<td style="text-align:center">api_secure</td>
<td style="text-align:center">设为True时，必须使用TLS。默认为<em>false</em>。如果为True，用户必须创建必要的证书和密钥文件。详见gwcli man文件。</td>
</tr>
<tr>
<td style="text-align:center">trusted_ip_list</td>
<td style="text-align:center">有权访问API的IPV4或IPV6列表。默认情况下，只有iSCSI网关节点有权访问。</td>
</tr>
</tbody>
</table>
<p><strong>部署</strong>：  </p>
<p>在Ansible安装节点，执行以下操作：  </p>
<ol>
<li><p>使用root执行Ansible手册：  </p>
<blockquote>
<h1 id="cd-usr-share-ceph-ansible"><a href="#cd-usr-share-ceph-ansible" class="headerlink" title="cd /usr/share/ceph-ansible"></a>cd /usr/share/ceph-ansible</h1><h1 id="ansible-playbook-site-yml-–limit-iscsigws"><a href="#ansible-playbook-site-yml-–limit-iscsigws" class="headerlink" title="ansible-playbook site.yml –limit iscsigws"></a>ansible-playbook site.yml –limit iscsigws</h1></blockquote>
</li>
<li><p>从iSCSI网关节点验证配置：  </p>
<blockquote>
<h1 id="gwcli-ls"><a href="#gwcli-ls" class="headerlink" title="gwcli ls"></a>gwcli ls</h1></blockquote>
</li>
</ol>
<p><strong>服务管理</strong>：  </p>
<p>ceph-iscsi包安装了配置管理逻辑和名为<code>rbd-target-api</code>的systemd服务。启用服务后，rbd-target-api会在引导时启动并恢复Linux IO状态。在部署期间，Ansible playbook禁用target 服务。下面是使用rbd-target-api systemd服务交互的结果：  </p>
<blockquote>
<p># systemctl \&lt;start|stop|restart|reload&gt; rbd-target-api  </p>
</blockquote>
<ul>
<li>reload  </li>
</ul>
<p>$\qquad$reload请求强制rbd-target-api重读配置并在当前环境下应用它。这不是常用请求，因为更改将从Ansible平行部署到所有的iSCSI网关节点。</p>
<ul>
<li>stop  </li>
</ul>
<p>$\qquad$stop请求将关闭网关的进出口，丢弃到客户端的连接并且从内核中擦除当前的LIO连接。这会将iSCSI网关返回到干净状态。当客户端断开连接时，客户端多路径层将活动I / O重新调度到其他iSCSI网关。  </p>
<p><strong>移除配置</strong>：  </p>
<p>ceph-ansible包提供了一个Ansible手册，用于移除iSCSI网关配置和连接RBD images。Ansible手册是<code>/usr/share/ceph-ansible/purge_gateways.yml</code>。当执行Ansible playbook时，系统会提示选择清除操作的类型：  </p>
<p><em>lio</em>：  </p>
<p>在该模式下，所有iSCSI网关定义的LIO配置都将被清除。Ceph存储集群中创建的硬盘将保持不变。   </p>
<p><em>all</em>：  </p>
<p>当选则all时，与LIO配置一同移除还有iSCSI网关环境中定义的所有RBD images，其它不相关的RBD images并不会被移除。确保选择了正确的模式，该操作将删除所有数据。  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">[root@rh7-iscsi-client ceph-ansible]<span class="comment"># ansible-playbook purge_gateways.yml</span></span><br><span class="line">Which configuration elements should be purged? (all, lio or abort) [abort]: all</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">PLAY [Confirm removal of the iSCSI gateway configuration] *********************</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">GATHERING FACTS ***************************************************************</span><br><span class="line">ok: [localhost]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">TASK: [Exit playbook <span class="keyword">if</span> user aborted the purge] *******************************</span><br><span class="line">skipping: [localhost]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">TASK: [set_fact ] *************************************************************</span><br><span class="line">ok: [localhost]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">PLAY [Removing the gateway configuration] *************************************</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">GATHERING FACTS ***************************************************************</span><br><span class="line">ok: [ceph-igw-1]</span><br><span class="line">ok: [ceph-igw-2]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">TASK: [igw_purge | purging the gateway configuration] *************************</span><br><span class="line">changed: [ceph-igw-1]</span><br><span class="line">changed: [ceph-igw-2]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">TASK: [igw_purge | deleting configured rbd devices] ***************************</span><br><span class="line">changed: [ceph-igw-1]</span><br><span class="line">changed: [ceph-igw-2]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">PLAY RECAP ********************************************************************</span><br><span class="line">ceph-igw-1                 : ok=3    changed=2    unreachable=0    failed=0</span><br><span class="line">ceph-igw-2                 : ok=3    changed=2    unreachable=0    failed=0</span><br><span class="line">localhost                  : ok=2    changed=0    unreachable=0    failed=0</span><br></pre></td></tr></table></figure>
<h4 id="使用命令行接口配置iSCSI-Target"><a href="#使用命令行接口配置iSCSI-Target" class="headerlink" title="使用命令行接口配置iSCSI Target"></a>使用命令行接口配置iSCSI Target</h4><p>Ceph iSCSI网关既是iSCSI target节点也是Ceph client节点。Ceph iSCSI网关可以是一个单独的节点，也可以托管于Ceph对象存储磁盘（OSD）节点。完成以下步骤后，将安装并配置Ceph iSCSI网关进行基本操作。  </p>
<p><strong>要求</strong>：  </p>
<ul>
<li>运行中的Ceph Luminous(12.2.x)或更新的集群</li>
<li>Red Hat Enterprise Linux/CentOS 7.5 (or newer); Linux kernel v4.16 (or newer)</li>
<li>必须从你的Linux 发行版的软件仓库安装下列包：  <ul>
<li>targetcli-2.1.fb47 or newer package</li>
<li>python-rtslib-2.1.fb68 or newer package  </li>
<li>tcmu-runner-1.4.0 or newer package</li>
<li>ceph-iscsi-3.2 or newer package  </li>
</ul>
</li>
</ul>
<p>在继续<em>安装</em>章节前，需要在Ceph iSCSI网关节点执行以下步骤：  </p>
<ol>
<li>如果Ceph iSCSI网关没有和OSD节点在一起，从存储集群中运行的Ceph节点的<code>/etc/ceph</code>目录下复制Ceph配置文件到iSCSI网关节点。iSCSI网关节点上的Ceph配置文件必须存在于<code>/etc/ceph</code>目录下。</li>
<li>安装并配置<a href="http://docs.ceph.com/docs/master/start/quick-rbd/#install-ceph" target="_blank" rel="noopener"><font color="red">Ceph Command-line Interface</font></a>。</li>
<li>如果需要，打开防火墙上的TCP 3260和5000端口。</li>
<li>创建一个新的或使用已存在的RBD。  </li>
</ol>
<p><strong>安装</strong>：  </p>
<p>如果使用上游ceph-iscsi包，请遵循<a href="https://docs.ceph.com/docs/master/rbd/iscsi-target-cli-manual-install/" target="_blank" rel="noopener"><font color="red">手动安装说明</font></a>。  </p>
<p>基于rpm指令执行以下命令：  </p>
<ol>
<li><p>在所有iSCSI网关节点上，使用root 安装 ceph-iscsi包：  </p>
<blockquote>
<h1 id="yum-install-ceph-iscsi"><a href="#yum-install-ceph-iscsi" class="headerlink" title="yum install ceph-iscsi"></a>yum install ceph-iscsi</h1></blockquote>
</li>
<li><p>在所有iSCSI网关节点上，使用root 安装 tcmu-runner包：  </p>
<blockquote>
<h1 id="yum-install-tcmu-runner"><a href="#yum-install-tcmu-runner" class="headerlink" title="yum install tcmu-runner"></a>yum install tcmu-runner</h1></blockquote>
</li>
</ol>
<p><strong>构建</strong>：  </p>
<ol>
<li><p>gwcli需要一个名为rbd的池，因此它可以存储像iSCSI配置这样的元数据。检查这个池是否已经创建运行:  </p>
<blockquote>
<h1 id="ceph-osd-lspools"><a href="#ceph-osd-lspools" class="headerlink" title="ceph osd lspools"></a>ceph osd lspools</h1></blockquote>
<p>如果不存在，创建资源池的操作可在<a href="http://docs.ceph.com/docs/master/rados/operations/pools/" target="_blank" rel="noopener"><font color="red">RADOS pool operations page</font></a> 找到。  </p>
</li>
<li><p>在iSCSI网关节点上，使用root在<code>/etc/ceph</code>目录下创建名为<code>iscsi-gateway.cfg</code>文件：  </p>
<blockquote>
<h1 id="touch-etc-ceph-iscsi-gateway-cfg"><a href="#touch-etc-ceph-iscsi-gateway-cfg" class="headerlink" title="touch /etc/ceph/iscsi-gateway.cfg"></a>touch /etc/ceph/iscsi-gateway.cfg</h1></blockquote>
<ol>
<li>编辑<code>iscsi-gateway.cfg</code>文件，添加以下内容：  </li>
</ol>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">[config]</span><br><span class="line"># Name of the Ceph storage cluster. A suitable Ceph configuration file allowing</span><br><span class="line"># access to the Ceph storage cluster from the gateway node is required, if not</span><br><span class="line"># colocated on an OSD node.</span><br><span class="line">cluster_name = ceph</span><br><span class="line"></span><br><span class="line"># Place a copy of the ceph cluster&apos;s admin keyring in the gateway&apos;s /etc/ceph</span><br><span class="line"># drectory and reference the filename here</span><br><span class="line">gateway_keyring = ceph.client.admin.keyring</span><br></pre></td></tr></table></figure>
<h1 id="API-settings"><a href="#API-settings" class="headerlink" title="API settings."></a>API settings.</h1><h1 id="The-API-supports-a-number-of-options-that-allow-you-to-tailor-it-to-your"><a href="#The-API-supports-a-number-of-options-that-allow-you-to-tailor-it-to-your" class="headerlink" title="The API supports a number of options that allow you to tailor it to your"></a>The API supports a number of options that allow you to tailor it to your</h1><h1 id="local-environment-If-you-want-to-run-the-API-under-https-you-will-need-to"><a href="#local-environment-If-you-want-to-run-the-API-under-https-you-will-need-to" class="headerlink" title="local environment. If you want to run the API under https, you will need to"></a>local environment. If you want to run the API under https, you will need to</h1><h1 id="create-cert-key-files-that-are-compatible-for-each-iSCSI-gateway-node-that-is"><a href="#create-cert-key-files-that-are-compatible-for-each-iSCSI-gateway-node-that-is" class="headerlink" title="create cert/key files that are compatible for each iSCSI gateway node, that is"></a>create cert/key files that are compatible for each iSCSI gateway node, that is</h1><h1 id="not-locked-to-a-specific-node-SSL-cert-and-key-files-must-be-called"><a href="#not-locked-to-a-specific-node-SSL-cert-and-key-files-must-be-called" class="headerlink" title="not locked to a specific node. SSL cert and key files must be called"></a>not locked to a specific node. SSL cert and key files <em>must</em> be called</h1><h1 id="‘iscsi-gateway-crt’-and-‘iscsi-gateway-key’-and-placed-in-the-‘-etc-ceph-‘-directory"><a href="#‘iscsi-gateway-crt’-and-‘iscsi-gateway-key’-and-placed-in-the-‘-etc-ceph-‘-directory" class="headerlink" title="‘iscsi-gateway.crt’ and ‘iscsi-gateway.key’ and placed in the ‘/etc/ceph/‘ directory"></a>‘iscsi-gateway.crt’ and ‘iscsi-gateway.key’ and placed in the ‘/etc/ceph/‘ directory</h1><h1 id="on-each-gateway-node-With-the-SSL-files-in-place-you-can-use-‘api-secure-true’"><a href="#on-each-gateway-node-With-the-SSL-files-in-place-you-can-use-‘api-secure-true’" class="headerlink" title="on each gateway node. With the SSL files in place, you can use ‘api_secure = true’"></a>on <em>each</em> gateway node. With the SSL files in place, you can use ‘api_secure = true’</h1><h1 id="to-switch-to-https-mode"><a href="#to-switch-to-https-mode" class="headerlink" title="to switch to https mode."></a>to switch to https mode.</h1><h1 id="To-support-the-API-the-bear-minimum-settings-are"><a href="#To-support-the-API-the-bear-minimum-settings-are" class="headerlink" title="To support the API, the bear minimum settings are:"></a>To support the API, the bear minimum settings are:</h1><p>api_secure = false</p>
<h1 id="Additional-API-configuration-options-are-as-follows-defaults-shown"><a href="#Additional-API-configuration-options-are-as-follows-defaults-shown" class="headerlink" title="Additional API configuration options are as follows, defaults shown."></a>Additional API configuration options are as follows, defaults shown.</h1><h1 id="api-user-admin"><a href="#api-user-admin" class="headerlink" title="api_user = admin"></a>api_user = admin</h1><h1 id="api-password-admin"><a href="#api-password-admin" class="headerlink" title="api_password = admin"></a>api_password = admin</h1><h1 id="api-port-5001"><a href="#api-port-5001" class="headerlink" title="api_port = 5001"></a>api_port = 5001</h1><h1 id="trusted-ip-list-192-168-0-10-192-168-0-11"><a href="#trusted-ip-list-192-168-0-10-192-168-0-11" class="headerlink" title="trusted_ip_list = 192.168.0.10,192.168.0.11"></a>trusted_ip_list = 192.168.0.10,192.168.0.11</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br></pre></td><td class="code"><pre><span class="line">    2. 使用root，将文件复制到所有iSCSI网关节点。</span><br><span class="line">   3. 在所有iSCSI网关节点上，使用root用户，启用并开启API服务：  </span><br><span class="line"></span><br><span class="line">      &gt; \# systemctl daemon-reload</span><br><span class="line">      &gt; \# systemctl enable rbd-target-api</span><br><span class="line">      &gt; \# systemctl start rbd-target-api  </span><br><span class="line"></span><br><span class="line">**配置**：  </span><br><span class="line"></span><br><span class="line">gwcli将创建和配置iSCSI target和RBD images，并复制上一节的网关设置中的配置。较低层的工具，如targetcli和rbd，可以用来查询本地配置，但不应该用来修改它。下一节将演示如何创建iSCSI target并将RBD image导出为LUN 0。  </span><br><span class="line"></span><br><span class="line">   1. 在一个iSCSI网关节点，使用root，开启iSCSI网关命令行接口：  </span><br><span class="line"></span><br><span class="line">      &gt; \# gwcli</span><br><span class="line"></span><br><span class="line">   2. 转到iscsi-target 并创建一个名为`iqn.2003-01.com.redhat.iscsi-gw:iscsi-igw`的target：  </span><br><span class="line"></span><br><span class="line">      &gt; /&gt; cd /iscsi-target</span><br><span class="line">      &gt; /iscsi-target&gt; create iqn.2003-01.com.redhat.iscsi-gw:iscsi-igw</span><br><span class="line"></span><br><span class="line">   3. 创建iSCSI网关。下面使用的ip将用于iSCSI数据，如读和写命令。它们可以是trusted_ip_list中列出的用于管理操作的相同ip，但是建议使用不同的ip。  </span><br><span class="line"></span><br><span class="line">      &gt; /iscsi-target&gt; cd iqn.2003-01.com.redhat.iscsi-gw:iscsi-igw/gateways</span><br><span class="line">      &gt; /iscsi-target...-igw/gateways&gt;  create ceph-gw-1 10.172.19.21</span><br><span class="line">      &gt; /iscsi-target...-igw/gateways&gt;  create ceph-gw-2 10.172.19.22</span><br><span class="line"></span><br><span class="line">      如果不使用RHEL/CentOS或使用上游或ceph-iscsi-test内核，则必须使用skipcheck =true参数。这将避免红帽内核和rpm检查:  </span><br><span class="line"></span><br><span class="line">      &gt; /iscsi-target&gt; cd iqn.2003-01.com.redhat.iscsi-gw:iscsi-igw/gateways</span><br><span class="line">      &gt; /iscsi-target...-igw/gateways&gt;  create ceph-gw-1 10.172.19.21 skipchecks=true</span><br><span class="line">      &gt; /iscsi-target...-igw/gateways&gt;  create ceph-gw-2 10.172.19.22 skipchecks=true</span><br><span class="line"></span><br><span class="line">   4. 在rbd池中新增名为disk_1的RBD image：  </span><br><span class="line"></span><br><span class="line">      &gt; /iscsi-target...-igw/gateways&gt; cd /disks</span><br><span class="line">      &gt; /disks&gt; create pool=rbd image=disk_1 size=30G</span><br><span class="line"></span><br><span class="line">   5. 新建一个名为iqn.1994-05.com.redhat:rh7-client的initiator 客户端：  </span><br><span class="line"></span><br><span class="line">      &gt; /disks&gt; cd /iscsi-target/iqn.2003-01.com.redhat.iscsi-gw:iscsi-igw/hosts</span><br><span class="line">      &gt; /iscsi-target...eph-igw/hosts&gt;  create iqn.1994-05.com.redhat:rh7-client</span><br><span class="line"></span><br><span class="line">   6. 设置客户端的CHAP，用户名为myiscsiusername，密码为myiscsipassword：  </span><br><span class="line"></span><br><span class="line">      &gt; /iscsi-target...at:rh7-client&gt;  auth username=myiscsiusername password=myiscsipassword </span><br><span class="line"></span><br><span class="line">   7. 给客户端添加硬盘：  </span><br><span class="line"></span><br><span class="line">      &gt; /iscsi-target...at:rh7-client&gt; disk add rbd/disk_1  </span><br><span class="line"></span><br><span class="line">### 配置iSCSI Initiators  </span><br><span class="line"></span><br><span class="line">#### iSCSI Initiator for Linux  </span><br><span class="line"></span><br><span class="line">**准备**：  </span><br><span class="line"></span><br><span class="line">  * iscsi-initiator-utils包</span><br><span class="line">  * device-mapper-multipath包  </span><br><span class="line"></span><br><span class="line">**安装**：  </span><br><span class="line"></span><br><span class="line">安装iSCSI initiator和multipath工具：  </span><br><span class="line"></span><br><span class="line">&gt; \# yum install iscsi-initiator-utils device-mapper-multipath</span><br><span class="line"></span><br><span class="line">**配置**：  </span><br><span class="line"></span><br><span class="line">1.  创建默认的`/etc/multipath.conf`文件并启用multipathd服务：  </span><br><span class="line">  </span><br><span class="line">   &gt; \# mpathconf --enable --with_multipathd y</span><br><span class="line"></span><br><span class="line">2. 添加以下内容到`/etc/multipath.conf`文件：  </span><br><span class="line"></span><br><span class="line">```text</span><br><span class="line">devices &#123;</span><br><span class="line">        device &#123;</span><br><span class="line">                vendor                 &quot;LIO-ORG&quot;</span><br><span class="line">                hardware_handler       &quot;1 alua&quot;</span><br><span class="line">                path_grouping_policy   &quot;failover&quot;</span><br><span class="line">                path_selector          &quot;queue-length 0&quot;</span><br><span class="line">                failback               60</span><br><span class="line">                path_checker           tur</span><br><span class="line">                prio                   alua</span><br><span class="line">                prio_args              exclusive_pref_bit</span><br><span class="line">                fast_io_fail_tmo       25</span><br><span class="line">                no_path_retry          queue</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
</ol>
<ol start="3">
<li>重启multipathd服务：  </li>
</ol>
<blockquote>
<p># systemctl reload multipathd  </p>
</blockquote>
<p><strong>iSCIS发现及安装</strong>：  </p>
<ol>
<li>如果CHAP已安装在iSCSI网关，请通过更新相应的<code>/etc/iscsi/iscsid.conf</code>来提供CHAP的用户和密码。</li>
<li>发现target门户：  </li>
</ol>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># iscsiadm -m discovery -t st -p 192.168.56.101</span></span><br><span class="line">192.168.56.101:3260,1 iqn.2003-01.org.linux-iscsi.rheln1</span><br><span class="line">192.168.56.102:3260,2 iqn.2003-01.org.linux-iscsi.rheln1</span><br></pre></td></tr></table></figure>
<ol start="3">
<li>登录：  </li>
</ol>
<blockquote>
<p># iscsiadm -m node -T iqn.2003-01.org.linux-iscsi.rheln1 -l  </p>
</blockquote>
<p><strong>Multipath IO安装</strong>：  </p>
<p>multipath守护进程（multipathd）将自动根据multipath.conf的设置来设置设备。运行multipath命令显示故障转移配置中的设备设置，每个路径都有一个优先级组。  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># multipath -ll</span></span><br><span class="line">mpathbt (360014059ca317516a69465c883a29603) dm-1 LIO-ORG ,IBLOCK</span><br><span class="line">size=1.0G features=<span class="string">'0'</span> hwhandler=<span class="string">'1 alua'</span> wp=rw</span><br><span class="line">|-+- policy=<span class="string">'queue-length 0'</span> prio=50 status=active</span><br><span class="line">| `- 28:0:0:1 sde  8:64  active ready running</span><br><span class="line">`-+- policy=<span class="string">'queue-length 0'</span> prio=10 status=enabled</span><br><span class="line">  `- 29:0:0:1 sdc  8:32  active ready running</span><br></pre></td></tr></table></figure>
<p>现在你可以像使用多路径iSCSI磁盘一样使用RBD image。  </p>
<h3 id="监视-iSCSI网关"><a href="#监视-iSCSI网关" class="headerlink" title="监视 iSCSI网关"></a>监视 iSCSI网关</h3><p>Ceph为iSCSI网关环境提供了一个额外的工具，用于监视导出的RADOS块设备(RBD) images的性能。  </p>
<p>gwtop工具是一个类似于top的工具，它显示通过iSCSI导出到客户端的RBD images的聚合性能指标。度量来自性能度量域代理(PMDA)。来自Linux-IO target(LIO) PMDA的信息用于列出每个导出的RBD image及其连接的客户机及其关联的I/O指标。  </p>
<p><strong>要求</strong>：  </p>
<ul>
<li>运行中的Ceph iSCSI网关  </li>
</ul>
<p><strong>安装</strong>：  </p>
<ol>
<li>使用root在每个iSCSI网关节点安装<code>ceph-iscsi-tools</code>包：  </li>
</ol>
<blockquote>
<p># yum install ceph-iscsi-tools  </p>
</blockquote>
<ol start="2">
<li>使用root在每个iSCSI网关节点安装<code>co-pilot</code>包：  </li>
</ol>
<blockquote>
<p># yum install pcp</p>
</blockquote>
<ol start="3">
<li>使用root在每个iSCSI网关节点安装LIO PMDA包：  </li>
</ol>
<blockquote>
<p># yum install pcp-pmda-lio  </p>
</blockquote>
<ol start="4">
<li>使用root在每个iSCSI网关节点上启用并开启performance co-pilot服务：  </li>
</ol>
<blockquote>
<p># systemctl enable pmcd<br># systemctl start pmcd  </p>
</blockquote>
<ol start="5">
<li>使用root注册pcp-pmda-lio代理：  </li>
</ol>
<blockquote>
<p>cd /var/lib/pcp/pmdas/lio<br>./Install  </p>
</blockquote>
<p>默认情况下，默认情况下，gwtop假定iSCSI网关配置对象存储在rbd池中名为gateway.conf的RADOS对象中。此配置定义了用于收集性能统计信息的iSCSI网关。这可以通过使用-g或-c标志覆盖。参见gwtop –help 了解更多细节。  </p>
<p>LIO配置决定从性能副驾驶中提取哪种类型的性能统计信息。当gwtop启动时，它查看LIO配置，如果找到用户空间磁盘，则gwtop自动选择LIO收集器。  </p>
<p><strong>“gwtop”输出示例</strong>  </p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">gwtop  2/2 Gateways   CPU% MIN:  4 MAX:  5    Network Total In:    2M  Out:    3M   10:20:00</span><br><span class="line">Capacity:   8G    Disks:   8   IOPS:  503   Clients:  1   Ceph: HEALTH_OK          OSDs:   3</span><br><span class="line">Pool.Image       Src    Size     iops     rMB/s     wMB/s   Client</span><br><span class="line">iscsi.t1703             500M        0      0.00      0.00</span><br><span class="line">iscsi.testme1           500M        0      0.00      0.00</span><br><span class="line">iscsi.testme2           500M        0      0.00      0.00</span><br><span class="line">iscsi.testme3           500M        0      0.00      0.00</span><br><span class="line">iscsi.testme5           500M        0      0.00      0.00</span><br><span class="line">rbd.myhost_1      T       4G      504      1.95      0.00   rh460p(CON)</span><br><span class="line">rbd.test_2                1G        0      0.00      0.00</span><br><span class="line">rbd.testme              500M        0      0.00      0.00</span><br></pre></td></tr></table></figure>
<p>在客户端列中，(CON)表示iSCSI启动程序(客户端)当前已登录到iSCSI网关。如果显示-multi-，则多个客户机被映射到单个RBD image。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://sheldon-lu.github.io/sheldon_blog/passages/ceph/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Sheldon Lu">
      <meta itemprop="description" content="静下心来写点东西">
      <meta itemprop="image" content="/sheldon_blog/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sheldon_Lu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/sheldon_blog/passages/ceph/" class="post-title-link" itemprop="url">ceph相关概念</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-09-02 21:34:50" itemprop="dateCreated datePublished" datetime="2019-09-02T21:34:50+08:00">2019-09-02</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-09-03 22:46:15" itemprop="dateModified" datetime="2019-09-03T22:46:15+08:00">2019-09-03</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/sheldon_blog/categories/Ceph/" itemprop="url" rel="index"><span itemprop="name">Ceph</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">Comments: </span>
                <a href="/sheldon_blog/passages/ceph/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/sheldon_blog/passages/ceph/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/sheldon_blog/passages/ceph/" class="leancloud_visitors" data-flag-title="ceph相关概念">
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">Views: </span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">19k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">17 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>Ceph是一个统一的分布式存储系统，最早起源于Sage就读博士期间的工作（最早的成果于2004年发表），随后贡献给开源社区。其设计初衷是提供较好的性能、可靠性和可扩展性。在经过多年的发展之后，目前已得到众多云计算厂商的支持并被广泛应用。RedHat及OpenStack都可与Ceph整合以支持虚拟机镜像的后端存储。  </p>
<p>Ceph的优势可以概括为以下四个方面：</p>
<ol>
<li><p>高性能 </p>
<ul>
<li>摒弃了传统的集中式存储元数据寻址的方案，采用CRUSH算法，数据分布均衡，并行度高</li>
<li>考虑了容灾域的隔离，能够实现各类负载的副本放置规则，例如跨机房、机架感知等</li>
<li>能够支持上千个存储节点的规模。支持TB到PB级的数据  </li>
</ul>
</li>
<li><p>高可用  </p>
<ul>
<li>副本数可以灵活控制</li>
<li>支持故障域分隔，数据强一致性</li>
<li>多种故障场景自动进行修复自愈</li>
<li>没有单点故障，自动管理  </li>
</ul>
</li>
<li><p>高扩展性</p>
<ul>
<li>去中心化</li>
<li>扩展灵活</li>
<li>随着节点增加，性能线性增长</li>
</ul>
</li>
<li><p>特性丰富</p>
<ul>
<li>支持三种存储接口：<font color="red">对象存储</font>，<font color="red">块设备存储</font>，<font color="red">文件存储</font></li>
<li>支持自定义接口，支持多种语言驱动  </li>
</ul>
</li>
</ol>
<h1 id="Ceph基本结构"><a href="#Ceph基本结构" class="headerlink" title="Ceph基本结构"></a>Ceph基本结构</h1><p>Ceph的基本组成结构如下图：  </p>
<p><img src="../images/Ceph架构图.png" alt="Ceph架构图">  </p>
<p>Ceph的底层是<strong>RADOS</strong>，RADOS本身也是分布式存储系统，Ceph所有的存储功能都是基于RADOS实现的。RADOS采用C++开发，所提供的原生Librados API包括C和C++两种。Ceph的上层应用调用本机上的librados API，再由后者通过socket与RADOS集群中的其他节点通信并完成各种操作。  </p>
<p>RADOS GateWay、RBD其作用是在librados库的基础上提供抽象层次更高、更便于应用或客户端使用的上层接口。其中RADOS GW是一个提供与Amazon S3和Swift兼容的RESTful API的gateway，以供相应的对象存储应用开发使用。RBD则提供了一个标准的块设备接口，常用于在虚拟化的场景下为虚拟机创建volume。目前，RedHat已经将RBD驱动集成在KVM/QEMU中，以提供虚拟机访问性能。这两种方式目前在云计算中应用的比较多。  </p>
<p>CephFS则提供了POSIX接口，用户可直接通过客户端挂载使用。它是内核态的程序，所有无需调用用户空间的librados库。它通过内核中的net模块来与RADOS进行交互。  </p>
<h1 id="Ceph基本组件及概念"><a href="#Ceph基本组件及概念" class="headerlink" title="Ceph基本组件及概念"></a>Ceph基本组件及概念</h1><h2 id="Ceph基本组件"><a href="#Ceph基本组件" class="headerlink" title="Ceph基本组件"></a>Ceph基本组件</h2><p><img src="../images/Ceph基本组件.png" alt="Ceph基本组件">  </p>
<p>如上图所示，Ceph主要有三个基本进程：  </p>
<ul>
<li>OSD  </li>
</ul>
<p>用于集群中所有数据与对象的存储。处理集群数据的复制、恢复、回填、再均衡。并向其他osd守护进程发送心跳，然后向Mon提供一些监控信息。  </p>
<p>当Ceph存储集群设定的数据有两个副本时（一共存两份），则至少需要两个OSD守护进程，即两个OSD节点，集群才能到达active+clean状态。  </p>
<ul>
<li>MDS(可选)  </li>
</ul>
<p>为Ceph文件系统提供元数据计算、缓存与同步。在Ceph中，元数据也是存储在osd节点中的，mds类似于元数据的代理缓存服务器。MDS进程并不是必须的进程，只有需要使用CephFS时，才需要配置MDS节点。  </p>
<ul>
<li>Monitor  </li>
</ul>
<p>监控整个集群的状态，维护集群的cluster MAP二进制表，保证集群数据的一致性。ClusterMAP描述了对象块存储的物理位置，以及一个将设备聚合到物理位置的桶列表。  </p>
<h2 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h2><ul>
<li>Object  </li>
</ul>
<p>Ceph最底层的存储单元，每个Object包含元数据和原始数据。  </p>
<ul>
<li>PG  </li>
</ul>
<p>全称Placement Groups，是一个逻辑的概念，一个PG包含多个OSD。引入PG这一层其实是为了更好的分配数据和定位数据。  </p>
<ul>
<li>RADOS  </li>
</ul>
<p>全称Reliable Autonomic Distributed Object Store，是Ceph集群的精华，用户实现数据分配、Failover等集群操作。  </p>
<ul>
<li>Librados  </li>
</ul>
<p>Librados是RADOS提供的库。因为RADOS很难直接访问，因此上层的RBD、RGW和CephFS都是通过Librados访问的。  </p>
<ul>
<li>CRUSH  </li>
</ul>
<p>是Ceph使用的数据分布算法，让数据分配到预期的地方。  </p>
<ul>
<li>RBD  </li>
</ul>
<p>全称RADOS block device，是Ceph对外提供的块设备服务。  </p>
<ul>
<li>RGW  </li>
</ul>
<p>全称RADOS GateWay，是Ceph对外提供的对象存储服务，接口与S3、Swift兼容。  </p>
<ul>
<li>CephFS  </li>
</ul>
<p>全称Ceph File System，是Ceph对外提供的文件系统服务。  </p>
<h1 id="Ceph-核心组件"><a href="#Ceph-核心组件" class="headerlink" title="Ceph 核心组件"></a>Ceph 核心组件</h1><p>Ceph的核心组件包括Ceph OSD、Ceph Monitor和Ceph MSD。</p>
<h2 id="OSD"><a href="#OSD" class="headerlink" title="OSD"></a>OSD</h2><p>首先介绍下Ceph数据的存储过程，如下图：  </p>
<p><img src="../images/Ceph数据存储过程.png" alt="Ceph数据存储过程">  </p>
<p>无论使用哪种存储方式（对象、块、文件系统），存储的数据都会被切分成对象（Objects）。Objects size大小可以由管理员调整，通常为2M或者4M。每个对象都会有一个唯一的OID，有ino和ono生成：ino是文件的File ID，用于全局唯一标示每一个文件，而ono是分片的编号。例如，一个文件FileID是A，它被切成两个对象，一个编号为0，另一个编号为1，那么这两个对象的OID则为A0和A1。OID的好处是可以唯一标示每一个不同的对象，并且存储了对象与文件的从属关系。由于Ceph的所有数据都虚拟成了整齐划一的对象，所以在读写时效率都会比较高。  </p>
<p>但是对象并不会直接存储进OSD中，因为对象的size很小，在一个大规模的集群中可能有几百到几千万个对象。如此多的对象光是遍历寻址速度都会很缓慢；而且如果将对象直接通过某种固定映射的哈希算法映射到osd上，当这个osd损坏时，对象无法自动迁移到其他osd上面（因为映射函数不允许）。为了解决这些问题，Ceph引入了归置组的概念，即PG。  </p>
<p>PG是一个逻辑概念，Linux系统中可以直接看到对象，但是无法直接看到PG。它在数据寻址时类似于数据库中的索引：每个对象都会固定映射进一个PG中，所以当我们寻找一个对象时，只需要先找到对象所属的PG，然后遍历这个PG就可以了，无需遍历所有对象。而且在数据迁移时，也是一PG作为基本单位进行迁移，Ceph不会直接操作对象。  </p>
<p>对象是如何映射进PG的？首先使用静态hash函数对OID做hash取出特征码，用特征码与PG的数量取模，得到的序号就是PGID。由于这种设计，PG的数量多寡直接决定了数据分布的均匀性，所以合理设置的PG数量可以很好地提升Ceph集群的性能并使数据均匀分布。  </p>
<p>最后PG会根据管理员设置的副本数量进行复制，然后通过CRUSH算法存储到不同的OSD节点上（其实是把PG中的所有对象存储到节点上），第一个osd节点即为主节点，其余均为从节点。  </p>
<p>下面是一段Ceph中的伪代码，简要描述了Ceph的数据存储流程：  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">locator = object_name</span><br><span class="line">obj_hash = hash(locator)</span><br><span class="line">pg = obj_hash % num_pg</span><br><span class="line">osds_for_pg = crush(pg) <span class="comment"># return a list of osds</span></span><br><span class="line">primary = osds_for_pg[<span class="number">0</span>]</span><br><span class="line">replicas = osds_for_og[<span class="number">1</span>:]</span><br></pre></td></tr></table></figure>
<p><img src="../images/OSDs.png" alt="OSDs">  </p>
<p>上图中更好地诠释了Ceph数据流的存储过程，数据无论是从三个接口哪一种写入的，最终都要切分成对象存储到底层的RADOS中。逻辑上通过算法先映射到PG上，最终存储进OSD节点里。图中除了之前介绍过的概念之外多了个pools的概念。  </p>
<p><img src="../images/pools.png" alt="pools">  </p>
<p>Pool是管理员自定义的命名空间，像其他的命名空间一样，用来隔离对象与PG。我们在调用API存储，即使用对象存储时，需要指定对象要存储进哪一个POOL中。除了隔离数据，我们也可以分别对不同的POOL设置不同的优化策略，比如副本数、数据清洗次数、数据块及对象大小等。  </p>
<p>OSD是强一致性的分布式存储，它的读写流程如下图：  </p>
<p><img src="../images/Ceph读写流程.png" alt="Ceph读写流程">  </p>
<p>Ceph的读写操作采用主从模型，客户端要读写数据时，只能向对象所对应的的主OSD节点发起请求。主节点在接受写请求时，会同步的向从OSD中写入数据。当所有的OSD节点都写入完成后，主节点才会向客户端报告写入完成的信息，因此保证了主从节点数据的高度一致性。而读取的时候，客户端也只会向主OSD节点发送读请求，并不会有类似数据库中读写分离的情况出现，这也是处于强一致性的考虑。由于所有写操作都要交给主OSD节点来处理，所以在数据量很大的时候，性能可能会比较慢，为了克服这个问题以及让Ceph能支持事务，每个OSD节点都包含一个journal文件，稍后介绍。  </p>
<p>数据流向的介绍就先到这，现在回到正题：OSD进程。在Ceph中，每一个OSD进程都可以称作是一个OSD节点，也就是说，每台存储服务器可靠包含了众多的OSD节点，每个OSD节点监听不同的端口，类似于在同一台服务器上跑多个MySQL或Redis。每个OSD节点可以设置一个目录作为实际存储区域，也可以是一个分区、一整块硬盘。如下图，当前这台机器上跑了两个OSD进程，每个OSD监听4个端口，分别用于接收客户请求、传输数据、发送心跳、同步数据等操作。  </p>
<p><img src="../images/OSD进程.png" alt="OSD进程">  </p>
<p>如上图所示，osd节点默认监听tcp的6800到6803端口，如果同一台服务器上有多个OSD节点，则依次往后排序。  </p>
<p>在生成环境中OSD最少可能都有上百个，所以每个OSD都有一个全局的编号，类似OSD0、OSD1、OSD2等等，序号根据OSD诞生的顺序排列，并且是全局唯一的。存储了相同PG的OSD节点除了想Mon节点发送心跳外，还会互相发送心跳信息以检测PG数据副本是否正常。  </p>
<p>之前在介绍数据流向时说过，每个OSD节点都包含一个journal文件，如下图：  </p>
<p><img src="../images/journal.png" alt="journal">  </p>
<p>默认大小为5G，也就是说创建一个OSD节点，还没使用就要被journal占用5G的空间。这个值是可以调整的，具体大小要依据OSD的总大小而定。  </p>
<p>Journal的作用类似于MySQL innodb引擎中的事务日志系统。当有突发的大量写入操作时，Ceph可以先把一些零散的，随机的IO请求保存到缓存中进行合并，然后在同一向内核发起IO请求。这样做效率会比较高，但是一旦OSD节点崩溃，缓存中的数据就会丢失，所以数据在还未写进硬盘时，都会记录到journal中，当OSD崩溃后重新启东市，会自动尝试从journal恢复因崩溃而丢失的缓存数据。因此journal的IO是非常密集的，而且由于一个数据IO两次，很大程度上也损耗了硬件的IO性能，所以通常在生产环境中，使用ssd来单独存储journal文件以提高Ceph的读写性能。  </p>
<h2 id="monitor节点"><a href="#monitor节点" class="headerlink" title="monitor节点"></a>monitor节点</h2><p>Mon节点监控着整个Ceph集群的状态信息，监听于tcp的6789端口。每个Ceph集群中至少要有一个Mon节点，官方推荐每个集群至少部署三台。Mon节点中保存了最新的版本集群数据分布图（Cluster Map）的主副本。客户端在使用时，需要挂载Mon节点的6789端口，下载最新的Cluster Map，通过CRUSH算法获得集群中各OSD的IP地址，然后再与OSD节点直接建立连接来传输数据。所以对于Ceph来说，并不需要有集中式的主节点用于计算与寻址，客户端分摊了这部分工作。而且客户端也可以直接和OSD通信，省去了中间代理服务器的额外开销。  </p>
<p>Mon节点之间使用Paxos算法来保持各节点Cluster Map的一致性；各Mon节点的功能总体是一样的，相互间的关系可以被简单理解为主备关系。如果主Mon节点损坏，其他Mon存活节点超过半数时，集群还可以正常运行。当故障Mon节点恢复时，会主动从其他Mon节点拉取最新的Cluster Map。  </p>
<p>Mon节点并不会主动轮询各个OSD的当前状态，相反，OSD只有在一些特殊情况下才会上报自己的信息，平常只会简单的发送心跳。特殊情况包括：1、新的OSD被加入集群；2、某个OSD发现自身或其他OSD发生异常。Mon节点在收到这些上报信息时，则会更新Cluster Map信息并加以扩缩。  </p>
<p>Cluster Map信息是以异步且lazy的形式扩散的。Monitor并不会在每一次Cluster Map版本更新后都将新版广播至全体OSD，而是有OSD向自己上报信息时，将更新恢复给对方。类似的，各个OSD也是在和其他OSD通信时，如果发现对方的OSD中持有的Cluster Map版本较低，则把自己更新的版本发送给对方。  </p>
<p><img src="../images/推荐架构.png" alt="推荐架构">  </p>
<p>这里的Ceph除了管理网段外，设了两个网段，一个用于客户端读写传输数据，另一个用于各OSD节点之间同步数据和发送心跳信息等。这样做的好处是可以分担网卡的IO压力。否则在数据清洗时，客户端的读写速度会变得极为缓慢。  </p>
<h2 id="MDS"><a href="#MDS" class="headerlink" title="MDS"></a>MDS</h2><p>Mds是Ceph集群中的元数据服务器，而通常情况它都不是必须的，因为只有在使用CephFS的时候才需要它，而目前云计算中用到的更广泛的是另外两种存储方式。  </p>
<p>Mds虽然是元数据服务器，但是它不负责存储元数据，元数据也是被切成对象存在各个OSD节点中，如下图：  </p>
<p><img src="../images/Ceph&#32;Architecture.png" alt="Ceph Architecture">  </p>
<p>在创建CephFS时，要至少创建两个Pool，一个用于存储数据，另一个用于存放元数据。Mds只是负责接收用户的元数据查询请求，然后从OSD中把数据取出来映射进自己的内存中供客户访问。所以Mds其实类似一个代理缓存服务器，替OSD分担了用户的访问压力，如下图：  </p>
<p><img src="../images/Mds.png" alt="Mds">  </p>
<h2 id="Ceph-各概念之间的关系"><a href="#Ceph-各概念之间的关系" class="headerlink" title="Ceph 各概念之间的关系"></a>Ceph 各概念之间的关系</h2><h3 id="存储数据与Object的关系"><a href="#存储数据与Object的关系" class="headerlink" title="存储数据与Object的关系"></a>存储数据与Object的关系</h3><p>当用户要将数据存储到Ceph集群时，存储数据都会被分割成多个Object，每个Object都有一个Object ID，每个Object的大小是可以设置的，默认为4MB。Object可以看做是Ceph存储的最小存储单元。 </p>
<h3 id="Object-与-PG"><a href="#Object-与-PG" class="headerlink" title="Object 与 PG"></a>Object 与 PG</h3><p>由于Object的数量很多，所以Ceph引入了PG的概念用于管理Object，每个Object最后都会通过CRUSH计算映射到某个PG中，一个PG可以包含多个Object。</p>
<p>Ceph条带化之后，将获得N个带有唯一OID（即Object的id）。Object id是进行线性映射生成的，即有file的元数据、Ceph条带化产生的Object的序号连缀而成。此时object需要映射到PG中，该映射包括两部分：  </p>
<ol>
<li>有Ceph集群指定的静态Hash函数计算Object的OID，获取到其Hash值</li>
<li>将该Hash值与mask进行操作，从而获得PG ID  </li>
</ol>
<p>根据RADOS的设计，假定集群中设定的PG总数为M（M一般为2的整数幂），则mask的值为M-1.由此，Hash值计算之后，进行按位与操作是想从所有PG中近似均匀地随机选择。基于该原理以及概率论的相关原理，当用于数据量庞大的Object以及PG时，获得到的PG ID是近似均匀的，  </p>
<h3 id="PG-与-OSD"><a href="#PG-与-OSD" class="headerlink" title="PG 与 OSD"></a>PG 与 OSD</h3><p>由PG映射到数据存储的实际单元OSD中，该映射是由CRUSH算法来确定的，将PG ID作为该算法的输入，获得到包含N个OSD的集合，集合的第一个OSD被作为主OSD，其余的OSD则依次作为从OSD。N为该PG所在POOL下的副本数量，在生产环境中N一般为3；OSD集合中的OSD将共同存储和维护该PG下的Object。需要注意的是，CRUSH算法的结果不是绝对不变的，而是受其他因素的影响。其影响因素主要有以下两个：  </p>
<ol>
<li>当前系统状态，也就是Cluster Map（集群映射）。当系统中的OSD状态、数量发生变化时，Cluster Map可能发生变化，而这种变化将会影响到PG和OSD之间的映射</li>
<li>存储策略配置。这里的策略主要与安全相关。利用策略配置，系统管理员可以指定承载同一个PG的3个OSD分别位于数据中心的不同服务器乃至机架上，从而进一步改善存储的可靠性。  </li>
</ol>
<p>因此，只有在Cluster Map和存储策略都不发生变化的时候，PG和OSD之间的映射关系才是固定不变的。在实际使用中，策略已经配置通常不会改变。而系统状态的改变或者是因为设备损坏，或者是因为存储集群规模扩大。好在Ceph本身提供了对于这种变化的自动化支持，因而即便PG和OSD之间的映射关系发生了变化，并不会对应用造成困扰。事实上，Ceph正是需要有目的的利用这种动态映射关系。正是利用了CRUSH的动态特性，Ceph才可以将一个PG根据需要动态迁移到不同的OSD组合上，从而自动化地实现高可靠性、数据分布re-blancing等特性。  </p>
<p>之所以在此次映射中使用CRUSH算法，而不是其他Hash算法，原因之一是CRUSH具有上述可配置特性，可以根据管理员的配置参数决定OSD的物理位置映射策略；另一方面是因为CRUSH具有特殊的“稳定性”，也就是当系统中加入新的OSD导致系统规模增大时，大部分PG与OSD之间的映射关系不会发生改变，只是少部分PG的映射关系会发生变化并引发数据迁移。这种可配置性和稳定性都不是普通Hash算法所能提供的。因此，CRUSH算法的设计也是Ceph的核心内容之一。  </p>
<h3 id="PG-与-PGP"><a href="#PG-与-PGP" class="headerlink" title="PG 与 PGP"></a>PG 与 PGP</h3><p>PG是用来存放Object的，PGP相当于是PG存放OSD的一种排列组合。举个例子，比如有3个OSD，OSD.1、OSD.2和OSD.3，副本数是2，如果PGP的数目为1，那么PG存放的OSD组合就只有一种可能：[OSD.1，OSD.2]，那么所有的PG主从副本分别存放到OSD.1和OSD.2；如果PGP设为2，那么OSD组合就有两种，[OSD.1，OSD.2]和[OSD.1，OSD.3]，是不是很像数学中的排列组合，PGP就是代表这个意思。一般来说应该将PG和PGP的数量设置为相等。接下来我们来通过一组实验来进一步说明：  </p>
<p>先创建一个名为testpool包含6个PG和6个PGP的存储池：  </p>
<blockquote>
<p>ceph osd pool create testpool 6 6</p>
</blockquote>
<p>通过写数据后我们查看下PG的分布情况，使用以下命令：      </p>
<blockquote>
<p>ceph pg dump pgs | grep ^1 | awk ‘{print $1,$2,$15}’  </p>
</blockquote>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">dumped pgs in format plain</span><br><span class="line">1.1 75 [3,6,0]</span><br><span class="line">1.0 83 [7,0,6]</span><br><span class="line">1.3 144 [4,1,2]</span><br><span class="line">1.2 146 [7,4,1]</span><br><span class="line">1.5 86 [4,6,3]</span><br><span class="line">1.4 80 [3,0,4]</span><br></pre></td></tr></table></figure>
<p>第1列为PG的ID，第2列为该PG所存储的对象数目，第3列为该PG所在的OSD。  </p>
<p>我们扩大PG再看看：  </p>
<blockquote>
<p>ceph osd pool set testpool pg_num 12  </p>
</blockquote>
<p>再用上面的命令查询分布情况：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">1.1 37 [3,6,0]</span><br><span class="line">1.9 38 [3,6,0]</span><br><span class="line">1.0 41 [7,0,6]</span><br><span class="line">1.8 42 [7,0,6]</span><br><span class="line">1.3 48 [4,1,2]</span><br><span class="line">1.b 48 [4,1,2]</span><br><span class="line">1.7 48 [4,1,2]</span><br><span class="line">1.2 48 [7,4,1]</span><br><span class="line">1.6 49 [7,4,1]</span><br><span class="line">1.a 49 [7,4,1]</span><br><span class="line">1.5 86 [4,6,3]</span><br><span class="line">1.4 80 [3,0,4]</span><br></pre></td></tr></table></figure>
<p>可以看到PG的数量增加到12个了，PG1.1的对象数量本来是75个，现在是37个，可以看到它把对象数分给新增的PG1.9了，刚好是38，加起来为75，而且可以看到PG1.1和PG1.9的OSD盘是一样的。而且可以看到OSD盘的组合还是那6种。  </p>
<p>我们增加PGP的数量来看下，使用命令：  </p>
<blockquote>
<p>ceph osd pool set testpool pgp_num 12  </p>
</blockquote>
<p>在看下：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">1.a 49 [1,2,6]</span><br><span class="line">1.b 48 [1,6,2]</span><br><span class="line">1.1 37 [3,6,0]</span><br><span class="line">1.0 41 [7,0,6]</span><br><span class="line">1.3 48 [4,1,2]</span><br><span class="line">1.2 48 [7,4,1]</span><br><span class="line">1.5 86 [4,6,3]</span><br><span class="line">1.4 80 [3,0,4]</span><br><span class="line">1.7 48 [1,6,0]</span><br><span class="line">1.6 49 [3,6,7]</span><br><span class="line">1.9 38 [1,4,2]</span><br><span class="line">1.8 42 [1,2,3]</span><br></pre></td></tr></table></figure>
<p>再看PG1.1和PG1.9，可以看到PG1.9不在[3,6,0]上，而是在[1,4,2]上，该组合时新增加的。可以知道增加PGP_NUM其实是增加了OSD盘的组合。  </p>
<p>通过上述实验可以得出：  </p>
<ol>
<li>PG是指定存储池存储对象的目录有多少个，PGP是存储池PG的OSD分布组合个数</li>
<li>PG的增加会引起PG内的数据进行分裂，分裂到相同的OSD上新生成的PG中</li>
<li>PGP的增加会引起部分PG的分布变化，但是不会引起PG内对象的变动</li>
</ol>
<h3 id="PG-与-Pool"><a href="#PG-与-Pool" class="headerlink" title="PG 与 Pool"></a>PG 与 Pool</h3><p>Pool也是一个逻辑存储概念，我们创建存储池pool的时候，都需要指定PG和PGP的数量，逻辑上来说PG是属于某个存储池的，就像Object是属于某个PG的一样。  </p>
<p>下图表明了存储数据、Object、PG、Pool、OSD、存储磁盘的关系：  </p>
<p><img src="../images/关系图.png" alt="关系图"></p>
<h2 id="Ceph-条带化"><a href="#Ceph-条带化" class="headerlink" title="Ceph 条带化"></a>Ceph 条带化</h2><p>众所周知，存储设备具有吞吐量限制，它影响读写性能和可扩展性能。所以存储系统通常都支持条带化以增加存储系统的吞吐量并提升性能，数据条带化最常见的方式是做RAID。与Ceph的条带化最相似的是RAID 0或者是“带区卷”。Ceph条带化提供了类似RAID 0的吞吐量，N路RAID镜像的可靠性已经更快速的恢复能力。  </p>
<p>在磁盘阵列中，数据是以条带（stripe）的方式贯穿在磁盘阵列所有硬盘中的。这种数据的分配方式可以弥补OS读取数据量跟不上的不足。  </p>
<p>1.将条带单元（stripe unit）从阵列的第一个硬盘到最后一个硬盘收集起来，就可以称为条带（stripe）。有的时候，条带单元也被称为交错深度。在光纤技术中，一个条带单元被叫做段。  </p>
<p>2.数据在阵列中的硬盘上是以条带的形式分布的，条带化是指数据在阵列中所有硬盘中的存储过程。文件中的数据被分割成小块的数据段在阵列中的硬盘顺序上的存储，这个最小数据块就叫做条带单元。  </p>
<p>决定Ceph条带化数据的3个因素：  </p>
<ul>
<li>对象大小：处于分布式集群中的对象拥有一个最大可配置的尺寸（例如，2MB，4MB等），对象大小应该足够大以适应大量的条带单元。</li>
<li>条带宽度：条带有一个可以配置的单元大小，Ceph Client端将数据写入对象，分成相同大小的条带单元，除了最后一个条带之外；每个条带宽度，应该是对象大小的一部分，这样使得一个对象可以包含多个条带单元。</li>
<li>条带总量：Ceph客户端写入一系列的条带单元到一系列的对象，这就决定了条带的总量，这些对象被称为对象集。当Ceph客户端写入的对象集合中的最后一个对象之后，它将会返回到对象集合中的第一个对象处。  </li>
</ul>
<h1 id="CRUSH-算法"><a href="#CRUSH-算法" class="headerlink" title="CRUSH 算法"></a>CRUSH 算法</h1><h2 id="数据分布算法挑战"><a href="#数据分布算法挑战" class="headerlink" title="数据分布算法挑战"></a>数据分布算法挑战</h2><ul>
<li><strong>数据分布和负载均衡</strong>：<ul>
<li>a.数据分布均衡，使数据能均匀地分布到各个节点上</li>
<li>b.负载均衡，使数据访问读写操作的负载在各个节点和磁盘的负载均衡</li>
</ul>
</li>
<li><strong>灵活应对集群伸缩</strong>：<ul>
<li>a.系统可以方便的增加或者删除节点设备，并且对节点失效进行处理</li>
<li>b.增加或者删除节点设备后，能自动实现数据的均衡，并且尽可能少的迁移数据</li>
</ul>
</li>
<li><strong>支持大规模集群</strong>：<ul>
<li>要求数据分布算法维护的元数据相对较小，并且计算量不能太大。随着集群规模的增加，数据分布算法的开销相对较小</li>
</ul>
</li>
</ul>
<h2 id="CRUSH-算法说明"><a href="#CRUSH-算法说明" class="headerlink" title="CRUSH 算法说明"></a>CRUSH 算法说明</h2><ul>
<li>CRUSH（Controlled Replication Under Scalable Hashing）是一种基于伪随机控制数据分布、复制的算法。Ceph是为大规模分布式存储系统（PB级的数据和成百上千台存储设备）而设计的，在大规模的存储系统里，必须考虑数据的平衡分布和负载（提高资源利用率）、最大化系统的性能以及系统的扩展和硬件容错等。CRUSH就是为解决上述问题而设计的。在Ceph集群里，CRUSH只需要一个简洁而层次清晰的设备描述，包括存储集群和副本放置策略，就可以有效地把数据对象映射到存储设备上，且这个过程是完全分布式的，在集群系统中的任何一方都可以独立计算任何对象的位置；另外，大型系统存储结构是动态变化的（存储节点的扩展或者缩容、硬件故障等），CRUSH能够处理存储设备的变更（添加或删除），并最小化由于存储设备的变更而导致的数据迁移。   </li>
<li>PG到OSD的映射过程的算法叫做CRUSH算法。</li>
<li>CRUSH算法是一个伪随机的过程，它可以从所有的OSD中，随机性选择一个OSD集合，但是同一个PG每次随机选择的结果是不变的，也就是映射的OSD集合是固定的。</li>
</ul>
<h2 id="CRUSH-基本原理"><a href="#CRUSH-基本原理" class="headerlink" title="CRUSH 基本原理"></a>CRUSH 基本原理</h2><p><strong>CRUSH 算法因子</strong>：  </p>
<ul>
<li><strong>层次化的Cluster Map</strong>： 反映了存储系统层级的物理拓扑结构。定义了OSD集群具有层级关系的静态拓扑结构。OSD层级使得CRUSH算法在选择OSD时实现了机架感知能力，也就是通过规则定义，使得副本可以分布在不同的机架、不同的机房中，提高数据的安全性</li>
<li><strong>Placement Rules</strong>： 决定了一个PG的对象副本如何选择的规则，通过这些可以自己设定规则，用户可以自定义设置副本在集群中的分布</li>
</ul>
<h2 id="CRUSH-关系分析"><a href="#CRUSH-关系分析" class="headerlink" title="CRUSH 关系分析"></a>CRUSH 关系分析</h2><p>从本质上讲，CRUSH算法是通过存储设备的权重来计算数据对象的分布。在计算过程中，通过Cluster Map（集群映射）、Data Distribution Policy（数据分布策略）和给出的一个随机数共同决定数据对象的最终位置。  </p>
<h3 id="Cluster-Map"><a href="#Cluster-Map" class="headerlink" title="Cluster Map"></a>Cluster Map</h3><p>Cluster Map记录所有可用的存储资源以及可用的存储资源及相互之间的空间层次结构（集群中有多少个机架、机架上有多少个服务器、每个服务器上有多少磁盘等信息）。所谓Map，顾名思义，就是类似于我们生活中的地图。在Ceph存储里，数据的索引都是通过各种不同的Map来实现的。另一方面，Map使得Ceph集群存储设备在物理层做了一层防护。例如，在多副本结构上，通过设置合理的Map（故障域设置为Host级），可以保证在某一服务器死机的情况下，有其他副本保留在正常的存储节点上，能够继续提供服务，实现存储的高可用。设置更高的故障域级别（如Rack、Row等）能保证整机柜或同一排机柜在掉电情况下数据的高可用性和完整性。  </p>
<h4 id="1-Cluster-Map的分层结构"><a href="#1-Cluster-Map的分层结构" class="headerlink" title="1.Cluster Map的分层结构"></a>1.Cluster Map的分层结构</h4><p>Cluster Map由Device和Bucket构成。它们都有自己的ID和权重值，并且形成一个以Device为叶子节点、Bucket为躯干的树状结果。  </p>
<p><img src="../images/CRUSH架构图.png" alt="CRUSH架构图">  </p>
<p>Bucket拥有不同的类型，如Host、Row、Rack、Room等，通常我们默认把机架类型定义为Rack，主机类型定义为Host，数据中心（IDC机房）定义为Data Center。Bucket的类型都是虚拟结构，可以根据自己的喜好设计合适的类型。Device节点的权重值代表了存储设备的容量与性能。其中，磁盘容量是权重大小的关键因素。  </p>
<p>OSD的权重值越高，对应磁盘会被分配写入更多的数据。总体来说，数据会被均匀写入分布于集群所有磁盘，从而提高整体性能和可靠性。无论磁盘的规格容量，总能够均匀使用。  </p>
<p>关于OSD权重值的大小值的配比，官方默认值设置为1TB容量的硬盘，对应权重为1。可以在/etc/init.d/ceph源码里查看相关的内容。  </p>
<h5 id="Bucket随机算法类型"><a href="#Bucket随机算法类型" class="headerlink" title="Bucket随机算法类型"></a>Bucket随机算法类型</h5><p><img src="../images/ceph_bucket.png" alt="Ceph Bucket">  </p>
<ul>
<li><strong>一般的buckets</strong>：适合所有子节点权重相同，而且很少添加删除item</li>
<li><strong>list buckets</strong>：适用于集群扩展类型。增加item，产生最优的数据移动，查找item，时间复杂对为O(n)</li>
<li><strong>tree buckets</strong>：查找复杂的为O(log n)，添加删除叶子节点是，其它节点node_id不变</li>
<li><strong>straw buckets</strong>：允许所有项通过类似抽签的方式来与其它项公平“竞争”。定位副本时，bucket中的每一项都对应一个随机长度的straw，且拥有最长长度的straw会获得胜利（被选中）；添加或者重新计算，子树之间的数据移动提供最优的解决方案</li>
</ul>
<h4 id="2-恢复与动态平衡"><a href="#2-恢复与动态平衡" class="headerlink" title="2.恢复与动态平衡"></a>2.恢复与动态平衡</h4><p>在默认情况下，当集群里有组件故障时（主要是OSD，也可能是磁盘或者网络等），Ceph会把OSD标记为down，如果在300s内未能回复，集群就会开始进行恢复状态。这个“300s”可以通过“mon osd down ourt interval”配置选项修改等待时间。PG（Placement Groups）是Ceph数据管理（包括复制、修复等操作）单元。当客户端把读写请求（对象单元）推送到Ceph时，通过CRUSH提供的Hash算法把对象映射到PG。PG在CRUSH策略的影响下，最终会被映射到OSD上。  </p>
<h3 id="Data-Distribution-Policy"><a href="#Data-Distribution-Policy" class="headerlink" title="Data Distribution Policy"></a>Data Distribution Policy</h3><p>Data Distribution Policy由Placement Rules组成。Rule决定了每个数据对象有多少个副本，这些副本存储的限制条件（比如3个副本放在不同的机架中）。一个典型的rule如下所示：  </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">rule replicated_ruleset &#123;                   ##rule名字</span><br><span class="line">      ruleset 0                             #rule的ID</span><br><span class="line">      type replicated                       ##类型为副本模式，另外一种模式为纠删码（EC）</span><br><span class="line">      min_size 1                            ##如果存储池的副本数大于这个值，此rule不会应用</span><br><span class="line">      max_size 10                           ##如果存储池的副本数大于这个值，此rule不会应用</span><br><span class="line">      step take default                     ##以default root 为入口</span><br><span class="line">      step chooseleaf firstn 0 type host    ##隔离城为host级，即不同副本在不同的主机上</span><br><span class="line">      step emit                             ##提交</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>根据实际的设备环境，可以定制符合自己需求的Rule。  </p>
<h2 id="CRUSH-中的伪随机"><a href="#CRUSH-中的伪随机" class="headerlink" title="CRUSH 中的伪随机"></a>CRUSH 中的伪随机</h2><p>$$CRUSH(x) \quad \rightarrow \quad (osd1,osd2 \cdots\cdots osdN)$$  </p>
<p>CRUSH使用了多参数的Hash函数，在Hash之后，映射都是按既定规则选择的，这使得从x到OSD的集合是确定的和独立的。CRUSH只使用Cluster Map、Placement Ruels、X。CRUSH是伪随机算法，相似输入的结果之间没有相关性。  </p>
<p>下面通过计算PG的ID来看CRUSH的一个计算过程：  </p>
<ol>
<li>Client输入pool ID和对象ID（如pool=’liverpool’,object-id=’john’）</li>
<li>CRUSH获得对象ID并对其Hash运算</li>
<li>CRUSH计算OSD个数，Hash取模获得PG的ID（如0x58）</li>
<li>CRUSH获得已命名pool的ID（如liverpool=4）</li>
<li>CRUSH预先考虑到pool ID相同的PG ID（如4.0x58）</li>
</ol>
<p>在Ceph集群里，当有数据对象要写入集群时，需要进行两次映射，第一次从Object–&gt;PG，第二层是PG–&gt;OSD set。每一次的映射都是与其他对象不相关的，这充分体现了CEUSH的独立性（充分分散）和确定性（可确定的存储位置）。  </p>
<h1 id="Ceph-IO流程及数据分布"><a href="#Ceph-IO流程及数据分布" class="headerlink" title="Ceph IO流程及数据分布"></a>Ceph IO流程及数据分布</h1><p><img src="../images/rados_io_1.png" alt="rados_io_1">  </p>
<h2 id="正常IO流程图"><a href="#正常IO流程图" class="headerlink" title="正常IO流程图"></a>正常IO流程图</h2><p><img src="../images/ceph_io_2.png" alt="ceph_io_2">  </p>
<p>步骤：  </p>
<ol>
<li>Client 创建Cluster handler</li>
<li>Client 读取配置文件</li>
<li>Client 连接上Monitor，获取集群map信息</li>
<li>Client 读写IO 根据Crushmap算法请求对应的主OSD数据节点</li>
<li>主OSD数据节点同时写入另外两个副本节点数据</li>
<li>等待主节点以及另外两个副本节点写完数据状态</li>
<li>主节点及副本节点写入状态都成功后，返回给Client，IO写入完成  </li>
</ol>
<h2 id="新主IO流程图"><a href="#新主IO流程图" class="headerlink" title="新主IO流程图"></a>新主IO流程图</h2><p>说明：  </p>
<p>如果新加入的OSD1取代了原有的OSD4成为Primary OSD，由于OSD1上未创建PG，不存在数据，那么PG上的IO无法进行，怎样工作呢？  </p>
<p><img src="../images/ceph_io_3.png" alt="ceph_io_3">  </p>
<p>步骤：  </p>
<ol>
<li>Client 连接Monitor获取集群Map信息</li>
<li>同时新主OSD1由于没有PG，数据会主动上报Monitor告知让OSD2临时接替为主</li>
<li>临时主OSD2会把数据全量同步给新主OSD1</li>
<li>Client IO读写直接连接临时主OSD2进行读写</li>
<li>OSD2收到读写IO，同时写入另外两个副本节点</li>
<li>等待OSD2以及另外两副本写入成功  </li>
<li>OSD2三分数据都写入成功返回给Client，此时Client IO读写完毕</li>
<li>如果OSD1数据同步完毕，临时主OSD2会交出主角色</li>
<li>OSD1成为主节点，OSD2变成副本  </li>
</ol>
<h2 id="Ceph-IO算法流程"><a href="#Ceph-IO算法流程" class="headerlink" title="Ceph IO算法流程"></a>Ceph IO算法流程</h2><p><img src="../images/ceph_io_4.png" alt="ceph_io_4">   </p>
<ol>
<li><p>File用户需要读写文件，File-&gt;Object映射：  </p>
<ul>
<li>a. ino（File的元数据，File的唯一ID）</li>
<li>b. ono（File切分产生的某个Object的序号，默认以4M切分一个块大小）</li>
<li>c. oid（object id： ino + ono）</li>
</ul>
</li>
<li><p>Object是RADOS需要的对象。Ceph指定一个静态Hash函数计算OID的值，将OID映射成一个近似均匀分布的伪随机值，然后和mask按位相与，得到PGID。Object-&gt;PG映射：  </p>
<ul>
<li>a. hash(oid) &amp; mask -&gt; pgid</li>
<li>b. mask = PG总数m（m为2的整数幂）-1</li>
</ul>
</li>
<li><p>PG（Placement Group），用途是对Object的存储进行组织和位置映射，类似Redis Cluster里面的slot的概念。一个PG里面会有很多Object。采用CRUSH算法，将PGID带入其中，然后得到一组OSD。PG-&gt;OSD映射：  </p>
<ul>
<li>CRUSH(pgid) -&gt; (osd1,osd2,osd3)</li>
</ul>
</li>
</ol>
<h2 id="Ceph-RBD-IO流程"><a href="#Ceph-RBD-IO流程" class="headerlink" title="Ceph RBD IO流程"></a>Ceph RBD IO流程</h2><p><img src="../images/ceph_rbd_io.png" alt="ceph_rbd_io">  </p>
<p>步骤：  </p>
<ol>
<li>客户端创建一个Pool，需要为这个Pool指定PG的数量</li>
<li>创建pool/image RBD设备进行挂载</li>
<li>用户写入的数据进行切块，每个块的大小默认为4M，并且每个块都有一个名字，即Object+序号</li>
<li>将每个Object通过PG进行副本位置的分配</li>
<li>PG根据CRUSH算法会寻找3个OSD，把这个Object分别保存在这三个OSD上</li>
<li>OSD上实际是把底层的Disk进行了格式化操作，一般部署工具会将它格式化为xfs文件系统</li>
<li>Object的存储就变成了存储一个名为rbd0.object1.file的文件  </li>
</ol>
<h2 id="Ceph-RBD-IO框架图"><a href="#Ceph-RBD-IO框架图" class="headerlink" title="Ceph RBD IO框架图"></a>Ceph RBD IO框架图</h2><p><img src="../images/ceph_rbd_io1.png" alt="ceph_rbd_io1">  </p>
<p>客户端写数据OSD过程：  </p>
<ol>
<li>采用的是librbd的形式，使用librbd创建一个块设备，向这个块设备写入数据  </li>
<li>在客户端本地通过调用librados接口，然后经过pool、rbd、object、pg进行层层映射，在PG这一层中，可以知道数据保存在哪3个OSD上，以及这3个OSD的主从关系</li>
<li>客户端与primary OSD建立socket通信，将要写入的数据传给primary OSD，由primary OSD再将数据发送给其他replica OSD数据节点  </li>
</ol>
<h2 id="Ceph-数据扩容PG分布"><a href="#Ceph-数据扩容PG分布" class="headerlink" title="Ceph 数据扩容PG分布"></a>Ceph 数据扩容PG分布</h2><p>场景数据迁移流程：  </p>
<ul>
<li>现状3个OSD，4个PG</li>
<li>扩容到4个OSD，4个PG</li>
</ul>
<p>现状：  </p>
<p><img src="../images/ceph_recory_1.png" alt="ceph_recory_1">  </p>
<p>扩容后：  </p>
<p><img src="../images/ceph_recory_2.png" alt="ceph_recory_2">  </p>
<p>说明：  </p>
<p>每个OSD上分布很多PG，并且每个PG会自动散落在不同的OSD上。如果扩容，那么相应的PG会自动迁移到新的OSD上，保证PG数量的均衡  </p>
<h1 id="Ceph心跳机制"><a href="#Ceph心跳机制" class="headerlink" title="Ceph心跳机制"></a>Ceph心跳机制</h1><h2 id="心跳介绍"><a href="#心跳介绍" class="headerlink" title="心跳介绍"></a>心跳介绍</h2><p>心跳是用于节点间检测对方是否发送故障，以便及时发现故障点进入相应的故障处理流程。  </p>
<p><strong>问题：</strong>  </p>
<ul>
<li>故障检测时间和心跳报文带来的负载之间做权衡</li>
<li>心跳频率太高则过多的心跳报文会影响系统性能</li>
<li>心跳频率过低则会延长发现故障节点的时间，从而影响系统的可用性  </li>
</ul>
<p><strong>故障检测策略应该能够做到：</strong></p>
<ul>
<li><strong>及时</strong>：节点发生异常如宕机或网络中断时，集群可以在可接受的时间范围内感知</li>
<li><strong>适当的压力</strong>：包括对节点的压力和网络的压力</li>
<li><strong>容忍网络抖动</strong>：网络偶尔延迟</li>
<li><strong>扩散机制</strong>：节点存活状态改变导致的元信息变化需要通过某种机制扩散到整个集群</li>
</ul>
<h2 id="Ceph-心跳检查"><a href="#Ceph-心跳检查" class="headerlink" title="Ceph 心跳检查"></a>Ceph 心跳检查</h2><p><img src="../images/ceph_heartbeat_1.png" alt="ceph_heartbeat">  </p>
<p><strong>OSD节点会监听public、cluster、front和back四个端口</strong></p>
<ul>
<li><strong>public端口</strong>：监听来自Monitor和Client的连接</li>
<li><strong>cluster端口</strong>：监听来自OSD Peer的连接</li>
<li><strong>front端口</strong>：供客户端连接集群使用的网卡，这里临时给集群内部之间进行心跳</li>
<li><strong>back端口</strong>：供集群内部使用的网卡，集群内部之间进行心跳</li>
<li><strong>hbclient</strong>：发送ping心跳的messenger</li>
</ul>
<h2 id="Ceph-OSD之间相互心跳检查"><a href="#Ceph-OSD之间相互心跳检查" class="headerlink" title="Ceph OSD之间相互心跳检查"></a>Ceph OSD之间相互心跳检查</h2><p><img src="../images/ceph_heartbeat_osd.png" alt="ceph_heartbeat_osd"></p>
<p>步骤：  </p>
<ul>
<li>同一个PG内的OSD相互心跳，它们互相发送PING/PONG信息</li>
<li>每隔6s检测一次（实际会在这个基础上加一个随机时间来避免峰值）</li>
<li>20s没有检测到心跳回复，加入failure队列  </li>
</ul>
<h2 id="Ceph-OSD与Mon心跳检测"><a href="#Ceph-OSD与Mon心跳检测" class="headerlink" title="Ceph OSD与Mon心跳检测"></a>Ceph OSD与Mon心跳检测</h2><p><img src="../images/ceph_heartbeat_mon.png" alt="ceph_heartbeat_mon">  </p>
<p><strong>OSD报告给Monitor</strong>：  </p>
<ul>
<li>OSD有事件发生时（比如故障、PG变更）</li>
<li>自身启动5s内</li>
<li>OSD周期性的上报给Monitor<ul>
<li>OSD检查failure_queue中的伙伴OSD失败信息</li>
<li>向Monitor发送失效报告，并将失败信息加入failure_pending队列，然后将其从failure_queue移除</li>
<li>收到来自failure_queue或者failure_pending中的OSD的心跳时，将其从这两个队列中移除，并告知Monitor取消之前的失效报告</li>
<li>当发生与Monitor网络重连时，会将failure_pending中的错误报告加回到failure_queue中，并再次发送给Monitor</li>
</ul>
</li>
<li>Monitor统计下线OSD<ul>
<li>Monitor收集来自OSD的伙伴失效报告</li>
<li>当错误报告指向的OSD失效超过一定阈值，且足够多的OSD报告其失效时，将该OSD下线  </li>
</ul>
</li>
</ul>
<h2 id="Ceph心跳检测总结"><a href="#Ceph心跳检测总结" class="headerlink" title="Ceph心跳检测总结"></a>Ceph心跳检测总结</h2><p>Ceph通过伙伴OSD汇报失效节点和Monitor统计来自OSD的心跳两种方式判定OSD节点失效。  </p>
<ul>
<li><strong>及时</strong>：伙伴OSD可以在秒级发现节点失效并汇报Monitor，并在几分钟内由Monitor将失效OSD下线</li>
<li><strong>适当压力</strong>：由于有伙伴OSD汇报机制，Monitor与OSD之间的心跳统计更像是一种保险措施，因此OSD向Monitor发送心跳的间隔可以长达600s，Monitor的检测阈值也可以长达900s。Ceph实际上是将故障检测过程中中心节点的压力分散到所有的OSD上，以此提高中心节点Monitor的可靠性，进而提高这个集群的可靠性</li>
<li><strong>容忍网络抖动</strong>：Monitor收到OSD对其伙伴OSD的汇报后，并没有马上将目标OSD下线，而是周期性地等待几个条件：  <ul>
<li>目标OSD的失效时间大于通过固定量osd_heartbeat_grace和历史网络条件动态确定的阈值</li>
<li>来自不同主机的汇报到的mod_osd_min_down_reports</li>
<li>慢速前两个条件失效汇报没有被源OSD取消</li>
</ul>
</li>
<li><strong>扩散</strong>：作为中心节点的Monitor并没有在更OSDMap后尝试广播通知所有的OSD和Client，而是惰性的等待OSD和Client来获取，以此来减少Monitor压力并简化交互逻辑  </li>
</ul>
<h1 id="Ceph通信框架"><a href="#Ceph通信框架" class="headerlink" title="Ceph通信框架"></a>Ceph通信框架</h1><h2 id="Ceph通信框架种类介绍"><a href="#Ceph通信框架种类介绍" class="headerlink" title="Ceph通信框架种类介绍"></a>Ceph通信框架种类介绍</h2><p><strong>网络通信框架三种不同的实现方式</strong>：  </p>
<ul>
<li><strong>Simple线程模式</strong><ul>
<li><strong>特点</strong>：每一个网络链接，都会创建两个线程，一个用于接收，一个用于发送</li>
<li><strong>缺点</strong>：大量的链接会产生大量的线程，会消耗CPU资源，影响性能</li>
</ul>
</li>
<li><strong>Async事件的I/O多路复用模式</strong><ul>
<li><strong>说明</strong>：这种是目前网络通信中广泛采用的方式。K版默认已经使用Async</li>
</ul>
</li>
<li><strong>XIO方式使用了开源的网络通信库accelio来实现</strong><ul>
<li><strong>说明</strong>：这种方式需要依赖第三方库accelio的稳定性</li>
</ul>
</li>
</ul>
<h2 id="Ceph通信框架设计模式"><a href="#Ceph通信框架设计模式" class="headerlink" title="Ceph通信框架设计模式"></a>Ceph通信框架设计模式</h2><p><strong>设计模式（Subscribe/Publish）</strong></p>
<p>订阅发布模式又名观察者模式，它意图是”定义对象间的一种一对多的依赖关系，当一个对象的状态发生改变是，所有依赖于它的对象都得到通知并被自动更新“。  </p>
<h2 id="Ceph通信框架流程图"><a href="#Ceph通信框架流程图" class="headerlink" title="Ceph通信框架流程图"></a>Ceph通信框架流程图</h2><p><img src="../images/ceph_message.png" alt="ceph_message">  </p>
<p>步骤：  </p>
<ul>
<li>Accepter监听peer的请求，调用SimpleMessenger::add_accept_pip3()创建新的pipe到SimpleMessenger::pipes来处理该请求</li>
<li>Pipe用于消息的读取和发生。该类主要有两个组件：Pipe::Reader，Pipe::Writer用来处理消息读取和发送</li>
<li>Messenger作为消息的发布者，各个Dispatcher子类作为消息的订阅者，Messenger收到消息之后，通过Pipe对取消息，然后转给Dispatcher处理</li>
<li>Dispatcher是订阅者的基类，具体的订阅者后端继承该类，初始化的时候通过Messenger::add_dispatcher_tail/head注册到Messenger::dispatchers。收到消息后，通知该类处理</li>
<li>DispatchQueue该类用来缓存收到的消息，然后唤醒DispatchQueue::dispatch_thread线程找到后端的Dispatch处理消息</li>
</ul>
<p><img src="../images/ceph_message_2.png" alt="ceph_message_2">  </p>
<h2 id="Ceph通信框架类图"><a href="#Ceph通信框架类图" class="headerlink" title="Ceph通信框架类图"></a>Ceph通信框架类图</h2><p><img src="../images/ceph_message_3.png" alt="ceph_message_3">  </p>
<h2 id="Ceph通信数据格式"><a href="#Ceph通信数据格式" class="headerlink" title="Ceph通信数据格式"></a>Ceph通信数据格式</h2><p>通信协议格式需要双方约定数据格式。  </p>
<p><strong>消息的内容主要分为三部分</strong>：  </p>
<ul>
<li>header        //消息头 类型消息的信封</li>
<li>user data     //需要发送的实际数据<ul>
<li>payload     //操作保存元数据</li>
<li>middle      //预留字段</li>
<li>data        //读写数据</li>
</ul>
</li>
<li>footer        //消息的结束标记  </li>
</ul>
<figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Message</span> :</span> <span class="keyword">public</span> RefCountedObject &#123;</span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line">  ceph_msg_header  header;      <span class="comment">// 消息头</span></span><br><span class="line">  ceph_msg_footer  footer;      <span class="comment">// 消息尾</span></span><br><span class="line">  bufferlist       payload;  <span class="comment">// "front" unaligned blob</span></span><br><span class="line">  bufferlist       middle;   <span class="comment">// "middle" unaligned blob</span></span><br><span class="line">  bufferlist       data;     <span class="comment">// data payload (page-alignment will be preserved where possible)</span></span><br><span class="line"></span><br><span class="line">  <span class="comment">/* recv_stamp is set when the Messenger starts reading the</span></span><br><span class="line"><span class="comment">   * Message off the wire */</span></span><br><span class="line">  <span class="keyword">utime_t</span> recv_stamp;       <span class="comment">//开始接收数据的时间戳</span></span><br><span class="line">  <span class="comment">/* dispatch_stamp is set when the Messenger starts calling dispatch() on</span></span><br><span class="line"><span class="comment">   * its endpoints */</span></span><br><span class="line">  <span class="keyword">utime_t</span> dispatch_stamp;   <span class="comment">//dispatch 的时间戳</span></span><br><span class="line">  <span class="comment">/* throttle_stamp is the point at which we got throttle */</span></span><br><span class="line">  <span class="keyword">utime_t</span> throttle_stamp;   <span class="comment">//获取throttle 的slot的时间戳</span></span><br><span class="line">  <span class="comment">/* time at which message was fully read */</span></span><br><span class="line">  <span class="keyword">utime_t</span> recv_complete_stamp;  <span class="comment">//接收完成的时间戳</span></span><br><span class="line"></span><br><span class="line">  ConnectionRef connection;     <span class="comment">//网络连接</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">uint32_t</span> magic = <span class="number">0</span>;           <span class="comment">//消息的魔术字</span></span><br><span class="line"></span><br><span class="line">  bi::list_member_hook&lt;&gt; dispatch_q;    <span class="comment">//boost::intrusive 成员字段</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">ceph_msg_header</span> &#123;</span></span><br><span class="line">    __le64 seq;       <span class="comment">// 当前session内 消息的唯一 序号</span></span><br><span class="line">    __le64 tid;       <span class="comment">// 消息的全局唯一的 id</span></span><br><span class="line">    __le16 type;      <span class="comment">// 消息类型</span></span><br><span class="line">    __le16 priority;  <span class="comment">// 优先级</span></span><br><span class="line">    __le16 version;   <span class="comment">// 版本号</span></span><br><span class="line"></span><br><span class="line">    __le32 front_len; <span class="comment">// payload 的长度</span></span><br><span class="line">    __le32 middle_len;<span class="comment">// middle 的长度</span></span><br><span class="line">    __le32 data_len;  <span class="comment">// data 的 长度</span></span><br><span class="line">    __le16 data_off;  <span class="comment">// 对象的数据偏移量</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="class"><span class="keyword">struct</span> <span class="title">ceph_entity_name</span> <span class="title">src</span>;</span> <span class="comment">//消息源</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">/* oldest code we think can decode this.  unknown if zero. */</span></span><br><span class="line">    __le16 compat_version;</span><br><span class="line">    __le16 reserved;</span><br><span class="line">    __le32 crc;       <span class="comment">/* header crc32c */</span></span><br><span class="line">&#125; __attribute__ ((packed));</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">ceph_msg_footer</span> &#123;</span></span><br><span class="line">    __le32 front_crc, middle_crc, data_crc; <span class="comment">//crc校验码</span></span><br><span class="line">    __le64  sig; <span class="comment">//消息的64位signature</span></span><br><span class="line">    __u8 flags; <span class="comment">//结束标志</span></span><br><span class="line">&#125; __attribute__ ((packed));</span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://sheldon-lu.github.io/sheldon_blog/passages/ceph集群安装/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Sheldon Lu">
      <meta itemprop="description" content="静下心来写点东西">
      <meta itemprop="image" content="/sheldon_blog/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sheldon_Lu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/sheldon_blog/passages/ceph集群安装/" class="post-title-link" itemprop="url">ceph集群安装</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-09-02 21:34:50" itemprop="dateCreated datePublished" datetime="2019-09-02T21:34:50+08:00">2019-09-02</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-09-03 22:46:33" itemprop="dateModified" datetime="2019-09-03T22:46:33+08:00">2019-09-03</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/sheldon_blog/categories/Ceph/" itemprop="url" rel="index"><span itemprop="name">Ceph</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">Comments: </span>
                <a href="/sheldon_blog/passages/ceph集群安装/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/sheldon_blog/passages/ceph集群安装/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/sheldon_blog/passages/ceph集群安装/" class="leancloud_visitors" data-flag-title="ceph集群安装">
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">Views: </span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">36k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">33 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h3 id="CentOS7下安装Ceph供Kubernetes使用"><a href="#CentOS7下安装Ceph供Kubernetes使用" class="headerlink" title="CentOS7下安装Ceph供Kubernetes使用**"></a>CentOS7下安装Ceph供Kubernetes使用**</h3><h4 id="1、环境说明"><a href="#1、环境说明" class="headerlink" title="1、环境说明"></a>1、环境说明</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">系统：CentOS7，一个非系统分区分配给ceph</span><br><span class="line">docker：18.06-ce</span><br><span class="line">kubernetes：1.11.3</span><br><span class="line">ceph：luminous</span><br></pre></td></tr></table></figure>
<h4 id="2、Ceph部署准备"><a href="#2、Ceph部署准备" class="headerlink" title="2、Ceph部署准备"></a>2、Ceph部署准备</h4><h5 id="2-1-节点规划"><a href="#2-1-节点规划" class="headerlink" title="2.1 节点规划"></a>2.1 节点规划</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">192.168.105.92 lab1  # master1</span><br><span class="line">192.168.105.93 lab2  # master2</span><br><span class="line">192.168.105.94 lab3  # master3</span><br><span class="line">192.168.105.95 lab4  # node4</span><br><span class="line">192.168.105.96 lab5  # node5</span><br><span class="line">192.168.105.97 lab6  # node6</span><br><span class="line">192.168.105.98 lab7  # node7</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">监控节点：lab1、lab2、lab3</span><br><span class="line">OSD节点：lab4、lab5、lab6、lab7</span><br><span class="line">MDS节点：lab4</span><br></pre></td></tr></table></figure>
<h5 id="2-2-添加yum源"><a href="#2-2-添加yum源" class="headerlink" title="2.2 添加yum源"></a>2.2 添加yum源</h5><p>我们使用阿里云yum源：(CeontOS和epel也是阿里云yum源)</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">vim /etc/yum.repos.d/ceph.repo</span><br><span class="line">[Ceph]</span><br><span class="line">name=Ceph packages for $basearch</span><br><span class="line">baseurl=https://mirrors.aliyun.com/ceph/rpm-luminous/el7/$basearch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br><span class="line"></span><br><span class="line">[Ceph-noarch]</span><br><span class="line">name=Ceph noarch packages</span><br><span class="line">baseurl=https://mirrors.aliyun.com/ceph/rpm-luminous/el7/noarch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br><span class="line"></span><br><span class="line">[ceph-source]</span><br><span class="line">name=Ceph source packages</span><br><span class="line">baseurl=https://mirrors.aliyun.com/ceph/rpm-luminous/el7/SRPMS</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=1</span><br><span class="line">type=rpm-md</span><br><span class="line">gpgkey=https://mirrors.aliyun.com/ceph/keys/release.asc</span><br></pre></td></tr></table></figure>
<p>注意：ceph集群中节点都需要添加该yum源</p>
<h5 id="2-3-安装Ceph部署工具"><a href="#2-3-安装Ceph部署工具" class="headerlink" title="2.3 安装Ceph部署工具"></a>2.3 安装Ceph部署工具</h5><p><strong>以下操作在lab1上root用户操作</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">安装 ceph-deploy</span><br><span class="line">yum -y install ceph-deploy</span><br></pre></td></tr></table></figure>
<h5 id="2-4-安装时间同步工具chrony"><a href="#2-4-安装时间同步工具chrony" class="headerlink" title="2.4 安装时间同步工具chrony"></a>2.4 安装时间同步工具chrony</h5><p>以下操作在所有ceph节点root用户操作</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">yum -y install chrony</span><br><span class="line">systemctl start chronyd</span><br><span class="line">systemctl enable chronyd</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 修改/etc/chrony.conf前面的server段为如下</span><br><span class="line">server ntp1.aliyun.com iburst</span><br><span class="line">server ntp2.aliyun.com iburst</span><br><span class="line">server ntp3.aliyun.com iburst</span><br><span class="line">server ntp4.aliyun.com iburst</span><br></pre></td></tr></table></figure>
<h5 id="2-5-安装SSH服务"><a href="#2-5-安装SSH服务" class="headerlink" title="2.5 安装SSH服务"></a>2.5 安装SSH服务</h5><p>默认已正常运行，略</p>
<h5 id="2-6-创建部署-CEPH-的用户"><a href="#2-6-创建部署-CEPH-的用户" class="headerlink" title="2.6 创建部署 CEPH 的用户"></a>2.6 创建部署 CEPH 的用户</h5><p><strong>以下操作在所有ceph节点root用户操作</strong></p>
<p>ceph-deploy 工具必须以普通用户登录 Ceph 节点，且此用户拥有无密码使用 sudo 的权限，因为它需要在安装软件及配置文件的过程中，不必输入密码。</p>
<p>较新版的 ceph-deploy 支持用 –username 选项提供可无密码使用 sudo 的用户名（包括 root ，虽然不建议这样做）。使用 ceph-deploy –username {username} 命令时，指定的用户必须能够通过无密码 SSH 连接到 Ceph 节点，因为 ceph-deploy 中途不会提示输入密码。</p>
<p>建议在集群内的所有 Ceph 节点上给 ceph-deploy 创建一个特定的用户，但不要用 “ceph” 这个名字。全集群统一的用户名可简化操作（非必需），然而你应该避免使用知名用户名，因为黑客们会用它做暴力破解（如 root 、 admin 、 {productname} ）。后续步骤描述了如何创建无 sudo 密码的用户，你要用自己取的名字替换 {username} 。</p>
<p>注意：<br>从 Infernalis 版起，用户名 “ceph” 保留给了 Ceph 守护进程。如果 Ceph 节点上已经有了 “ceph” 用户，升级前必须先删掉这个用户。</p>
<p><strong>我们使用用户名ceph-admin</strong></p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">username="ceph-admin"</span><br><span class="line">useradd $&#123;username&#125; &amp;&amp; echo 'PASSWORD' | passwd $&#123;ceph-admin&#125; --stdinecho "$&#123;username&#125; ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/$&#123;username&#125;</span><br><span class="line">chmod 0440 /etc/sudoers.d/$&#123;username&#125;</span><br><span class="line">chmod a+x /etc/sudoers.d/</span><br></pre></td></tr></table></figure>
<h5 id="2-7-允许无密码-SSH-登录"><a href="#2-7-允许无密码-SSH-登录" class="headerlink" title="2.7 允许无密码 SSH 登录"></a>2.7 允许无密码 SSH 登录</h5><p>以下操作在lab1节点ceph-admin用户操作</p>
<p>正因为 ceph-deploy 不支持输入密码，你必须在管理节点上生成 SSH 密钥并把其公钥分发到各 Ceph 节点。 ceph-deploy 会尝试给初始 monitors 生成 SSH 密钥对。</p>
<p>生成 SSH 密钥对，但不要用 sudo 或 root 用户。提示 “Enter passphrase” 时，直接回车，口令即为空：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">su - ceph-admin  # 切换到此用户，因为ceph-deploy也用此用户</span><br><span class="line">ssh-keygen</span><br><span class="line"></span><br><span class="line">Generating public/private rsa key pair.</span><br><span class="line">Enter file in which to save the key (/home/ceph-admin/.ssh/id_rsa): </span><br><span class="line">/home/ceph-admin/.ssh/id_rsa already exists.</span><br><span class="line">Overwrite (y/n)? y</span><br><span class="line">Enter passphrase (empty for no passphrase): </span><br><span class="line">Enter same passphrase again: </span><br><span class="line">Your identification has been saved in /home/ceph-admin/.ssh/id_rsa.</span><br><span class="line">Your public key has been saved in /home/ceph-admin/.ssh/id_rsa.pub.</span><br><span class="line">The key fingerprint is:</span><br></pre></td></tr></table></figure>
<p>把公钥拷贝到各 Ceph 节点，把下列命令中的 {username} 替换成前面创建部署 Ceph 的用户里的用户名。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">username="ceph-admin"</span><br><span class="line">ssh-copy-id $&#123;username&#125;@lab1</span><br><span class="line">ssh-copy-id $&#123;username&#125;@lab2</span><br><span class="line">ssh-copy-id $&#123;username&#125;@lab3</span><br></pre></td></tr></table></figure>
<p>（推荐做法）修改 ceph-deploy 管理节点上的 ~/.ssh/config 文件，这样 ceph-deploy 就能用你所建的用户名登录 Ceph 节点了，而无需每次执行 ceph-deploy 都要指定 –username {username} 。这样做同时也简化了 ssh 和 scp 的用法。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Host lab1</span><br><span class="line">   Hostname lab1</span><br><span class="line">   User ceph-admin</span><br><span class="line">Host lab2</span><br><span class="line">   Hostname lab2</span><br><span class="line">   User ceph-admin</span><br><span class="line">Host lab3</span><br><span class="line">   Hostname lab3</span><br><span class="line">   User ceph-admin</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Bad owner or permissions on /home/ceph-admin/.ssh/config</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 需要用chmod 600 ~/.ssh/config解决。</span><br></pre></td></tr></table></figure>
<h5 id="2-8-开放所需端口"><a href="#2-8-开放所需端口" class="headerlink" title="2.8 开放所需端口"></a>2.8 开放所需端口</h5><blockquote>
<p>以下操作在所有监视器节点<code>root</code>用户操作</p>
</blockquote>
<p>Ceph Monitors 之间默认使用 <code>6789</code> 端口通信， OSD 之间默认用 6800:7300 这个范围内的端口通信。</p>
<p><code>firewall-cmd --zone=public --add-port=6789/tcp --permanent &amp;&amp; firewall-cmd --reload</code></p>
<h5 id="2-9-终端（-TTY-）"><a href="#2-9-终端（-TTY-）" class="headerlink" title="2.9 终端（ TTY ）"></a>2.9 终端（ TTY ）</h5><blockquote>
<p>以下操作在所有ceph节点root用户操作</p>
</blockquote>
<p>在 CentOS 和 RHEL 上执行 ceph-deploy 命令时可能会报错。如果你的 Ceph 节点默认设置了 requiretty ，执行 sudo visudo 禁用它，并找到 Defaults requiretty 选项，把它改为 Defaults:ceph !requiretty 或者直接注释掉，这样 ceph-deploy 就可以用之前创建的用户（创建部署 Ceph 的用户 ）连接了。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i -r 's@Defaults(.*)!visiblepw@Defaults:ceph-admin\1!visiblepw@g' /etc/sudoers</span><br></pre></td></tr></table></figure>
<h5 id="2-10-SELINUX"><a href="#2-10-SELINUX" class="headerlink" title="2.10 SELINUX"></a>2.10 SELINUX</h5><blockquote>
<p>以下操作在所有ceph节点root用户操作</p>
</blockquote>
<p>如果原来是开启的，需要重启生效。</p>
<p>永久关闭 修改/etc/sysconfig/selinux文件设置</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed -i 's/SELINUX=.*/SELINUX=disabled/' /etc/sysconfig/selinux</span><br></pre></td></tr></table></figure>
<h5 id="2-11-整理以上所有ceph节点操作"><a href="#2-11-整理以上所有ceph节点操作" class="headerlink" title="2.11 整理以上所有ceph节点操作"></a>2.11 整理以上所有ceph节点操作</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">yum -y install chrony</span><br><span class="line">systemctl start chronyd</span><br><span class="line">systemctl enable chronyd</span><br><span class="line"></span><br><span class="line">username="ceph-admin"</span><br><span class="line">useradd $&#123;username&#125; &amp;&amp; echo 'PASSWORD' | passwd $&#123;username&#125; --stdin</span><br><span class="line">echo "$&#123;username&#125; ALL = (root) NOPASSWD:ALL" | sudo tee /etc/sudoers.d/$&#123;username&#125;</span><br><span class="line">chmod 0440 /etc/sudoers.d/$&#123;username&#125;</span><br><span class="line">chmod a+x /etc/sudoers.d/</span><br><span class="line">sed -i -r 's@Defaults(.*)!visiblepw@Defaults:ceph-admin\1!visiblepw@g' /etc/sudoers</span><br><span class="line">sed -i 's/SELINUX=.*/SELINUX=disabled/' /etc/sysconfig/selinux</span><br><span class="line">yum -y install ceph</span><br></pre></td></tr></table></figure>
<h4 id="3、存储集群部署"><a href="#3、存储集群部署" class="headerlink" title="3、存储集群部署"></a>3、存储集群部署</h4><blockquote>
<p>以下操作在lab节点<code>ceph-admin</code>用户操作</p>
</blockquote>
<p>我们创建一个 Ceph 存储集群，它有一个 Monitor 和两个 OSD 守护进程。一旦集群达到 active + clean 状态，再扩展它：增加第三个 OSD 、增加元数据服务器和两个 Ceph Monitors。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">su - ceph-admin</span><br><span class="line">mkdir ceph-cluster</span><br></pre></td></tr></table></figure>
<p>如果在某些地方碰到麻烦，想从头再来，可以用下列命令清除配置：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy purgedata &#123;ceph-node&#125; [&#123;ceph-node&#125;]</span><br><span class="line">ceph-deploy forgetkeys</span><br><span class="line">rm -rf /etc/ceph/*</span><br><span class="line">rm -rf /var/lib/ceph/*/*</span><br><span class="line">rm -rf /var/log/ceph/*</span><br><span class="line">rm -rf /var/run/ceph/*</span><br></pre></td></tr></table></figure>
<h5 id="3-1-创建集群并准备配置"><a href="#3-1-创建集群并准备配置" class="headerlink" title="3.1 创建集群并准备配置"></a>3.1 创建集群并准备配置</h5><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd ceph-cluster</span><br><span class="line">ceph-deploy new lab1</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">[ceph-admin@lab1 ceph-cluster]$ ls -l</span><br><span class="line">total 24</span><br><span class="line">-rw-rw-r-- 1 ceph-admin ceph-admin   196 Aug 17 14:31 ceph.conf</span><br><span class="line">-rw-rw-r-- 1 ceph-admin ceph-admin 12759 Aug 17 14:31 ceph-deploy-ceph.log</span><br><span class="line">-rw------- 1 ceph-admin ceph-admin    73 Aug 17 14:31 ceph.mon.keyring</span><br><span class="line">[ceph-admin@lab1 ceph-cluster]$ more ceph.mon.keyring </span><br><span class="line">[mon.]</span><br><span class="line">key = AQC2a3ZbAAAAABAAor15nkYQCXuC681B/Q53og==</span><br><span class="line">caps mon = allow *</span><br><span class="line">[ceph-admin@lab1 ceph-cluster]$ more ceph.conf </span><br><span class="line">[global]</span><br><span class="line">fsid = fb212173-233c-4c5e-a98e-35be9359f8e2</span><br><span class="line">mon_initial_members = lab1</span><br><span class="line">mon_host = 192.168.105.92</span><br><span class="line">auth_cluster_required = cephx</span><br><span class="line">auth_service_required = cephx</span><br><span class="line">auth_client_required = cephx</span><br></pre></td></tr></table></figure>
<p>把 Ceph 配置文件里的默认副本数从 3 改成 2 ，这样只有两个 OSD 也可以达到 active + clean 状态。把下面这行加入 [global] 段：</p>
<p><code>osd pool default size = 2</code></p>
<p>再把 <code>public network</code> 写入 Ceph 配置文件的<code>[global]</code>段下</p>
<p><code>public network = 192.168.105.0/24</code></p>
<p>安装 Ceph</p>
<p><code>ceph-deploy install lab1 lab4 lab5 --no-adjust-repos</code></p>
<p>配置monitor(s)并收集所有密钥：</p>
<p><code>ceph-deploy mon create-initial</code></p>
<p>完成上述操作后，当前目录里应该会出现这些密钥环：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br></pre></td><td class="code"><pre><span class="line">[ceph-admin@lab1 ceph-cluster]$ ceph-deploy mon create-initial</span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /home/ceph-admin/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO ] Invoked (2.0.1): /bin/ceph-deploy mon create-initial</span><br><span class="line">[ceph_deploy.cli][INFO ] ceph-deploy options:</span><br><span class="line">[ceph_deploy.cli][INFO ]  username                      : None</span><br><span class="line">[ceph_deploy.cli][INFO ]  verbose                       : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  overwrite_conf                : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  subcommand                    : create-initial</span><br><span class="line">[ceph_deploy.cli][INFO ]  quiet                         : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x1160f38&gt;</span><br><span class="line">[ceph_deploy.cli][INFO ]  cluster                       : ceph</span><br><span class="line">[ceph_deploy.cli][INFO ]  func                          : &lt;function mon at 0x115c2a8&gt;</span><br><span class="line">[ceph_deploy.cli][INFO ]  ceph_conf                     : None</span><br><span class="line">[ceph_deploy.cli][INFO ]  default_release               : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  keyrings                      : None</span><br><span class="line">[ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts lab1</span><br><span class="line">[ceph_deploy.mon][DEBUG ] detecting platform for host lab1 ...</span><br><span class="line">[lab1][DEBUG ] connection detected need for sudo</span><br><span class="line">[lab1][DEBUG ] connected to host: lab1 </span><br><span class="line">[lab1][DEBUG ] detect platform information from remote host</span><br><span class="line">[lab1][DEBUG ] detect machine type</span><br><span class="line">[lab1][DEBUG ] find the location of an executable</span><br><span class="line">[ceph_deploy.mon][INFO ] distro info: CentOS Linux 7.5.1804 Core</span><br><span class="line">[lab1][DEBUG ] determining if provided host has same hostname in remote</span><br><span class="line">[lab1][DEBUG ] get remote short hostname</span><br><span class="line">[lab1][DEBUG ] deploying mon to lab1</span><br><span class="line">[lab1][DEBUG ] get remote short hostname</span><br><span class="line">[lab1][DEBUG ] remote hostname: lab1</span><br><span class="line">[lab1][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</span><br><span class="line">[lab1][DEBUG ] create the mon path if it does not exist</span><br><span class="line">[lab1][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-lab1/done</span><br><span class="line">[lab1][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-lab1/done</span><br><span class="line">[lab1][INFO ] creating keyring file: /var/lib/ceph/tmp/ceph-lab1.mon.keyring</span><br><span class="line">[lab1][DEBUG ] create the monitor keyring file</span><br><span class="line">[lab1][INFO ] Running command: sudo ceph-mon --cluster ceph --mkfs -i lab1 --keyring /var/lib/ceph/tmp/ceph-lab1.mon.keyring --setuser 167 --setgroup 167</span><br><span class="line">[lab1][INFO ] unlinking keyring file /var/lib/ceph/tmp/ceph-lab1.mon.keyring</span><br><span class="line">[lab1][DEBUG ] create a done file to avoid re-doing the mon deployment</span><br><span class="line">[lab1][DEBUG ] create the init path if it does not exist</span><br><span class="line">[lab1][INFO ] Running command: sudo systemctl enable ceph.target</span><br><span class="line">[lab1][INFO ] Running command: sudo systemctl enable ceph-mon@lab1</span><br><span class="line">[lab1][INFO ] Running command: sudo systemctl start ceph-mon@lab1</span><br><span class="line">[lab1][INFO ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.lab1.asok mon_status</span><br><span class="line">[lab1][DEBUG ] ********************************************************************************</span><br><span class="line">[lab1][DEBUG ] status for monitor: mon.lab1</span><br><span class="line">[lab1][DEBUG ] &#123;</span><br><span class="line">[lab1][DEBUG ]   "election_epoch": 3, </span><br><span class="line">[lab1][DEBUG ]   "extra_probe_peers": [], </span><br><span class="line">[lab1][DEBUG ]   "feature_map": &#123;</span><br><span class="line">[lab1][DEBUG ]     "mon": &#123;</span><br><span class="line">[lab1][DEBUG ]       "group": &#123;</span><br><span class="line">[lab1][DEBUG ]         "features": "0x3ffddff8eea4fffb", </span><br><span class="line">[lab1][DEBUG ]         "num": 1, </span><br><span class="line">[lab1][DEBUG ]         "release": "luminous"</span><br><span class="line">[lab1][DEBUG ]       &#125;</span><br><span class="line">[lab1][DEBUG ]     &#125;</span><br><span class="line">[lab1][DEBUG ]   &#125;, </span><br><span class="line">[lab1][DEBUG ]   "features": &#123;</span><br><span class="line">[lab1][DEBUG ]     "quorum_con": "4611087853745930235", </span><br><span class="line">[lab1][DEBUG ]     "quorum_mon": [</span><br><span class="line">[lab1][DEBUG ]       "kraken", </span><br><span class="line">[lab1][DEBUG ]       "luminous"</span><br><span class="line">[lab1][DEBUG ]     ], </span><br><span class="line">[lab1][DEBUG ]     "required_con": "153140804152475648", </span><br><span class="line">[lab1][DEBUG ]     "required_mon": [</span><br><span class="line">[lab1][DEBUG ]       "kraken", </span><br><span class="line">[lab1][DEBUG ]       "luminous"</span><br><span class="line">[lab1][DEBUG ]     ]</span><br><span class="line">[lab1][DEBUG ]   &#125;, </span><br><span class="line">[lab1][DEBUG ]   "monmap": &#123;</span><br><span class="line">[lab1][DEBUG ]     "created": "2018-08-17 14:46:18.770540", </span><br><span class="line">[lab1][DEBUG ]     "epoch": 1, </span><br><span class="line">[lab1][DEBUG ]     "features": &#123;</span><br><span class="line">[lab1][DEBUG ]       "optional": [], </span><br><span class="line">[lab1][DEBUG ]       "persistent": [</span><br><span class="line">[lab1][DEBUG ]         "kraken", </span><br><span class="line">[lab1][DEBUG ]         "luminous"</span><br><span class="line">[lab1][DEBUG ]       ]</span><br><span class="line">[lab1][DEBUG ]     &#125;, </span><br><span class="line">[lab1][DEBUG ]     "fsid": "fb212173-233c-4c5e-a98e-35be9359f8e2", </span><br><span class="line">[lab1][DEBUG ]     "modified": "2018-08-17 14:46:18.770540", </span><br><span class="line">[lab1][DEBUG ]     "mons": [</span><br><span class="line">[lab1][DEBUG ]       &#123;</span><br><span class="line">[lab1][DEBUG ]         "addr": "192.168.105.92:6789/0", </span><br><span class="line">[lab1][DEBUG ]         "name": "lab1", </span><br><span class="line">[lab1][DEBUG ]         "public_addr": "192.168.105.92:6789/0", </span><br><span class="line">[lab1][DEBUG ]         "rank": 0</span><br><span class="line">[lab1][DEBUG ]       &#125;</span><br><span class="line">[lab1][DEBUG ]     ]</span><br><span class="line">[lab1][DEBUG ]   &#125;, </span><br><span class="line">[lab1][DEBUG ]   "name": "lab1", </span><br><span class="line">[lab1][DEBUG ]   "outside_quorum": [], </span><br><span class="line">[lab1][DEBUG ]   "quorum": [</span><br><span class="line">[lab1][DEBUG ]     0</span><br><span class="line">[lab1][DEBUG ]   ], </span><br><span class="line">[lab1][DEBUG ]   "rank": 0, </span><br><span class="line">[lab1][DEBUG ]   "state": "leader", </span><br><span class="line">[lab1][DEBUG ]   "sync_provider": []</span><br><span class="line">[lab1][DEBUG ] &#125;</span><br><span class="line">[lab1][DEBUG ] ********************************************************************************</span><br><span class="line">[lab1][INFO ] monitor: mon.lab1 is running</span><br><span class="line">[lab1][INFO ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.lab1.asok mon_status</span><br><span class="line">[ceph_deploy.mon][INFO ] processing monitor mon.lab1</span><br><span class="line">[lab1][DEBUG ] connection detected need for sudo</span><br><span class="line">[lab1][DEBUG ] connected to host: lab1 </span><br><span class="line">[lab1][DEBUG ] detect platform information from remote host</span><br><span class="line">[lab1][DEBUG ] detect machine type</span><br><span class="line">[lab1][DEBUG ] find the location of an executable</span><br><span class="line">[lab1][INFO ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.lab1.asok mon_status</span><br><span class="line">[ceph_deploy.mon][INFO ] mon.lab1 monitor has reached quorum!</span><br><span class="line">[ceph_deploy.mon][INFO ] all initial monitors are running and have formed quorum</span><br><span class="line">[ceph_deploy.mon][INFO ] Running gatherkeys...</span><br><span class="line">[ceph_deploy.gatherkeys][INFO ] Storing keys in temp directory /tmp/tmpfUkCWD</span><br><span class="line">[lab1][DEBUG ] connection detected need for sudo</span><br><span class="line">[lab1][DEBUG ] connected to host: lab1 </span><br><span class="line">[lab1][DEBUG ] detect platform information from remote host</span><br><span class="line">[lab1][DEBUG ] detect machine type</span><br><span class="line">[lab1][DEBUG ] get remote short hostname</span><br><span class="line">[lab1][DEBUG ] fetch remote file</span><br><span class="line">[lab1][INFO ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --admin-daemon=/var/run/ceph/ceph-mon.lab1.asok mon_status</span><br><span class="line">[lab1][INFO ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-lab1/keyring auth get client.admin</span><br><span class="line">[lab1][INFO ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-lab1/keyring auth get-or-create client.admin osd allow * mds allow * mon allow * mgr allow *</span><br><span class="line">[lab1][INFO ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-lab1/keyring auth get client.bootstrap-mds</span><br><span class="line">[lab1][INFO ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-lab1/keyring auth get-or-create client.bootstrap-mds mon allow profile bootstrap-mds</span><br><span class="line">[lab1][INFO ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-lab1/keyring auth get client.bootstrap-mgr</span><br><span class="line">[lab1][INFO ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-lab1/keyring auth get-or-create client.bootstrap-mgr mon allow profile bootstrap-mgr</span><br><span class="line">[lab1][INFO ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-lab1/keyring auth get client.bootstrap-osd</span><br><span class="line">[lab1][INFO ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-lab1/keyring auth get-or-create client.bootstrap-osd mon allow profile bootstrap-osd</span><br><span class="line">[lab1][INFO ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-lab1/keyring auth get client.bootstrap-rgw</span><br><span class="line">[lab1][INFO ] Running command: sudo /usr/bin/ceph --connect-timeout=25 --cluster=ceph --name mon. --keyring=/var/lib/ceph/mon/ceph-lab1/keyring auth get-or-create client.bootstrap-rgw mon allow profile bootstrap-rgw</span><br><span class="line">[ceph_deploy.gatherkeys][INFO ] Storing ceph.client.admin.keyring</span><br><span class="line">[ceph_deploy.gatherkeys][INFO ] Storing ceph.bootstrap-mds.keyring</span><br><span class="line">[ceph_deploy.gatherkeys][INFO ] Storing ceph.bootstrap-mgr.keyring</span><br><span class="line">[ceph_deploy.gatherkeys][INFO ] keyring 'ceph.mon.keyring' already exists</span><br><span class="line">[ceph_deploy.gatherkeys][INFO ] Storing ceph.bootstrap-osd.keyring</span><br><span class="line">[ceph_deploy.gatherkeys][INFO ] Storing ceph.bootstrap-rgw.keyring</span><br><span class="line">[ceph_deploy.gatherkeys][INFO ] Destroy temp directory /tmp/tmpfUkCWD</span><br></pre></td></tr></table></figure>
<p>用 ceph-deploy 把配置文件和 admin 密钥拷贝到管理节点和 Ceph 节点，这样你每次执行 Ceph 命令行时就无需指定 <code>monitor</code> 地址和<code>ceph.client.admin.keyring</code>了。</p>
<p><code>ceph-deploy admin lab1 lab4 lab5</code></p>
<blockquote>
<p>注意<br>ceph-deploy 和本地管理主机（ admin-node ）通信时，必须通过主机名可达。必要时可修改 /etc/hosts ，加入管理主机的名字。</p>
</blockquote>
<p>确保你对 ceph.client.admin.keyring 有正确的操作权限。</p>
<p><code>sudo chmod +r /etc/ceph/ceph.client.admin.keyring</code></p>
<p>安装mrg</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy mgr create lab1</span><br><span class="line">ceph-deploy mgr create lab2</span><br><span class="line">ceph-deploy mgr create lab3</span><br></pre></td></tr></table></figure>
<p>检查集群的健康状况。</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[ceph-admin@lab1 ceph-cluster]$ ceph health</span><br><span class="line">HEALTH_OK</span><br></pre></td></tr></table></figure>
<h5 id="3-2-增加OSD"><a href="#3-2-增加OSD" class="headerlink" title="3.2 增加OSD"></a>3.2 增加OSD</h5><p>列举磁盘并擦净</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br></pre></td><td class="code"><pre><span class="line">[ceph-admin@lab1 ceph-cluster]$ ceph-deploy disk list lab4</span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /home/ceph-admin/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO ] Invoked (2.0.1): /bin/ceph-deploy disk list lab4</span><br><span class="line">[ceph_deploy.cli][INFO ] ceph-deploy options:</span><br><span class="line">[ceph_deploy.cli][INFO ]  username                      : None</span><br><span class="line">[ceph_deploy.cli][INFO ]  verbose                       : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  debug                         : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  overwrite_conf                : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  subcommand                    : list</span><br><span class="line">[ceph_deploy.cli][INFO ]  quiet                         : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x20d97e8&gt;</span><br><span class="line">[ceph_deploy.cli][INFO ]  cluster                       : ceph</span><br><span class="line">[ceph_deploy.cli][INFO ]  host                          : ['lab4']</span><br><span class="line">[ceph_deploy.cli][INFO ]  func                          : &lt;function disk at 0x20ca7d0&gt;</span><br><span class="line">[ceph_deploy.cli][INFO ]  ceph_conf                     : None</span><br><span class="line">[ceph_deploy.cli][INFO ]  default_release               : False</span><br><span class="line">[lab4][DEBUG ] connection detected need for sudo</span><br><span class="line">[lab4][DEBUG ] connected to host: lab4 </span><br><span class="line">[lab4][DEBUG ] detect platform information from remote host</span><br><span class="line">[lab4][DEBUG ] detect machine type</span><br><span class="line">[lab4][DEBUG ] find the location of an executable</span><br><span class="line">[lab4][INFO ] Running command: sudo fdisk -l</span><br><span class="line">[lab4][INFO ] Disk /dev/sda: 107.4 GB, 107374182400 bytes, 209715200 sectors</span><br><span class="line">[lab4][INFO ] Disk /dev/sdb: 214.7 GB, 214748364800 bytes, 419430400 sectors</span><br><span class="line">[lab4][INFO ] Disk /dev/mapper/cl-root: 97.8 GB, 97840529408 bytes, 191094784 sectors</span><br><span class="line">[lab4][INFO ] Disk /dev/mapper/cl-swap: 8455 MB, 8455716864 bytes, 16515072 sectors</span><br><span class="line">[lab4][INFO ] Disk /dev/mapper/vg_a66945efa6324ffeb209d165cac8ede9-tp_1f4ce4f4bfb224aa385f35516236af43_tmeta: 12 MB, 12582912 bytes, 24576 sectors</span><br><span class="line">[lab4][INFO ] Disk /dev/mapper/vg_a66945efa6324ffeb209d165cac8ede9-tp_1f4ce4f4bfb224aa385f35516236af43_tdata: 2147 MB, 2147483648 bytes, 4194304 sectors</span><br><span class="line">[lab4][INFO ] Disk /dev/mapper/vg_a66945efa6324ffeb209d165cac8ede9-tp_1f4ce4f4bfb224aa385f35516236af43-tpool: 2147 MB, 2147483648 bytes, 4194304 sectors</span><br><span class="line">[lab4][INFO ] Disk /dev/mapper/vg_a66945efa6324ffeb209d165cac8ede9-tp_1f4ce4f4bfb224aa385f35516236af43: 2147 MB, 2147483648 bytes, 4194304 sectors</span><br><span class="line">[lab4][INFO ] Disk /dev/mapper/vg_a66945efa6324ffeb209d165cac8ede9-brick_1f4ce4f4bfb224aa385f35516236af43: 2147 MB, 2147483648 bytes, 4194304 sectors</span><br><span class="line">[ceph-admin@lab1 ceph-cluster]$ ceph-deploy disk zap lab4 /dev/sdb</span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /home/ceph-admin/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO ] Invoked (2.0.1): /bin/ceph-deploy disk zap lab4 /dev/sdb</span><br><span class="line">[ceph_deploy.cli][INFO ] ceph-deploy options:</span><br><span class="line">[ceph_deploy.cli][INFO ]  username                      : None</span><br><span class="line">[ceph_deploy.cli][INFO ]  verbose                       : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  debug                         : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  overwrite_conf                : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  subcommand                    : zap</span><br><span class="line">[ceph_deploy.cli][INFO ]  quiet                         : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0xd447e8&gt;</span><br><span class="line">[ceph_deploy.cli][INFO ]  cluster                       : ceph</span><br><span class="line">[ceph_deploy.cli][INFO ]  host                          : lab4</span><br><span class="line">[ceph_deploy.cli][INFO ]  func                          : &lt;function disk at 0xd357d0&gt;</span><br><span class="line">[ceph_deploy.cli][INFO ]  ceph_conf                     : None</span><br><span class="line">[ceph_deploy.cli][INFO ]  default_release               : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  disk                          : ['/dev/sdb']</span><br><span class="line">[ceph_deploy.osd][DEBUG ] zapping /dev/sdb on lab4</span><br><span class="line">[lab4][DEBUG ] connection detected need for sudo</span><br><span class="line">[lab4][DEBUG ] connected to host: lab4 </span><br><span class="line">[lab4][DEBUG ] detect platform information from remote host</span><br><span class="line">[lab4][DEBUG ] detect machine type</span><br><span class="line">[lab4][DEBUG ] find the location of an executable</span><br><span class="line">[ceph_deploy.osd][INFO ] Distro info: CentOS Linux 7.5.1804 Core</span><br><span class="line">[lab4][DEBUG ] zeroing last few blocks of device</span><br><span class="line">[lab4][DEBUG ] find the location of an executable</span><br><span class="line">[lab4][INFO ] Running command: sudo /usr/sbin/ceph-volume lvm zap /dev/sdb</span><br><span class="line">[lab4][DEBUG ] --&gt; Zapping: /dev/sdb</span><br><span class="line">[lab4][DEBUG ] Running command: /usr/sbin/cryptsetup status /dev/mapper/</span><br><span class="line">[lab4][DEBUG ]  stdout: /dev/mapper/ is inactive.</span><br><span class="line">[lab4][DEBUG ] Running command: wipefs --all /dev/sdb</span><br><span class="line">[lab4][DEBUG ] Running command: dd if=/dev/zero of=/dev/sdb bs=1M count=10</span><br><span class="line">[lab4][DEBUG ] --&gt; Zapping successful for: /dev/sdb</span><br></pre></td></tr></table></figure>
<p>同理，lab5的sdb也一样。</p>
<p>创建pv、vg、lv，略。</p>
<p>创建 OSD</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line">[ceph-admin@lab1 ceph-cluster]$ ceph-deploy osd create lab4 --fs-type btrfs --data vg1/lvol0     </span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /home/ceph-admin/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO ] Invoked (2.0.1): /bin/ceph-deploy osd create lab4 --fs-type btrfs --data vg1/lvol0</span><br><span class="line">[ceph_deploy.cli][INFO ] ceph-deploy options:</span><br><span class="line">[ceph_deploy.cli][INFO ]  verbose                       : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  bluestore                     : None</span><br><span class="line">[ceph_deploy.cli][INFO ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x26d4908&gt;</span><br><span class="line">[ceph_deploy.cli][INFO ]  cluster                       : ceph</span><br><span class="line">[ceph_deploy.cli][INFO ]  fs_type                       : btrfs</span><br><span class="line">[ceph_deploy.cli][INFO ]  block_wal                     : None</span><br><span class="line">[ceph_deploy.cli][INFO ]  default_release               : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  username                      : None</span><br><span class="line">[ceph_deploy.cli][INFO ]  journal                       : None</span><br><span class="line">[ceph_deploy.cli][INFO ]  subcommand                    : create</span><br><span class="line">[ceph_deploy.cli][INFO ]  host                          : lab4</span><br><span class="line">[ceph_deploy.cli][INFO ]  filestore                     : None</span><br><span class="line">[ceph_deploy.cli][INFO ]  func                          : &lt;function osd at 0x26c4758&gt;</span><br><span class="line">[ceph_deploy.cli][INFO ]  ceph_conf                     : None</span><br><span class="line">[ceph_deploy.cli][INFO ]  zap_disk                      : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  data                          : vg1/lvol0</span><br><span class="line">[ceph_deploy.cli][INFO ]  block_db                      : None</span><br><span class="line">[ceph_deploy.cli][INFO ]  dmcrypt                       : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  overwrite_conf                : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  dmcrypt_key_dir               : /etc/ceph/dmcrypt-keys</span><br><span class="line">[ceph_deploy.cli][INFO ]  quiet                         : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  debug                         : False</span><br><span class="line">[ceph_deploy.osd][DEBUG ] Creating OSD on cluster ceph with data device vg1/lvol0</span><br><span class="line">[lab4][DEBUG ] connection detected need for sudo</span><br><span class="line">[lab4][DEBUG ] connected to host: lab4 </span><br><span class="line">[lab4][DEBUG ] detect platform information from remote host</span><br><span class="line">[lab4][DEBUG ] detect machine type</span><br><span class="line">[lab4][DEBUG ] find the location of an executable</span><br><span class="line">[ceph_deploy.osd][INFO ] Distro info: CentOS Linux 7.5.1804 Core</span><br><span class="line">[ceph_deploy.osd][DEBUG ] Deploying osd to lab4</span><br><span class="line">[lab4][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</span><br><span class="line">[lab4][DEBUG ] find the location of an executable</span><br><span class="line">[lab4][INFO ] Running command: sudo /usr/sbin/ceph-volume --cluster ceph lvm create --bluestore --data vg1/lvol0</span><br><span class="line">[lab4][DEBUG ] Running command: /bin/ceph-authtool --gen-print-key</span><br><span class="line">[lab4][DEBUG ] Running command: /bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring -i - osd new 7bb2b8f4-9e9d-4cd2-a2da-802a953a4d62</span><br><span class="line">[lab4][DEBUG ] Running command: /bin/ceph-authtool --gen-print-key</span><br><span class="line">[lab4][DEBUG ] Running command: mount -t tmpfs tmpfs /var/lib/ceph/osd/ceph-1</span><br><span class="line">[lab4][DEBUG ] Running command: chown -h ceph:ceph /dev/vg1/lvol0</span><br><span class="line">[lab4][DEBUG ] Running command: chown -R ceph:ceph /dev/dm-2</span><br><span class="line">[lab4][DEBUG ] Running command: ln -s /dev/vg1/lvol0 /var/lib/ceph/osd/ceph-1/block</span><br><span class="line">[lab4][DEBUG ] Running command: ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /var/lib/ceph/osd/ceph-1/activate.monmap</span><br><span class="line">[lab4][DEBUG ]  stderr: got monmap epoch 1</span><br><span class="line">[lab4][DEBUG ] Running command: ceph-authtool /var/lib/ceph/osd/ceph-1/keyring --create-keyring --name osd.1 --add-key AQAmjHZbiiGUChAAVCWdPZqHms99mLgSZ7M+fQ==</span><br><span class="line">[lab4][DEBUG ]  stdout: creating /var/lib/ceph/osd/ceph-1/keyring</span><br><span class="line">[lab4][DEBUG ] added entity osd.1 auth auth(auid = 18446744073709551615 key=AQAmjHZbiiGUChAAVCWdPZqHms99mLgSZ7M+fQ== with 0 caps)</span><br><span class="line">[lab4][DEBUG ] Running command: chown -R ceph:ceph /var/lib/ceph/osd/ceph-1/keyring</span><br><span class="line">[lab4][DEBUG ] Running command: chown -R ceph:ceph /var/lib/ceph/osd/ceph-1/</span><br><span class="line">[lab4][DEBUG ] Running command: /bin/ceph-osd --cluster ceph --osd-objectstore bluestore --mkfs -i 1 --monmap /var/lib/ceph/osd/ceph-1/activate.monmap --keyfile - --osd-data /var/lib/ceph/osd/ceph-1/ --osd-uuid 7bb2b8f4-9e9d-4cd2-a2da-802a953a4d62 --setuser ceph --setgroup ceph</span><br><span class="line">[lab4][DEBUG ] --&gt; ceph-volume lvm prepare successful for: vg1/lvol0</span><br><span class="line">[lab4][DEBUG ] Running command: ceph-bluestore-tool --cluster=ceph prime-osd-dir --dev /dev/vg1/lvol0 --path /var/lib/ceph/osd/ceph-1</span><br><span class="line">[lab4][DEBUG ] Running command: ln -snf /dev/vg1/lvol0 /var/lib/ceph/osd/ceph-1/block</span><br><span class="line">[lab4][DEBUG ] Running command: chown -h ceph:ceph /var/lib/ceph/osd/ceph-1/block</span><br><span class="line">[lab4][DEBUG ] Running command: chown -R ceph:ceph /dev/dm-2</span><br><span class="line">[lab4][DEBUG ] Running command: chown -R ceph:ceph /var/lib/ceph/osd/ceph-1</span><br><span class="line">[lab4][DEBUG ] Running command: systemctl enable ceph-volume@lvm-1-7bb2b8f4-9e9d-4cd2-a2da-802a953a4d62</span><br><span class="line">[lab4][DEBUG ]  stderr: Created symlink from /etc/systemd/system/multi-user.target.wants/ceph-volume@lvm-1-7bb2b8f4-9e9d-4cd2-a2da-802a953a4d62.service to /usr/lib/systemd/system/ceph-volume@.service.</span><br><span class="line">[lab4][DEBUG ] Running command: systemctl start ceph-osd@1</span><br><span class="line">[lab4][DEBUG ] --&gt; ceph-volume lvm activate successful for osd ID: 1</span><br><span class="line">[lab4][DEBUG ] --&gt; ceph-volume lvm create successful for: vg1/lvol0</span><br><span class="line">[lab4][INFO ] checking OSD status...</span><br><span class="line">[lab4][DEBUG ] find the location of an executable</span><br><span class="line">[lab4][INFO ] Running command: sudo /bin/ceph --cluster=ceph osd stat --format=json</span><br><span class="line">[lab4][WARNIN] there is 1 OSD down</span><br><span class="line">[lab4][WARNIN] there is 1 OSD out</span><br><span class="line">[ceph_deploy.osd][DEBUG ] Host lab4 is now ready for osd use.</span><br></pre></td></tr></table></figure>
<p><code>ceph-deploy osd create lab5 --fs-type btrfs --data vg1/lvol0</code></p>
<h4 id="4、扩展集群"><a href="#4、扩展集群" class="headerlink" title="4、扩展集群"></a>4、扩展集群</h4><p>一个基本的集群启动并开始运行后，下一步就是扩展集群。在 lab6、lab7 各上添加一个 OSD 守护进程和一个元数据服务器。然后分别在 lab2 和 lab3 上添加 <code>Ceph Monitor</code>，以形成 Monitors 的法定人数。</p>
<p>添加 MONITORS</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ceph-deploy mon add lab2</span><br><span class="line">ceph-deploy mon add lab3</span><br></pre></td></tr></table></figure>
<p>过程：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br></pre></td><td class="code"><pre><span class="line">[ceph-admin@lab1 ceph-cluster]$ ceph-deploy mon add lab3</span><br><span class="line">[ceph_deploy.conf][DEBUG ] found configuration file at: /home/ceph-admin/.cephdeploy.conf</span><br><span class="line">[ceph_deploy.cli][INFO ] Invoked (2.0.1): /bin/ceph-deploy mon add lab3</span><br><span class="line">[ceph_deploy.cli][INFO ] ceph-deploy options:</span><br><span class="line">[ceph_deploy.cli][INFO ]  username                      : None</span><br><span class="line">[ceph_deploy.cli][INFO ]  verbose                       : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  overwrite_conf                : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  subcommand                    : add</span><br><span class="line">[ceph_deploy.cli][INFO ]  quiet                         : False</span><br><span class="line">[ceph_deploy.cli][INFO ]  cd_conf                       : &lt;ceph_deploy.conf.cephdeploy.Conf instance at 0x29f0f38&gt;</span><br><span class="line">[ceph_deploy.cli][INFO ]  cluster                       : ceph</span><br><span class="line">[ceph_deploy.cli][INFO ]  mon                           : ['lab3']</span><br><span class="line">[ceph_deploy.cli][INFO ]  func                          : &lt;function mon at 0x29ec2a8&gt;</span><br><span class="line">[ceph_deploy.cli][INFO ]  address                       : None</span><br><span class="line">[ceph_deploy.cli][INFO ]  ceph_conf                     : None</span><br><span class="line">[ceph_deploy.cli][INFO ]  default_release               : False</span><br><span class="line">[ceph_deploy.mon][INFO ] ensuring configuration of new mon host: lab3</span><br><span class="line">[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to lab3</span><br><span class="line">[lab3][DEBUG ] connection detected need for sudo</span><br><span class="line">[lab3][DEBUG ] connected to host: lab3 </span><br><span class="line">[lab3][DEBUG ] detect platform information from remote host</span><br><span class="line">[lab3][DEBUG ] detect machine type</span><br><span class="line">[lab3][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</span><br><span class="line">[ceph_deploy.mon][DEBUG ] Adding mon to cluster ceph, host lab3</span><br><span class="line">[ceph_deploy.mon][DEBUG ] using mon address by resolving host: 192.168.105.94</span><br><span class="line">[ceph_deploy.mon][DEBUG ] detecting platform for host lab3 ...</span><br><span class="line">[lab3][DEBUG ] connection detected need for sudo</span><br><span class="line">[lab3][DEBUG ] connected to host: lab3 </span><br><span class="line">[lab3][DEBUG ] detect platform information from remote host</span><br><span class="line">[lab3][DEBUG ] detect machine type</span><br><span class="line">[lab3][DEBUG ] find the location of an executable</span><br><span class="line">[ceph_deploy.mon][INFO ] distro info: CentOS Linux 7.5.1804 Core</span><br><span class="line">[lab3][DEBUG ] determining if provided host has same hostname in remote</span><br><span class="line">[lab3][DEBUG ] get remote short hostname</span><br><span class="line">[lab3][DEBUG ] adding mon to lab3</span><br><span class="line">[lab3][DEBUG ] get remote short hostname</span><br><span class="line">[lab3][DEBUG ] write cluster configuration to /etc/ceph/&#123;cluster&#125;.conf</span><br><span class="line">[lab3][DEBUG ] create the mon path if it does not exist</span><br><span class="line">[lab3][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-lab3/done</span><br><span class="line">[lab3][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-lab3/done</span><br><span class="line">[lab3][INFO ] creating keyring file: /var/lib/ceph/tmp/ceph-lab3.mon.keyring</span><br><span class="line">[lab3][DEBUG ] create the monitor keyring file</span><br><span class="line">[lab3][INFO ] Running command: sudo ceph --cluster ceph mon getmap -o /var/lib/ceph/tmp/ceph.lab3.monmap</span><br><span class="line">[lab3][WARNIN] got monmap epoch 2</span><br><span class="line">[lab3][INFO ] Running command: sudo ceph-mon --cluster ceph --mkfs -i lab3 --monmap /var/lib/ceph/tmp/ceph.lab3.monmap --keyring /var/lib/ceph/tmp/ceph-lab3.mon.keyring --setuser 167 --setgroup 167</span><br><span class="line">[lab3][INFO ] unlinking keyring file /var/lib/ceph/tmp/ceph-lab3.mon.keyring</span><br><span class="line">[lab3][DEBUG ] create a done file to avoid re-doing the mon deployment</span><br><span class="line">[lab3][DEBUG ] create the init path if it does not exist</span><br><span class="line">[lab3][INFO ] Running command: sudo systemctl enable ceph.target</span><br><span class="line">[lab3][INFO ] Running command: sudo systemctl enable ceph-mon@lab3</span><br><span class="line">[lab3][WARNIN] Created symlink from /etc/systemd/system/ceph-mon.target.wants/ceph-mon@lab3.service to /usr/lib/systemd/system/ceph-mon@.service.</span><br><span class="line">[lab3][INFO ] Running command: sudo systemctl start ceph-mon@lab3</span><br><span class="line">[lab3][INFO ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.lab3.asok mon_status</span><br><span class="line">[lab3][WARNIN] lab3 is not defined in `mon initial members`</span><br><span class="line">[lab3][WARNIN] monitor lab3 does not exist in monmap</span><br><span class="line">[lab3][INFO ] Running command: sudo ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.lab3.asok mon_status</span><br><span class="line">[lab3][DEBUG ] ********************************************************************************</span><br><span class="line">[lab3][DEBUG ] status for monitor: mon.lab3</span><br><span class="line">[lab3][DEBUG ] &#123;</span><br><span class="line">[lab3][DEBUG ]   "election_epoch": 0, </span><br><span class="line">[lab3][DEBUG ]   "extra_probe_peers": [</span><br><span class="line">[lab3][DEBUG ]     "192.168.105.93:6789/0"</span><br><span class="line">[lab3][DEBUG ]   ], </span><br><span class="line">[lab3][DEBUG ]   "feature_map": &#123;</span><br><span class="line">[lab3][DEBUG ]     "mon": &#123;</span><br><span class="line">[lab3][DEBUG ]       "group": &#123;</span><br><span class="line">[lab3][DEBUG ]         "features": "0x3ffddff8eea4fffb", </span><br><span class="line">[lab3][DEBUG ]         "num": 1, </span><br><span class="line">[lab3][DEBUG ]         "release": "luminous"</span><br><span class="line">[lab3][DEBUG ]       &#125;</span><br><span class="line">[lab3][DEBUG ]     &#125;</span><br><span class="line">[lab3][DEBUG ]   &#125;, </span><br><span class="line">[lab3][DEBUG ]   "features": &#123;</span><br><span class="line">[lab3][DEBUG ]     "quorum_con": "0", </span><br><span class="line">[lab3][DEBUG ]     "quorum_mon": [], </span><br><span class="line">[lab3][DEBUG ]     "required_con": "144115188077969408", </span><br><span class="line">[lab3][DEBUG ]     "required_mon": [</span><br><span class="line">[lab3][DEBUG ]       "kraken", </span><br><span class="line">[lab3][DEBUG ]       "luminous"</span><br><span class="line">[lab3][DEBUG ]     ]</span><br><span class="line">[lab3][DEBUG ]   &#125;, </span><br><span class="line">[lab3][DEBUG ]   "monmap": &#123;</span><br><span class="line">[lab3][DEBUG ]     "created": "2018-08-17 16:38:21.075805", </span><br><span class="line">[lab3][DEBUG ]     "epoch": 3, </span><br><span class="line">[lab3][DEBUG ]     "features": &#123;</span><br><span class="line">[lab3][DEBUG ]       "optional": [], </span><br><span class="line">[lab3][DEBUG ]       "persistent": [</span><br><span class="line">[lab3][DEBUG ]         "kraken", </span><br><span class="line">[lab3][DEBUG ]         "luminous"</span><br><span class="line">[lab3][DEBUG ]       ]</span><br><span class="line">[lab3][DEBUG ]     &#125;, </span><br><span class="line">[lab3][DEBUG ]     "fsid": "4395328d-17fc-4039-96d0-1d3241a4cafa", </span><br><span class="line">[lab3][DEBUG ]     "modified": "2018-08-17 17:58:23.179585", </span><br><span class="line">[lab3][DEBUG ]     "mons": [</span><br><span class="line">[lab3][DEBUG ]       &#123;</span><br><span class="line">[lab3][DEBUG ]         "addr": "192.168.105.92:6789/0", </span><br><span class="line">[lab3][DEBUG ]         "name": "lab1", </span><br><span class="line">[lab3][DEBUG ]         "public_addr": "192.168.105.92:6789/0", </span><br><span class="line">[lab3][DEBUG ]         "rank": 0</span><br><span class="line">[lab3][DEBUG ]       &#125;, </span><br><span class="line">[lab3][DEBUG ]       &#123;</span><br><span class="line">[lab3][DEBUG ]         "addr": "192.168.105.93:6789/0", </span><br><span class="line">[lab3][DEBUG ]         "name": "lab2", </span><br><span class="line">[lab3][DEBUG ]         "public_addr": "192.168.105.93:6789/0", </span><br><span class="line">[lab3][DEBUG ]         "rank": 1</span><br><span class="line">[lab3][DEBUG ]       &#125;, </span><br><span class="line">[lab3][DEBUG ]       &#123;</span><br><span class="line">[lab3][DEBUG ]         "addr": "192.168.105.94:6789/0", </span><br><span class="line">[lab3][DEBUG ]         "name": "lab3", </span><br><span class="line">[lab3][DEBUG ]         "public_addr": "192.168.105.94:6789/0", </span><br><span class="line">[lab3][DEBUG ]         "rank": 2</span><br><span class="line">[lab3][DEBUG ]       &#125;</span><br><span class="line">[lab3][DEBUG ]     ]</span><br><span class="line">[lab3][DEBUG ]   &#125;, </span><br><span class="line">[lab3][DEBUG ]   "name": "lab3", </span><br><span class="line">[lab3][DEBUG ]   "outside_quorum": [</span><br><span class="line">[lab3][DEBUG ]     "lab3"</span><br><span class="line">[lab3][DEBUG ]   ], </span><br><span class="line">[lab3][DEBUG ]   "quorum": [], </span><br><span class="line">[lab3][DEBUG ]   "rank": 2, </span><br><span class="line">[lab3][DEBUG ]   "state": "probing", </span><br><span class="line">[lab3][DEBUG ]   "sync_provider": []</span><br><span class="line">[lab3][DEBUG ] &#125;</span><br><span class="line">[lab3][DEBUG ] ********************************************************************************</span><br><span class="line">[lab3][INFO ] monitor: mon.lab3 is running</span><br></pre></td></tr></table></figure>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">[ceph-admin@lab1 ceph-cluster]$ ceph quorum_status --format json-pretty</span><br><span class="line"></span><br><span class="line">&#123;</span><br><span class="line">    "election_epoch": 12,</span><br><span class="line">    "quorum": [</span><br><span class="line">        0,</span><br><span class="line">        1,</span><br><span class="line">        2</span><br><span class="line">    ],</span><br><span class="line">    "quorum_names": [</span><br><span class="line">        "lab1",</span><br><span class="line">        "lab2",</span><br><span class="line">        "lab3"</span><br><span class="line">    ],</span><br><span class="line">    "quorum_leader_name": "lab1",</span><br><span class="line">    "monmap": &#123;</span><br><span class="line">        "epoch": 3,</span><br><span class="line">        "fsid": "4395328d-17fc-4039-96d0-1d3241a4cafa",</span><br><span class="line">        "modified": "2018-08-17 17:58:23.179585",</span><br><span class="line">        "created": "2018-08-17 16:38:21.075805",</span><br><span class="line">        "features": &#123;</span><br><span class="line">            "persistent": [</span><br><span class="line">                "kraken",</span><br><span class="line">                "luminous"</span><br><span class="line">            ],</span><br><span class="line">            "optional": []</span><br><span class="line">        &#125;,</span><br><span class="line">        "mons": [</span><br><span class="line">            &#123;</span><br><span class="line">                "rank": 0,</span><br><span class="line">                "name": "lab1",</span><br><span class="line">                "addr": "192.168.105.92:6789/0",</span><br><span class="line">                "public_addr": "192.168.105.92:6789/0"</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                "rank": 1,</span><br><span class="line">                "name": "lab2",</span><br><span class="line">                "addr": "192.168.105.93:6789/0",</span><br><span class="line">                "public_addr": "192.168.105.93:6789/0"</span><br><span class="line">            &#125;,</span><br><span class="line">            &#123;</span><br><span class="line">                "rank": 2,</span><br><span class="line">                "name": "lab3",</span><br><span class="line">                "addr": "192.168.105.94:6789/0",</span><br><span class="line">                "public_addr": "192.168.105.94:6789/0"</span><br><span class="line">            &#125;</span><br><span class="line">        ]</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><em>添加元数据服务器</em></p>
<p>至少需要一个元数据服务器才能使用 CephFS ，执行下列命令创建元数据服务器：</p>
<p><code>ceph-deploy mds create lab4</code></p>
<p>到此，可以创建RBD和cephFS的ceph集群搭建完成。</p>
<h4 id="5、Ceph使用技巧"><a href="#5、Ceph使用技巧" class="headerlink" title="5、Ceph使用技巧"></a>5、Ceph使用技巧</h4><p><em>推送配置文件</em></p>
<p>只推送配置文件</p>
<p><code>ceph-deploy --overwrite-conf config push lab1 lab2</code></p>
<p>推送配置文件和client.admin key</p>
<p><code>ceph-deploy admin lab1 lab2</code></p>
<p><em>查看状态的常用命令</em></p>
<p>集群状态</p>
<p><code>ceph -s</code></p>
<p>查看正在操作的动作</p>
<p><code>ceph -w</code></p>
<p>查看已经创建的磁盘</p>
<p><code>rbd ls -l</code></p>
<p>查看ceph集群 </p>
<p><code>ceph osd tree</code></p>
<p>查看ceph授权信息</p>
<p><code>ceph auth get client.admin</code></p>
<p>移除monitor节点</p>
<p><code>ceph-deploy mon destroy lab1</code></p>
<p>详细列出集群每块磁盘的使用情况</p>
<p><code>ceph osd df</code></p>
<p>检查 MDS 状态:</p>
<p><code>ceph mds stat</code></p>
<p><em>开启Dashbord管理界面</em></p>
<p>创建管理域密钥<br><code>ceph auth get-or-create mgr.lab1 mon &#39;allow profile mgr&#39; osd &#39;allow *&#39; mds &#39;allow *&#39;</code><br>或：<br><code>ceph auth get-key client.admin | base64</code></p>
<p><em>开启 ceph-mgr 管理域</em></p>
<p><code>ceph-mgr -i master</code></p>
<p>开启dashboard</p>
<p><code>ceph mgr module enable dashboard</code></p>
<p>绑定开启 dashboard 模块的 ceph-mgr 节点的 ip 地址</p>
<p><code>ceph config-key set mgr/dashboard/master/server_addr 192.168.105.92</code></p>
<p>dashboard 默认运行在7000端口</p>
<p><em>RBD常用命令</em></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">创建pool</span><br><span class="line">若少于5个OSD， 设置pg_num为128。</span><br><span class="line">5~10个OSD，设置pg_num为512。</span><br><span class="line">10~50个OSD，设置pg_num为4096。</span><br><span class="line"></span><br><span class="line">超过50个OSD，可以参考pgcalc计算。</span><br><span class="line">ceph osd pool create rbd 128 128 </span><br><span class="line">rbd pool init rbd</span><br><span class="line"></span><br><span class="line">删除pool</span><br><span class="line">ceph osd pool rm rbd rbd –yes-i-really-really-mean-it </span><br><span class="line">## ceph.conf 添加 </span><br><span class="line">## mon_allow_pool_delete = true</span><br><span class="line"></span><br><span class="line">手动创建一个rbd磁盘</span><br><span class="line">rbd create --image-feature layering [rbd-name] -s 10240</span><br></pre></td></tr></table></figure>
<p><em>OSD常用命令</em></p>
<p>清除磁盘上的逻辑卷</p>
<p><code>ceph-volume lvm zap --destroy /dev/vdc</code>   # 本机操作<br><code>ceph-deploy disk zap lab4 /dev/sdb</code>           # 远程操作</p>
<p>创建osd</p>
<p><code>ceph-deploy osd create lab4 --fs-type btrfs --data vg1/lvol0</code></p>
<p>删除osd节点的node4</p>
<p>查看节点node4上的所有osd，比如osd.9 osd.10：</p>
<p><code>ceph osd tree</code>                                                        #查看目前cluster状态</p>
<p>把node4上的所欲osd踢出集群：（node1节点上执行）</p>
<p><code>ceph osd out osd.9</code><br><code>ceph osd out osd.10</code></p>
<p>让node4上的所有osd停止工作：（node4上执行）</p>
<p><code>service ceph stop osd.9</code><br><code>service ceph stop osd.10</code></p>
<p>查看node4上osd的状态是否为down，权重为0</p>
<p><code>ceph osd tree</code></p>
<p>移除node4上的所有osd：</p>
<p><code>ceph osd crush remove osd.9</code><br><code>ceph osd crush remove osd.10</code></p>
<p>删除节点node4：</p>
<p><code>ceph osd crush remove ceph-node4</code></p>
<p>替换一个失效的磁盘驱动</p>
<p>首先<code>ceph osd tree</code> 查看down掉的osd，将因盘问题down掉的osd及相关key删除</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ceph osd out osd.0                         # 都在node1节点下执行</span><br><span class="line">ceph osd crush rm osd.0</span><br><span class="line">ceph auth del osd.0</span><br><span class="line">ceph osd rm osd.0</span><br></pre></td></tr></table></figure>
<p>zap新磁盘 清理新磁盘：<br><code>ceph-deploy disk zap node1 /dev/sdb</code><br>在磁盘上新建一个osd，ceph会把它添加为osd:0：<br><code>ceph-deploy --overwrite-conf osd create node1 /dev/sdb</code></p>
<h4 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h4><p>[1] <a href="http://docs.ceph.com/docs/master/start/" target="_blank" rel="noopener">http://docs.ceph.com/docs/master/start/</a><br>[2] <a href="http://docs.ceph.org.cn/start/" target="_blank" rel="noopener">http://docs.ceph.org.cn/start/</a><br>[3] <a href="http://docs.ceph.org.cn/install/manual-deployment/" target="_blank" rel="noopener">http://docs.ceph.org.cn/install/manual-deployment/</a><br>[4] <a href="http://www.cnblogs.com/freedom314/p/9247602.html" target="_blank" rel="noopener">http://www.cnblogs.com/freedom314/p/9247602.html</a><br>[5] <a href="http://docs.ceph.org.cn/rados/operations/monitoring/" target="_blank" rel="noopener">http://docs.ceph.org.cn/rados/operations/monitoring/</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://sheldon-lu.github.io/sheldon_blog/passages/ceph单节点安装/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Sheldon Lu">
      <meta itemprop="description" content="静下心来写点东西">
      <meta itemprop="image" content="/sheldon_blog/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sheldon_Lu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/sheldon_blog/passages/ceph单节点安装/" class="post-title-link" itemprop="url">ceph-单节点安装</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-09-02 21:34:50" itemprop="dateCreated datePublished" datetime="2019-09-02T21:34:50+08:00">2019-09-02</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-09-03 22:46:25" itemprop="dateModified" datetime="2019-09-03T22:46:25+08:00">2019-09-03</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/sheldon_blog/categories/Ceph/" itemprop="url" rel="index"><span itemprop="name">Ceph</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">Comments: </span>
                <a href="/sheldon_blog/passages/ceph单节点安装/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/sheldon_blog/passages/ceph单节点安装/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/sheldon_blog/passages/ceph单节点安装/" class="leancloud_visitors" data-flag-title="ceph-单节点安装">
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">Views: </span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">6.7k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">6 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="一、Ceph集群单节点安装"><a href="#一、Ceph集群单节点安装" class="headerlink" title="一、Ceph集群单节点安装"></a><strong><em>一、Ceph集群单节点安装</em></strong></h2><p>1、创建虚拟机（正常步骤）</p>
<p>2、添加ceph.repo，安装ceph-deploy</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">cat /etc/yum.repos.d/ceph.repo</span><br><span class="line"></span><br><span class="line">[Ceph]</span><br><span class="line">name=Ceph packages for $basearch</span><br><span class="line">baseurl=http://hk.ceph.com/rpm-jewel/el7/$basearch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line"></span><br><span class="line">[Ceph-noarch]</span><br><span class="line">name=Ceph noarch packages</span><br><span class="line">baseurl=http://hk.ceph.com/rpm-jewel/el7/noarch</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line"></span><br><span class="line">[ceph-source]</span><br><span class="line">name=Ceph source packages</span><br><span class="line">baseurl=http://hk.ceph.com/rpm-jewel/el7/SRPMS</span><br><span class="line">enabled=1</span><br><span class="line">gpgcheck=0</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 安装ceph-deploy</span><br><span class="line">yum clean all</span><br><span class="line">yum install ceph-deploy</span><br></pre></td></tr></table></figure>
<p>3.创建集群，并安装ceph相关软件包</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 创建目录</span><br><span class="line">mkdir -p  /root/cluster &amp; cd /root/cluster &amp; rm -f /root/cluster/*</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 进入目录，创建集群</span><br><span class="line">cd /root/cluster</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 安装依赖</span><br><span class="line">yum install -y yum-utils &amp;&amp; yum-config-manager --add-repo  https://dl.fedoraproject.org/pub/epel/7/x86_64/ &amp;&amp; yum install --nogpgcheck -y  epel-release &amp;&amp; rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 &amp;&amp; rm -f  /etc/yum.repos.d/dl.fedoraproject.org*</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 这一步非常重要，如果跳过这一步，直接进行ceph的安装，可能会报如下的错误：</span><br><span class="line">Error: Package: 1:ceph-common-10.2.10-0.el7.x86_64 (Ceph)</span><br><span class="line">​           Requires: libbabeltrace.so.1()(64bit)</span><br><span class="line">Error: Package: 1:librados2-10.2.10-0.el7.x86_64 (Ceph)</span><br><span class="line">​           Requires: liblttng-ust.so.0()(64bit)</span><br><span class="line">Error: Package: 1:librgw2-10.2.10-0.el7.x86_64 (Ceph)</span><br><span class="line">​           Requires: libfcgi.so.0()(64bit)</span><br><span class="line">Error: Package: 1:librbd1-10.2.10-0.el7.x86_64 (Ceph)</span><br><span class="line">​           Requires: liblttng-ust.so.0()(64bit)</span><br><span class="line">Error: Package: 1:ceph-common-10.2.10-0.el7.x86_64 (Ceph)</span><br><span class="line">​           Requires: libbabeltrace-ctf.so.1()(64bit)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 安装ceph软件包</span><br><span class="line">yum install ceph ceph-radosgw</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 虚机准备工作</span><br><span class="line">sed -i 's/SELINUX=.*/SELINUX=disabled/' /etc/selinux/config</span><br><span class="line">setenforce 0</span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br><span class="line"><span class="meta">#</span> swapoff -a</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 确认ceph软件包版本</span><br><span class="line">ceph-deploy --version</span><br><span class="line">ceph -v</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 开始部署</span><br><span class="line">ceph-deploy new $HOSTNAME</span><br></pre></td></tr></table></figure>
<p>4.修改配置文件，配置初始化</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> osd的pool副本数size为1，可设置</span><br><span class="line">echo osd pool default size = 1 &gt;&gt; ceph.conf   </span><br><span class="line">echo osd crush chooseleaf type = 0 &gt;&gt; ceph.conf</span><br><span class="line">echo osd max object name len = 256 &gt;&gt; ceph.conf</span><br><span class="line">echo osd journal size = 128 &gt;&gt; ceph.conf</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> ceph.conf配置</span><br><span class="line">cat ceph.conf</span><br><span class="line">[global]</span><br><span class="line">fsid = 90602ee8-b900-44f2-b218-9dd4fc8273af</span><br><span class="line">mon_initial_members = ceph-node</span><br><span class="line">mon_host = 10.100.2.36</span><br><span class="line">auth_cluster_required = cephx</span><br><span class="line">auth_service_required = cephx</span><br><span class="line">auth_client_required = cephx</span><br><span class="line">osd pool default size = 1    </span><br><span class="line">osd crush chooseleaf type = 0</span><br><span class="line">osd max object name len = 256</span><br><span class="line">osd journal size = 128</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> monitor初始化</span><br><span class="line">ceph-deploy mon create-initial</span><br></pre></td></tr></table></figure>
<p>5.添加osd</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span> 创建文件夹，添加osd</span><br><span class="line">mkdir -p /var/run/ceph/</span><br><span class="line">chown ceph:ceph /var/run/ceph/</span><br><span class="line">mkdir -p /osd &amp; rm -rf /osd/*</span><br><span class="line">chown ceph:ceph /osd</span><br><span class="line">ceph-deploy mon create-initial</span><br><span class="line">ceph-deploy osd prepare $HOSTNAME:/osd</span><br><span class="line">ceph-deploy osd activate $HOSTNAME:/osd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 验证ceph状态是否健康</span><br><span class="line">ceph -s</span><br></pre></td></tr></table></figure>
<h2 id="二、ceph开启CephFS挂载"><a href="#二、ceph开启CephFS挂载" class="headerlink" title="二、ceph开启CephFS挂载"></a><strong><em>二、ceph开启CephFS挂载</em></strong></h2><p>1、安装前确定集群状态是正常的</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line">ceph -s</span><br><span class="line">​    cluster 817696dd-7d7d-459d-8ce3-314326f2c056</span><br><span class="line">​     health HEALTH_OK</span><br><span class="line">​     monmap e1: 1 mons at &#123;ceph-node=10.100.2.36:6789/0&#125;</span><br><span class="line">​            election epoch 3, quorum 0 ceph-node</span><br><span class="line">​      fsmap e5: 1/1/1 up &#123;0=ceph-node=up:active&#125;</span><br><span class="line">​     osdmap e10: 1 osds: 1 up, 1 in</span><br><span class="line">​            flags sortbitwise,require_jewel_osds</span><br><span class="line">​      pgmap v10806: 192 pgs, 3 pools, 2068 bytes data, 20 objects</span><br><span class="line">​            1681 MB used, 49493 MB / 51175 MB avail</span><br><span class="line">​                 192 active+clean</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> cephfs需要启用mds元数据数据服务，检查元数据服务是否创建</span><br><span class="line">ceph mds stat</span><br><span class="line"><span class="meta">#</span> 如下则为正常</span><br><span class="line">ceph mds stat</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 正常显示为 e5: 1/1/1 up &#123;0=ceph-node=up:active&#125;</span><br><span class="line"><span class="meta">#</span> 全为0的话，则添加mds节点</span><br><span class="line">ceph-deploy mds create ceph-node</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 创建cephfs文件系统</span><br><span class="line">ceph osd pool create cephfs_data 64</span><br><span class="line">ceph osd pool create cephfs_metadata 64</span><br><span class="line">ceph fs new cephfs cephfs_metadata cephfs_data</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 挂载，这里默认使用admin用户；可自己创建用户挂载</span><br><span class="line">mkdir /mnt/cephfs</span><br><span class="line">mount -t ceph 10.100.2.36:6789:/ /aseit-data/cephfs -o name=admin,secret=AQBM82RdI6TAExAAoix7KkMBtSPDSh6ZG69iHg==</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> secret在/etc/ceph/ceph.client.admin.keyring的base64</span><br><span class="line"><span class="meta">#</span> 可创建client用户来使用</span><br><span class="line"><span class="meta">#</span> 验证</span><br><span class="line">df -h</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>## 安装以及挂载cephfs时出现的问题：</span><br><span class="line"><span class="meta">#</span> 1.mount error 5 = Input/output error</span><br><span class="line"><span class="meta">#</span> 2. mount error 22 = Invalid argument</span><br><span class="line"><span class="meta">#</span> 第一个，首先先查mds服务是正常，不存在则添加，即ceph-deploy mds create ceph-node</span><br><span class="line"><span class="meta">#</span> 第二个，密钥不正确，检查密钥，即-o name=admin,secret=AQBM82RdI6TAExAAoix7KkMBtSPDSh6ZG69iHg==挂载时使用</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span> 下面为卸载ceph操作</span><br><span class="line">ps aux|grep ceph |awk '&#123;print $2&#125;'|xargs kill -9</span><br><span class="line">ps -ef|grep ceph</span><br><span class="line">ceph-deploy uninstall  [&#123;ceph-node&#125;]                                  # 卸载所有ceph程序</span><br><span class="line">ceph-deploy purge   [[ceph-node&#125; [&#123;ceph-node&#125;]                        #删除ceph相关的包</span><br><span class="line">ceph-deploy purgedata &#123;ceph-node&#125; [&#123;ceph-node&#125;]                       # 删除ceph相关的包</span><br><span class="line">ceph-deploy forgetkeys</span><br></pre></td></tr></table></figure>
<h3 id="三、"><a href="#三、" class="headerlink" title="三、"></a><strong><em>三、</em></strong></h3><h3 id="附录："><a href="#附录：" class="headerlink" title="附录："></a><strong><em>附录：</em></strong></h3><h4 id="单节点安装ceph脚本"><a href="#单节点安装ceph脚本" class="headerlink" title="单节点安装ceph脚本"></a>单节点安装ceph脚本</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span>!/bin/bash</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>echo 10.100.2.36 $HOSTNAME &gt;&gt;  /etc/hosts</span><br><span class="line"></span><br><span class="line">yum clean all</span><br><span class="line"><span class="meta">#</span>rm -rf /etc/yum.repos.d/*.repo</span><br><span class="line"><span class="meta">#</span>wget -O /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo</span><br><span class="line"><span class="meta">#</span>wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo</span><br><span class="line"><span class="meta">#</span>sed -i '/aliyuncs/d' /etc/yum.repos.d/CentOS-Base.repo</span><br><span class="line"><span class="meta">#</span>sed -i 's/$releasever/7/g' /etc/yum.repos.d/CentOS-Base.repo</span><br><span class="line"><span class="meta">#</span>sed -i '/aliyuncs/d' /etc/yum.repos.d/epel.repo</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span>echo "</span><br><span class="line"><span class="meta">#</span>[ceph]</span><br><span class="line"><span class="meta">#</span>name=ceph</span><br><span class="line"><span class="meta">#</span>baseurl=https://mirrors.aliyun.com/ceph/rpm-jewel/el7/x86_64/</span><br><span class="line"><span class="meta">#</span>gpgcheck=0</span><br><span class="line"><span class="meta">#</span>[ceph-noarch]</span><br><span class="line"><span class="meta">#</span>name=cephnoarch</span><br><span class="line"><span class="meta">#</span>baseurl=https://mirrors.aliyun.com/ceph/rpm-jewel/el7/noarch/</span><br><span class="line"><span class="meta">#</span>gpgcheck=0</span><br><span class="line"><span class="meta">#</span>" &gt; /etc/yum.repos.d/ceph.repo</span><br><span class="line"></span><br><span class="line">yum install -y yum-utils &amp;&amp; yum-config-manager --add-repo https://dl.fedoraproject.org/pub/epel/7/x86_64/ &amp;&amp; yum install --nogpgcheck -y epel-release &amp;&amp; rpm --import /etc/pki/rpm-gpg/RPM-GPG-KEY-EPEL-7 &amp;&amp; rm -f /etc/yum.repos.d/dl.fedoraproject.org*</span><br><span class="line"></span><br><span class="line">yum install ceph ceph-radosgw ceph-deploy -y</span><br><span class="line"></span><br><span class="line">mkdir -p  /root/cluster &amp; cd /root/cluster &amp; rm -f /root/cluster/*</span><br><span class="line">cd /root/cluster</span><br><span class="line"></span><br><span class="line">sed -i 's/SELINUX=.*/SELINUX=disabled/' /etc/selinux/config</span><br><span class="line">setenforce 0</span><br><span class="line">systemctl stop firewalld</span><br><span class="line">systemctl disable firewalld</span><br><span class="line"><span class="meta">#</span>swapoff -a</span><br><span class="line"></span><br><span class="line">ceph-deploy new $HOSTNAME</span><br><span class="line"></span><br><span class="line">echo osd pool default size = 1 &gt;&gt; ceph.conf</span><br><span class="line">echo osd crush chooseleaf type = 0 &gt;&gt; ceph.conf</span><br><span class="line">echo osd max object name len = 256 &gt;&gt; ceph.conf</span><br><span class="line">echo osd journal size = 128 &gt;&gt; ceph.conf</span><br><span class="line"></span><br><span class="line">mkdir -p /var/run/ceph/</span><br><span class="line">chown ceph:ceph /var/run/ceph/</span><br><span class="line">mkdir -p /osd &amp; rm -rf /osd/*</span><br><span class="line">chown ceph:ceph /osd</span><br><span class="line">ceph-deploy mon create-initial</span><br><span class="line">ceph-deploy osd prepare $HOSTNAME:/osd</span><br><span class="line">ceph-deploy osd activate  $HOSTNAME:/osd</span><br><span class="line"></span><br><span class="line">ceph -s</span><br></pre></td></tr></table></figure>
<h4 id="ceph重装清理脚本"><a href="#ceph重装清理脚本" class="headerlink" title="ceph重装清理脚本"></a>ceph重装清理脚本</h4><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">\#!/bin/bash</span><br><span class="line"></span><br><span class="line">ps aux|grep ceph |awk '&#123;print $2&#125;'|xargs kill -9</span><br><span class="line">ps -ef|grep ceph</span><br><span class="line"></span><br><span class="line">umount /var/lib/ceph/osd/*</span><br><span class="line">rm -rf /var/lib/ceph/osd/*</span><br><span class="line">rm -rf /var/lib/ceph/mon/*</span><br><span class="line">rm -rf /var/lib/ceph/mds/*</span><br><span class="line">rm -rf /var/lib/ceph/bootstrap-mds/*</span><br><span class="line">rm -rf /var/lib/ceph/bootstrap-osd/*</span><br><span class="line">rm -rf /var/lib/ceph/bootstrap-mon/*</span><br><span class="line">rm -rf /var/lib/ceph/tmp/*</span><br><span class="line">rm -rf /etc/ceph/*</span><br><span class="line">rm -rf /var/run/ceph/*</span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://sheldon-lu.github.io/sheldon_blog/passages/Kubernetes-Istio-Gateway-Ingress-Controller/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Sheldon Lu">
      <meta itemprop="description" content="静下心来写点东西">
      <meta itemprop="image" content="/sheldon_blog/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sheldon_Lu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/sheldon_blog/passages/Kubernetes-Istio-Gateway-Ingress-Controller/" class="post-title-link" itemprop="url">Kubernetes_Istio-Gateway_Ingress-Controller</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-07-01 22:37:27" itemprop="dateCreated datePublished" datetime="2019-07-01T22:37:27+08:00">2019-07-01</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-09-03 22:34:14" itemprop="dateModified" datetime="2019-09-03T22:34:14+08:00">2019-09-03</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/sheldon_blog/categories/Kubernetes/" itemprop="url" rel="index"><span itemprop="name">Kubernetes</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">Comments: </span>
                <a href="/sheldon_blog/passages/Kubernetes-Istio-Gateway-Ingress-Controller/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/sheldon_blog/passages/Kubernetes-Istio-Gateway-Ingress-Controller/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/sheldon_blog/passages/Kubernetes-Istio-Gateway-Ingress-Controller/" class="leancloud_visitors" data-flag-title="Kubernetes_Istio-Gateway_Ingress-Controller">
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">Views: </span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">8.7k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">8 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <blockquote>
<p>最近在istio，然后想总结下ingress-controller和istio-gateway的区别。</p>
<p>在Kubernetes环境中，Kubernetes Ingress用于配置需要在集群外部公开的服务。</p>
<p>但是在Istio服务网格中，更好的方法是使用新的配置模型，即Istio Gateway；Gateway允许将Istio流量管理的功能应用于进入集群的流量。</p>
<p>这里的ingress为nginx ingress</p>
</blockquote>
<p>通过灰度发布，来看一下ingress与istio的实现方式的差异
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/sheldon_blog/passages/Kubernetes-Istio-Gateway-Ingress-Controller/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </p></div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://sheldon-lu.github.io/sheldon_blog/passages/Kubernetes-1-13-5部署ingress/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Sheldon Lu">
      <meta itemprop="description" content="静下心来写点东西">
      <meta itemprop="image" content="/sheldon_blog/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sheldon_Lu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/sheldon_blog/passages/Kubernetes-1-13-5部署ingress/" class="post-title-link" itemprop="url">Kubernetes_1.13.5_ingress</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-06-14 09:23:38" itemprop="dateCreated datePublished" datetime="2019-06-14T09:23:38+08:00">2019-06-14</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-07-01 22:20:33" itemprop="dateModified" datetime="2019-07-01T22:20:33+08:00">2019-07-01</time>
              
            
          </span>

          

          
            
            
              
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">Comments: </span>
                <a href="/sheldon_blog/passages/Kubernetes-1-13-5部署ingress/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/sheldon_blog/passages/Kubernetes-1-13-5部署ingress/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/sheldon_blog/passages/Kubernetes-1-13-5部署ingress/" class="leancloud_visitors" data-flag-title="Kubernetes_1.13.5_ingress">
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">Views: </span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">0</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">1 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://sheldon-lu.github.io/sheldon_blog/passages/Kubernetes部署mysql-opreator/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Sheldon Lu">
      <meta itemprop="description" content="静下心来写点东西">
      <meta itemprop="image" content="/sheldon_blog/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sheldon_Lu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/sheldon_blog/passages/Kubernetes部署mysql-opreator/" class="post-title-link" itemprop="url">Kubernetes部署mysql-opreator</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-06-13 11:04:31" itemprop="dateCreated datePublished" datetime="2019-06-13T11:04:31+08:00">2019-06-13</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-07-01 22:24:44" itemprop="dateModified" datetime="2019-07-01T22:24:44+08:00">2019-07-01</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/sheldon_blog/categories/Kubernetes/" itemprop="url" rel="index"><span itemprop="name">Kubernetes</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">Comments: </span>
                <a href="/sheldon_blog/passages/Kubernetes部署mysql-opreator/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/sheldon_blog/passages/Kubernetes部署mysql-opreator/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/sheldon_blog/passages/Kubernetes部署mysql-opreator/" class="leancloud_visitors" data-flag-title="Kubernetes部署mysql-opreator">
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">Views: </span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">0</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">1 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            
          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://sheldon-lu.github.io/sheldon_blog/passages/Kubernetes安装-kubeadm-1-13-5-非外部etcd/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Sheldon Lu">
      <meta itemprop="description" content="静下心来写点东西">
      <meta itemprop="image" content="/sheldon_blog/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sheldon_Lu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/sheldon_blog/passages/Kubernetes安装-kubeadm-1-13-5-非外部etcd/" class="post-title-link" itemprop="url">Kubernetes安装-kubeadm_1.13.5(非外部etcd)</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-04-17 23:09:10" itemprop="dateCreated datePublished" datetime="2019-04-17T23:09:10+08:00">2019-04-17</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-07-01 22:21:23" itemprop="dateModified" datetime="2019-07-01T22:21:23+08:00">2019-07-01</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/sheldon_blog/categories/Kubernetes/" itemprop="url" rel="index"><span itemprop="name">Kubernetes</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">Comments: </span>
                <a href="/sheldon_blog/passages/Kubernetes安装-kubeadm-1-13-5-非外部etcd/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/sheldon_blog/passages/Kubernetes安装-kubeadm-1-13-5-非外部etcd/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/sheldon_blog/passages/Kubernetes安装-kubeadm-1-13-5-非外部etcd/" class="leancloud_visitors" data-flag-title="Kubernetes安装-kubeadm_1.13.5(非外部etcd)">
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">Views: </span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">13k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">12 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <blockquote>
<p>之前写过一篇kubernetes安装的，用到的是外部的etcd，即etcd单独二进制安装，kubeadm初始化指定即可；</p>
<p>本篇用的纯kubeadm安装高可用集群，master组件静态pod</p>
</blockquote>
<h2 id="环境描述"><a href="#环境描述" class="headerlink" title="环境描述"></a>环境描述</h2><blockquote>
<p>kubernetes version：1.13.5</p>
<p>docker version：18.6.3</p>
<p>Redhat：7.6</p>
</blockquote>
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/sheldon_blog/passages/Kubernetes安装-kubeadm-1-13-5-非外部etcd/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://sheldon-lu.github.io/sheldon_blog/passages/Python-set集合操作/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Sheldon Lu">
      <meta itemprop="description" content="静下心来写点东西">
      <meta itemprop="image" content="/sheldon_blog/images/avatar.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sheldon_Lu">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/sheldon_blog/passages/Python-set集合操作/" class="post-title-link" itemprop="url">Python_set集合操作</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-04-16 21:30:11" itemprop="dateCreated datePublished" datetime="2019-04-16T21:30:11+08:00">2019-04-16</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-09-03 22:34:47" itemprop="dateModified" datetime="2019-09-03T22:34:47+08:00">2019-09-03</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/sheldon_blog/categories/Python/" itemprop="url" rel="index"><span itemprop="name">Python</span></a></span>

                
                
              
            </span>
          

          
            
            
              
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
            
                <span class="post-meta-item-text">Comments: </span>
                <a href="/sheldon_blog/passages/Python-set集合操作/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count valine-comment-count" data-xid="/sheldon_blog/passages/Python-set集合操作/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          
            <span id="/sheldon_blog/passages/Python-set集合操作/" class="leancloud_visitors" data-flag-title="Python_set集合操作">
              <span class="post-meta-divider">|</span>
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              
                <span class="post-meta-item-text">Views: </span>
              
                <span class="leancloud-visitors-count"></span>
            </span>
          

          

          
            <div class="post-symbolscount">
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-file-word-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Symbols count in article: </span>
                
                <span title="Symbols count in article">2.9k</span>
              

              
                <span class="post-meta-divider">|</span>
              

              
                <span class="post-meta-item-icon">
                  <i class="fa fa-clock-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Reading time &asymp;</span>
                
                <span title="Reading time">3 mins.</span>
              
            </div>
          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          <h2 id="Python-set集合类型小结"><a href="#Python-set集合类型小结" class="headerlink" title="Python_set集合类型小结"></a>Python_set集合类型小结</h2><p>下面来点简单的小例子说明把。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = set(<span class="string">'spam'</span>)</span><br><span class="line">y = set([<span class="string">'h'</span>,<span class="string">'a'</span>,<span class="string">'m'</span>])</span><br><span class="line">x, y</span><br><span class="line">(set([<span class="string">'a'</span>, <span class="string">'p'</span>, <span class="string">'s'</span>, <span class="string">'m'</span>]), set([<span class="string">'a'</span>, <span class="string">'h'</span>, <span class="string">'m'</span>]))</span><br></pre></td></tr></table></figure>
<p>再来些小应用。
          <!--noindex-->
          
            <div class="post-button text-center">
              <a class="btn" href="/sheldon_blog/passages/Python-set集合操作/#more" rel="contents">
                Read more &raquo;
              </a>
            </div>
          
          <!--/noindex-->
        
      
    </p></div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/sheldon_blog/page/2/">2</a><a class="page-number" href="/sheldon_blog/page/3/">3</a><a class="extend next" rel="next" href="/sheldon_blog/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image" src="/sheldon_blog/images/avatar.jpg" alt="Sheldon Lu">
            
              <p class="site-author-name" itemprop="name">Sheldon Lu</p>
              <div class="site-description motion-element" itemprop="description">静下心来写点东西</div>
          </div>
          
          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/sheldon_blog/archives/">
                
                    <span class="site-state-item-count">24</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                      <a href="/sheldon_blog/categories/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">6</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                      <a href="/sheldon_blog/tags/">
                    
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">17</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
          

          

          

          
            <div class="links-of-author motion-element">
              
                <span class="links-of-author-item">
                  
                  
                    
                  
                  
                    
                  
                  <a href="https://github.com/sheldon-lu" title="GitHub &rarr; https://github.com/sheldon-lu" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
                </span>
              
            </div>
          

          <div id="music163player">
            <iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width="330" height="450" src="//music.163.com/outchain/player?type=0&id=2743277027&auto=1&height=430">
            <!--<iframe frameborder="no" border="0" marginwidth="0" marginheight="0" width=330 height=86 src="//music.163.com/outchain/player?type=2&id=548556767&auto=1&height=66">-->
            </iframe>
          </div>

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>
  


        
        <script>
//暂时储存文章中的内容
var div = $('.post-body');
//暂时储存目录的内容
var toc=$('.post-toc-wrap')
function password() {
  if(''){
  //将文章内容删除
    div.remove();
 //将目录删除 
    toc.remove();
  //将文章删除后，向原来文章的地方添加，应该出现的提示用户输入密码的样式
  //下面这里的第一个用textarea是因为如果在手机端的时候只能显示一部分文字，
  //只是拓展:input里面的字只能显示一行，不会自动换行，目前上网搜索没有发现好的办法，所以用了textarea，右下角的小三角通过resize:none 去掉。
   $('.post-header').after('<textarea class="description" value="Please enter your password and press enter to build" style="border: none;display: block;' +'width: 60%;margin: 0 auto;text-align: center;outline: none;margin-bottom: 50px;resize:none ">
      Please enter your password and press enter to build</textarea>' +
      '<div class="qiang" style="height: 100px;width: 60%;margin:0 auto">' +
      '<input class="password"  type="text" value="" style="border: none;display: block;border-bottom: 1px solid #ccc;' +
      'margin: 0 auto;outline: none;width:95%"/>' +
      '</div>')
      //绑定点击事件，如果是点击的.password 这个div就改变样式，如果是document中除了div之外的其他任何元素，就变回原来的样式。
    document.onclick = function (event) {
      var e = event || window.event;
      var elem = e.srcElement || e.target;

      while (elem) {
        if (elem != document) {
          if (elem.className == "password") {
            $(".password").animate({paddingTop:"30px",width:"100%",borderWidth:"2px"},300)
            return;
          }
          elem = elem.parentNode;
        } else {
          $(".password").animate({paddingTop:"0px",width:"95%",borderWidth:"1px"},300)
          return;
        }
      }
    }
    //绑定enter键按下后离开的事件
    $(document).keyup(function(event){
      if(event.keyCode ==13&&$('.password').length>0){
        //console.log($('.password').val())
        //console.log('')
        if ($('.password').val() == '') {
        //恢复文章内容
          (div).appendTo($(".post-header"))
          //恢复目录
          toc.appendTo($(".sidebar-inner"))
                 //删除本页面的输入密码组件
           $(".description").remove();
          $(".qiang").remove();
          $(".password").remove();
          //重新处理pjax事件,如果没有加pjax的从下面这行起到下面的else之间的代码需要去掉。
          //图片懒加载，没有加入此功能的这个函数需要去掉
	          $('img').lazyload({
	             placeholder: '../images/loading.gif',
	             effect: 'fadeIn',
	             threshold : 100,
	             failure_limit : 20,
	             skip_invisible : false
	           });
	           //pjax后出现文章不显示，没有pjax的下面四行需要去掉
	            $(".post-block").css({opacity:1});
	            $(".post-header").css({opacity:1});
	            $(".post-body").css({opacity:1});
	            $(".pagination").css({opacity:1});
        }else {
          alert("Sorry, the password is wrong.")
        }
      }
      //将document的keyup移除，防止在pjax的情况下会重复绑定事件
    });
  }
}
password();
</script>
      </div>
      <script>
//暂时储存文章中的内容
var div = $('.post-body');
//暂时储存目录的内容
var toc=$('.post-toc-wrap')
function password() {
  if(''){
  //将文章内容删除
    div.remove();
 //将目录删除 
    toc.remove();
  //将文章删除后，向原来文章的地方添加，应该出现的提示用户输入密码的样式
  //下面这里的第一个用textarea是因为如果在手机端的时候只能显示一部分文字，
  //只是拓展:input里面的字只能显示一行，不会自动换行，目前上网搜索没有发现好的办法，所以用了textarea，右下角的小三角通过resize:none 去掉。
   $('.post-header').after('<textarea class="description" value="Please enter your password and press enter to build" style="border: none;display: block;' +'width: 60%;margin: 0 auto;text-align: center;outline: none;margin-bottom: 50px;resize:none ">
      Please enter your password and press enter to build</textarea>' +
      '<div class="qiang" style="height: 100px;width: 60%;margin:0 auto">' +
      '<input class="password"  type="text" value="" style="border: none;display: block;border-bottom: 1px solid #ccc;' +
      'margin: 0 auto;outline: none;width:95%"/>' +
      '</div>')
      //绑定点击事件，如果是点击的.password 这个div就改变样式，如果是document中除了div之外的其他任何元素，就变回原来的样式。
    document.onclick = function (event) {
      var e = event || window.event;
      var elem = e.srcElement || e.target;

      while (elem) {
        if (elem != document) {
          if (elem.className == "password") {
            $(".password").animate({paddingTop:"30px",width:"100%",borderWidth:"2px"},300)
            return;
          }
          elem = elem.parentNode;
        } else {
          $(".password").animate({paddingTop:"0px",width:"95%",borderWidth:"1px"},300)
          return;
        }
      }
    }
    //绑定enter键按下后离开的事件
    $(document).keyup(function(event){
      if(event.keyCode ==13&&$('.password').length>0){
        //console.log($('.password').val())
        //console.log('')
        if ($('.password').val() == '') {
        //恢复文章内容
          (div).appendTo($(".post-header"))
          //恢复目录
          toc.appendTo($(".sidebar-inner"))
                 //删除本页面的输入密码组件
           $(".description").remove();
          $(".qiang").remove();
          $(".password").remove();
          //重新处理pjax事件,如果没有加pjax的从下面这行起到下面的else之间的代码需要去掉。
          //图片懒加载，没有加入此功能的这个函数需要去掉
	          $('img').lazyload({
	             placeholder: '../images/loading.gif',
	             effect: 'fadeIn',
	             threshold : 100,
	             failure_limit : 20,
	             skip_invisible : false
	           });
	           //pjax后出现文章不显示，没有pjax的下面四行需要去掉
	            $(".post-block").css({opacity:1});
	            $(".post-header").css({opacity:1});
	            $(".post-body").css({opacity:1});
	            $(".pagination").css({opacity:1});
        }else {
          alert("Sorry, the password is wrong.")
        }
      }
      //将document的keyup移除，防止在pjax的情况下会重复绑定事件
    });
  }
}
password();
</script>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <!--<i class="fa fa-user"></i>-->
    <i class="fa fa-heart" aria-hidden="true"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Sheldon Lu</span>
  
  

  
</div>

<!--

  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a></div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a><span>　<i class="fa fa-bomb"></i></span>
  <span id="showDays"></span>
  </div>

-->
<span><i class="fa fa-bomb"></i></span>
<span id="showDays"></span>

<div>
<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<!--<span id="busuanzi_container_site_pv" style='display:none'>
    本站总访问量 <span id="busuanzi_value_site_pv"></span> 次
    <span class="post-meta-divider">|</span>
</span>-->
<span id="busuanzi_container_site_uv" style="display:none">
    有<span id="busuanzi_value_site_uv"></span>人看过我的博客啦
</span>
</div>



<script>
  var seconds = 1000;
  var minutes = seconds * 60;
  var hours = minutes * 60;
  var days = hours * 24;
  var years = days * 365;
  var birthDay = Date.UTC(2019,03,15,00,00,00); // 这里设置建站时间
  setInterval(function() {
    var today = new Date();
    var todayYear = today.getFullYear();
    var todayMonth = today.getMonth()+1;
    var todayDate = today.getDate();
    var todayHour = today.getHours();
    var todayMinute = today.getMinutes();
    var todaySecond = today.getSeconds();
    var now = Date.UTC(todayYear,todayMonth,todayDate,todayHour,todayMinute,todaySecond);
    var diff = now - birthDay;
    var diffYears = Math.floor(diff/years);
    var diffDays = Math.floor((diff/days)-diffYears*365);
    var diffHours = Math.floor((diff-(diffYears*365+diffDays)*days)/hours);
    var diffMinutes = Math.floor((diff-(diffYears*365+diffDays)*days-diffHours*hours)/minutes);
    var diffSeconds = Math.floor((diff-(diffYears*365+diffDays)*days-diffHours*hours-diffMinutes*minutes)/seconds);
      document.getElementById('showDays').innerHTML="本站已运行 "+diffYears+" 年 "+diffDays+" 天 "+diffHours+" 小时 "+diffMinutes+" 分钟 "+diffSeconds+" 秒";
  }, 1000);
</script>
        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>














  
    
    
  
  <script color="0,0,255" opacity="0.5" zindex="-1" count="99" src="/sheldon_blog/lib/canvas-nest/canvas-nest.min.js"></script>













  
  <script src="/sheldon_blog/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/sheldon_blog/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/sheldon_blog/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/sheldon_blog/js/src/utils.js?v=7.0.1"></script>

  <script src="/sheldon_blog/js/src/motion.js?v=7.0.1"></script>



  
  


  <script src="/sheldon_blog/js/src/schemes/muse.js?v=7.0.1"></script>




  

  


  <script src="/sheldon_blog/js/src/next-boot.js?v=7.0.1"></script>


  

  

  

  
  

<script src="//cdn1.lncld.net/static/js/3.11.1/av-min.js"></script>



<script src="//unpkg.com/valine/dist/Valine.min.js"></script>

<script>
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(function(item) {
    return GUEST.indexOf(item) > -1;
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: 'wcykO4AM4RdvFXlzAJAOVgGz-gzGzoHsz',
    appKey: '1okhim5n5Pq6KwHuFKmTRC9t',
    placeholder: 'Just go go',
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: false,
    lang: '' || 'zh-cn'
  });
</script>





  
  <script>
    // Popup Window;
    var isfetched = false;
    var isXml = true;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length === 0) {
      search_path = "search.xml";
    } else if (/json$/i.test(search_path)) {
      isXml = false;
    }
    var path = "/sheldon_blog/" + search_path;
    // monitor main search box;

    var onPopupClose = function (e) {
      $('.popup').hide();
      $('#local-search-input').val('');
      $('.search-result-list').remove();
      $('#no-result').remove();
      $(".local-search-pop-overlay").remove();
      $('body').css('overflow', '');
    }

    function proceedsearch() {
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay"></div>')
        .css('overflow', 'hidden');
      $('.search-popup-overlay').click(onPopupClose);
      $('.popup').toggle();
      var $localSearchInput = $('#local-search-input');
      $localSearchInput.attr("autocapitalize", "none");
      $localSearchInput.attr("autocorrect", "off");
      $localSearchInput.focus();
    }

    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';

      // start loading animation
      $("body")
        .append('<div class="search-popup-overlay local-search-pop-overlay">' +
          '<div id="search-loading-icon">' +
          '<i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>' +
          '</div>' +
          '</div>')
        .css('overflow', 'hidden');
      $("#search-loading-icon").css('margin', '20% auto 0 auto').css('text-align', 'center');

      

      $.ajax({
        url: path,
        dataType: isXml ? "xml" : "json",
        async: true,
        success: function(res) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = isXml ? $("entry", res).map(function() {
            return {
              title: $("title", this).text(),
              content: $("content",this).text(),
              url: $("url" , this).text()
            };
          }).get() : res;
          var input = document.getElementById(search_id);
          var resultContent = document.getElementById(content_id);
          var inputEventFunction = function() {
            var searchText = input.value.trim().toLowerCase();
            var keywords = searchText.split(/[\s\-]+/);
            if (keywords.length > 1) {
              keywords.push(searchText);
            }
            var resultItems = [];
            if (searchText.length > 0) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var hitCount = 0;
                var searchTextCount = 0;
                var title = data.title.trim();
                var titleInLowerCase = title.toLowerCase();
                var content = data.content.trim().replace(/<[^>]+>/g,"");
                
                var contentInLowerCase = content.toLowerCase();
                var articleUrl = decodeURIComponent(data.url).replace(/\/{2,}/g, '/');
                var indexOfTitle = [];
                var indexOfContent = [];
                // only match articles with not empty titles
                if(title != '') {
                  keywords.forEach(function(keyword) {
                    function getIndexByWord(word, text, caseSensitive) {
                      var wordLen = word.length;
                      if (wordLen === 0) {
                        return [];
                      }
                      var startPosition = 0, position = [], index = [];
                      if (!caseSensitive) {
                        text = text.toLowerCase();
                        word = word.toLowerCase();
                      }
                      while ((position = text.indexOf(word, startPosition)) > -1) {
                        index.push({position: position, word: word});
                        startPosition = position + wordLen;
                      }
                      return index;
                    }

                    indexOfTitle = indexOfTitle.concat(getIndexByWord(keyword, titleInLowerCase, false));
                    indexOfContent = indexOfContent.concat(getIndexByWord(keyword, contentInLowerCase, false));
                  });
                  if (indexOfTitle.length > 0 || indexOfContent.length > 0) {
                    isMatch = true;
                    hitCount = indexOfTitle.length + indexOfContent.length;
                  }
                }

                // show search results

                if (isMatch) {
                  // sort index by position of keyword

                  [indexOfTitle, indexOfContent].forEach(function (index) {
                    index.sort(function (itemLeft, itemRight) {
                      if (itemRight.position !== itemLeft.position) {
                        return itemRight.position - itemLeft.position;
                      } else {
                        return itemLeft.word.length - itemRight.word.length;
                      }
                    });
                  });

                  // merge hits into slices

                  function mergeIntoSlice(text, start, end, index) {
                    var item = index[index.length - 1];
                    var position = item.position;
                    var word = item.word;
                    var hits = [];
                    var searchTextCountInSlice = 0;
                    while (position + word.length <= end && index.length != 0) {
                      if (word === searchText) {
                        searchTextCountInSlice++;
                      }
                      hits.push({position: position, length: word.length});
                      var wordEnd = position + word.length;

                      // move to next position of hit

                      index.pop();
                      while (index.length != 0) {
                        item = index[index.length - 1];
                        position = item.position;
                        word = item.word;
                        if (wordEnd > position) {
                          index.pop();
                        } else {
                          break;
                        }
                      }
                    }
                    searchTextCount += searchTextCountInSlice;
                    return {
                      hits: hits,
                      start: start,
                      end: end,
                      searchTextCount: searchTextCountInSlice
                    };
                  }

                  var slicesOfTitle = [];
                  if (indexOfTitle.length != 0) {
                    slicesOfTitle.push(mergeIntoSlice(title, 0, title.length, indexOfTitle));
                  }

                  var slicesOfContent = [];
                  while (indexOfContent.length != 0) {
                    var item = indexOfContent[indexOfContent.length - 1];
                    var position = item.position;
                    var word = item.word;
                    // cut out 100 characters
                    var start = position - 20;
                    var end = position + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if (end < position + word.length) {
                      end = position + word.length;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    slicesOfContent.push(mergeIntoSlice(content, start, end, indexOfContent));
                  }

                  // sort slices in content by search text's count and hits' count

                  slicesOfContent.sort(function (sliceLeft, sliceRight) {
                    if (sliceLeft.searchTextCount !== sliceRight.searchTextCount) {
                      return sliceRight.searchTextCount - sliceLeft.searchTextCount;
                    } else if (sliceLeft.hits.length !== sliceRight.hits.length) {
                      return sliceRight.hits.length - sliceLeft.hits.length;
                    } else {
                      return sliceLeft.start - sliceRight.start;
                    }
                  });

                  // select top N slices in content

                  var upperBound = parseInt('1');
                  if (upperBound >= 0) {
                    slicesOfContent = slicesOfContent.slice(0, upperBound);
                  }

                  // highlight title and content

                  function highlightKeyword(text, slice) {
                    var result = '';
                    var prevEnd = slice.start;
                    slice.hits.forEach(function (hit) {
                      result += text.substring(prevEnd, hit.position);
                      var end = hit.position + hit.length;
                      result += '<b class="search-keyword">' + text.substring(hit.position, end) + '</b>';
                      prevEnd = end;
                    });
                    result += text.substring(prevEnd, slice.end);
                    return result;
                  }

                  var resultItem = '';

                  if (slicesOfTitle.length != 0) {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + highlightKeyword(title, slicesOfTitle[0]) + "</a>";
                  } else {
                    resultItem += "<li><a href='" + articleUrl + "' class='search-result-title'>" + title + "</a>";
                  }

                  slicesOfContent.forEach(function (slice) {
                    resultItem += "<a href='" + articleUrl + "'>" +
                      "<p class=\"search-result\">" + highlightKeyword(content, slice) +
                      "...</p>" + "</a>";
                  });

                  resultItem += "</li>";
                  resultItems.push({
                    item: resultItem,
                    searchTextCount: searchTextCount,
                    hitCount: hitCount,
                    id: resultItems.length
                  });
                }
              })
            };
            if (keywords.length === 1 && keywords[0] === "") {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-search fa-5x"></i></div>'
            } else if (resultItems.length === 0) {
              resultContent.innerHTML = '<div id="no-result"><i class="fa fa-frown-o fa-5x"></i></div>'
            } else {
              resultItems.sort(function (resultLeft, resultRight) {
                if (resultLeft.searchTextCount !== resultRight.searchTextCount) {
                  return resultRight.searchTextCount - resultLeft.searchTextCount;
                } else if (resultLeft.hitCount !== resultRight.hitCount) {
                  return resultRight.hitCount - resultLeft.hitCount;
                } else {
                  return resultRight.id - resultLeft.id;
                }
              });
              var searchResultList = '<ul class=\"search-result-list\">';
              resultItems.forEach(function (result) {
                searchResultList += result.item;
              })
              searchResultList += "</ul>";
              resultContent.innerHTML = searchResultList;
            }
          }

          if ('auto' === 'auto') {
            input.addEventListener('input', inputEventFunction);
          } else {
            $('.search-icon').click(inputEventFunction);
            input.addEventListener('keypress', function (event) {
              if (event.keyCode === 13) {
                inputEventFunction();
              }
            });
          }

          // remove loading animation
          $(".local-search-pop-overlay").remove();
          $('body').css('overflow', '');

          proceedsearch();
        }
      });
    }

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched === false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(onPopupClose);
    $('.popup').click(function(e){
      e.stopPropagation();
    });
    $(document).on('keyup', function (event) {
      var shouldDismissSearchPopup = event.which === 27 &&
        $('.search-popup').is(':visible');
      if (shouldDismissSearchPopup) {
        onPopupClose();
      }
    });
  </script>





  
  
  <script>
    
    function showTime(Counter) {
      var entries = [];
      var $visitors = $('.leancloud_visitors');

      $visitors.each(function() {
        entries.push( $(this).attr('id').trim() );
      });

      Counter('get', '/classes/Counter', { where: JSON.stringify({ url: { '$in': entries } }) })
        .done(function({ results }) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.url;
            var time = item.time;
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for (var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if (countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function ({ responseJSON }) {
          console.log('LeanCloud Counter Error: ' + responseJSON.code + ' ' + responseJSON.error);
        });
    }
    

    $(function() {
      $.get('https://app-router.leancloud.cn/2/route?appId=' + 'wcykO4AM4RdvFXlzAJAOVgGz-gzGzoHsz')
        .done(function({ api_server }) {
          var Counter = function(method, url, data) {
            return $.ajax({
              method: method,
              url: 'https://' + api_server + '/1.1' + url,
              headers: {
                'X-LC-Id': 'wcykO4AM4RdvFXlzAJAOVgGz-gzGzoHsz',
                'X-LC-Key': '1okhim5n5Pq6KwHuFKmTRC9t',
                'Content-Type': 'application/json',
              },
              data: data
            });
          };
          
            if ($('.post-title-link').length >= 1) {
              showTime(Counter);
            }
          
        });
    });
  </script>



  

  

  

  

  

  

  

  

  

  

  

  

  

</body>
</html>
